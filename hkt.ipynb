{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee78220d8fa94b07a8039e909e59cd45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6eefb0b83c0e42bca141f1d6b2232205",
              "IPY_MODEL_98e2162572254dc491110c4fbb03a6a4",
              "IPY_MODEL_aa82db9aef42486d8cc80b06ccecda51"
            ],
            "layout": "IPY_MODEL_6b347dae670245939cd8c896784baeed"
          }
        },
        "6eefb0b83c0e42bca141f1d6b2232205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43ea9d0aeaf942e4850d1f0572d12bc0",
            "placeholder": "​",
            "style": "IPY_MODEL_b53c9960bdec470bad7efe07f872174b",
            "value": "Downloading: 100%"
          }
        },
        "98e2162572254dc491110c4fbb03a6a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4372c74bbfea462fa7025415dd582e3f",
            "max": 760289,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_460fd763a36445f6a12f3d35ad374fcb",
            "value": 760289
          }
        },
        "aa82db9aef42486d8cc80b06ccecda51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dea6bc3d035e4b0f96394905dc8bb970",
            "placeholder": "​",
            "style": "IPY_MODEL_41662771d3eb4fe7a45bb421dfc3c303",
            "value": " 760k/760k [00:00&lt;00:00, 1.97MB/s]"
          }
        },
        "6b347dae670245939cd8c896784baeed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43ea9d0aeaf942e4850d1f0572d12bc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b53c9960bdec470bad7efe07f872174b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4372c74bbfea462fa7025415dd582e3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "460fd763a36445f6a12f3d35ad374fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dea6bc3d035e4b0f96394905dc8bb970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41662771d3eb4fe7a45bb421dfc3c303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c00a28e18ea149098c13e6109a941116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e9d472775f3473dabca50db53d8542a",
              "IPY_MODEL_26ef5a0e52e84159800e5a87a6710a2f",
              "IPY_MODEL_e0a149a9b0f54144bc7c7a9209cdac37"
            ],
            "layout": "IPY_MODEL_263b28ed50504afc8eb595e7545038f7"
          }
        },
        "4e9d472775f3473dabca50db53d8542a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e70e23e7f054bf08bd5983d7bd14f3c",
            "placeholder": "​",
            "style": "IPY_MODEL_c0055876f39049edb404c17d13a0440f",
            "value": "Downloading: 100%"
          }
        },
        "26ef5a0e52e84159800e5a87a6710a2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ed5aad5ef8f49788fd2e0534c362985",
            "max": 684,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e1e694bac1a4656a1ce884de073eb7f",
            "value": 684
          }
        },
        "e0a149a9b0f54144bc7c7a9209cdac37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2da6bcc911974189a4b2eb98cf3add2b",
            "placeholder": "​",
            "style": "IPY_MODEL_24db70fdcbed4234b7f1c14c17f7ee85",
            "value": " 684/684 [00:00&lt;00:00, 47.8kB/s]"
          }
        },
        "263b28ed50504afc8eb595e7545038f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e70e23e7f054bf08bd5983d7bd14f3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0055876f39049edb404c17d13a0440f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ed5aad5ef8f49788fd2e0534c362985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e1e694bac1a4656a1ce884de073eb7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2da6bcc911974189a4b2eb98cf3add2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24db70fdcbed4234b7f1c14c17f7ee85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ece487748b1f434784282a0825c23797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dcdc9ed0744c4cc693b13c4616e7b3dc",
              "IPY_MODEL_d2b6c342af43412285972f5752b0b1c4",
              "IPY_MODEL_34d9059989f74876b63d4c34cdff9ead"
            ],
            "layout": "IPY_MODEL_488b49976fd647c2999f496e40cb02ac"
          }
        },
        "dcdc9ed0744c4cc693b13c4616e7b3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a14925a9da3443e7a500b4e0438abc6c",
            "placeholder": "​",
            "style": "IPY_MODEL_fb86522349c64781b2a888d5263b8f27",
            "value": "Downloading: 100%"
          }
        },
        "d2b6c342af43412285972f5752b0b1c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a76c6c2ca844c25ba802e0dca4f13ff",
            "max": 47376696,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fb69e947aaf43f69ed3506f60ea3d1f",
            "value": 47376696
          }
        },
        "34d9059989f74876b63d4c34cdff9ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f6340b082524929bdcca9bb6a023d37",
            "placeholder": "​",
            "style": "IPY_MODEL_188d09b27efc4273af6b326d2e522382",
            "value": " 47.4M/47.4M [00:00&lt;00:00, 72.2MB/s]"
          }
        },
        "488b49976fd647c2999f496e40cb02ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a14925a9da3443e7a500b4e0438abc6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb86522349c64781b2a888d5263b8f27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a76c6c2ca844c25ba802e0dca4f13ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fb69e947aaf43f69ed3506f60ea3d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f6340b082524929bdcca9bb6a023d37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "188d09b27efc4273af6b326d2e522382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mvLg3mQz5lV",
        "outputId": "f417af39-9e04-4bd3-ab36-6ba89e4fb228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiHTSMzt2FHU",
        "outputId": "d7d6019f-656e-4a12-b3c6-75c8fe1d5b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 20 18:32:04 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==2.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKlHxNvt2yr7",
        "outputId": "94a3b547-1ae5-452b-df78-0c754094684e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==2.10.0\n",
            "  Downloading transformers-2.10.0-py3-none-any.whl (660 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m660.2/660.2 KB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==2.10.0) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==2.10.0) (2022.6.2)\n",
            "Collecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp38-cp38-manylinux1_x86_64.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==2.10.0) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==2.10.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==2.10.0) (4.64.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.10.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.10.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.10.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==2.10.0) (2022.12.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==2.10.0) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=e3ad3c4275c1f04dda44730c7f7a400846ac82b2a431def8f2e9836710561b08\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.7.0 transformers-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install wandb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOVp4tVIhj-s",
        "outputId": "34dac51b-c0bc-496a-ce26-b77ae7649e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.10-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.15.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=69874dfec8752e175a1f7f88cc0eb3c28bdd9a5881439d4628ed499d36e90fbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, urllib3, smmap, setproctitle, docker-pycreds, sentry-sdk, gitdb, GitPython, wandb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.15.0 setproctitle-1.3.2 smmap-5.0.0 urllib3-1.26.14 wandb-0.13.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv0YiT_pjIzm",
        "outputId": "e83e5acc-aa99-4e84-a7d3-a2e752d0de4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/32/HKT/global_config.py')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/32/HKT')"
      ],
      "metadata": {
        "id": "kaIz1INbShp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import global_config"
      ],
      "metadata": {
        "id": "di18tQPzTSDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visual_features_list=list(range(55,91))\n",
        "acoustic_features_list=list(range(0,60))\n",
        "\n",
        "ACOUSTIC_DIM = len(acoustic_features_list)\n",
        "VISUAL_DIM = len(visual_features_list)\n",
        "HCF_DIM=4\n",
        "LANGUAGE_DIM=768\n",
        "\n",
        "VISUAL_DIM_ALL = 91\n",
        "ACOUSTIC_DIM_ALL = 81\n",
        "\n",
        "H_MERGE_SENT = 768\n",
        "DATASET_LOCATION = \"./dataset/\"\n",
        "SEP_TOKEN_ID = 3"
      ],
      "metadata": {
        "id": "F9XxkWkL0ewl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import sys\n",
        "# from global_config import *\n",
        "import numpy as np\n",
        "import wandb \n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from torch.nn import CrossEntropyLoss, L1Loss, BCEWithLogitsLoss\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from transformers import (\n",
        "    AlbertConfig,\n",
        "    AlbertTokenizer,\n",
        "    AlbertForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BertForNextSentencePrediction,\n",
        "    BertTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "# from models import *\n",
        "from transformers.optimization import AdamW\n",
        "import copy"
      ],
      "metadata": {
        "id": "9uj9QMB2RgCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975b565a-5d4b-431b-f6c6-38de08182c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import sys\n",
        "# from global_config import *\n",
        "import math\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from transformers import (\n",
        "    AlbertModel,\n",
        "    AlbertPreTrainedModel,\n",
        "    AlbertConfig,\n",
        "    load_tf_weights_in_albert,\n",
        ")\n",
        "# from transformers.modeling_albert import AlbertEmbeddings, AlbertLayerGroup\n"
      ],
      "metadata": {
        "id": "7KDSWtr01wou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parser = argparse.ArgumentParser()"
      ],
      "metadata": {
        "id": "8fU9z6FIVb0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# parser.add_argument(\n",
        "#     # \"--model\", type=str, choices=[\"HKT\",\"language_only\", \"acoustic_only\", \"visual_only\",\"hcf_only\"], default=\"HKT\",\n",
        "#     \"--model\", type=str,  default=\"HKT\",\n",
        "# )"
      ],
      "metadata": {
        "id": "B2MVnA78VeSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parser.add_argument(\"--dataset\", type=str, choices=[\"sarcasm\"], default=\"sarcasm\")\n",
        "# parser.add_argument(\"--batch_size\", type=int, default=16)\n",
        "# parser.add_argument(\"--max_seq_length\", type=int, default=85)\n",
        "# parser.add_argument(\"--n_layers\", type=int, default=1)\n",
        "# parser.add_argument(\"--n_heads\", type=int, default=1)\n",
        "# parser.add_argument(\"--cross_n_layers\", type=int, default=1)\n",
        "# parser.add_argument(\"--cross_n_heads\", type=int, default=4)\n",
        "# parser.add_argument(\"--fusion_dim\", type=int, default=172)\n",
        "# parser.add_argument(\"--dropout\", type=float, default=0.2366)\n",
        "# parser.add_argument(\"--epochs\", type=int, default=20)\n",
        "\n",
        "# parser.add_argument(\"--seed\", type=int, default=100)\n",
        "\n",
        "# parser.add_argument(\"--learning_rate\", type=float, default=0.000005)\n",
        "# parser.add_argument(\"--learning_rate_a\", type=float, default=0.003)\n",
        "# parser.add_argument(\"--learning_rate_h\", type=float, default=0.0003)\n",
        "# parser.add_argument(\"--learning_rate_v\", type=float, default=0.003)\n",
        "# parser.add_argument(\"--warmup_ratio\", type=float, default=0.07178)\n",
        "# parser.add_argument(\"--save_weight\", type=str, choices=[\"True\",\"False\"], default=\"False\")"
      ],
      "metadata": {
        "id": "CQPjKogIViS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def return_unk():\n",
        "#     return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# args, unknown = parser.parse_known_args()\n",
        "# # args = parser.parse_args()"
      ],
      "metadata": {
        "id": "admN-C2nRlzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBHqbC-WlQdM",
        "outputId": "36160a8d-31c0-4b8e-b439-cdef60b29539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, num_layers=1, nhead=1, dropout=0.1, dim_feedforward=128, max_seq_length=5000):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoder = nn.Embedding(max_seq_length, d_model)\n",
        "        self.encoder = TransformerEncoder(TransformerLayer(d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout), num_layers=num_layers)\n",
        "        self.decoder = nn.Linear(d_model, 1)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, input, attention_mask=None):\n",
        "        seq_length = input.size()[1]\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input.device)\n",
        "        positions_embedding = self.pos_encoder(position_ids).unsqueeze(0).expand(input.size()) # (seq_length, d_model) => (batch_size, seq_length, d_model)\n",
        "        input = input + positions_embedding\n",
        "        input = self.norm(input)\n",
        "        hidden = self.encoder(input, attention_mask=attention_mask)\n",
        "        out = self.decoder(hidden) # (batch_size, seq_len, hidden_dim)\n",
        "        out = (out[:,0,:], out, hidden) # ([CLS] token embedding, full output, last hidden layer)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, nhead=1, dim_feedforward=128, dropout=0.1):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.self_attention = Attention(hidden_size, nhead, dropout)\n",
        "        self.fc = nn.Sequential(nn.Linear(hidden_size, dim_feedforward), nn.ReLU(), nn.Linear(dim_feedforward, hidden_size))\n",
        "        self.norm1 = nn.LayerNorm(hidden_size)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, attention_mask=None):\n",
        "        src_1 = self.self_attention(src, src, attention_mask=attention_mask)\n",
        "        src = src + self.dropout1(src_1)\n",
        "        src = self.norm1(src)\n",
        "        src_2 = self.fc(src)\n",
        "        src = src + self.dropout2(src_2)\n",
        "        src = self.norm2(src)\n",
        "\n",
        "        return src\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, layer, num_layers):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = _get_clones(layer, num_layers)\n",
        "    def forward(self, src, attention_mask=None):\n",
        "        for layer in self.layers:\n",
        "            new_src = layer(src, attention_mask=attention_mask)\n",
        "            src = src + new_src\n",
        "        return src\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob, ctx_dim=None):\n",
        "        super().__init__()\n",
        "        if hidden_size % num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        # visual_dim = 2048\n",
        "        if ctx_dim is None:\n",
        "            ctx_dim = hidden_size\n",
        "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(ctx_dim, self.all_head_size)\n",
        "        self.value = nn.Linear(ctx_dim, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, context, attention_mask=None):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(context)\n",
        "        mixed_value_layer = self.value(context)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        # Apply the attention mask is \n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer\n",
        "\n",
        "\n",
        "class CrossAttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, context_size, nhead=1, dropout=0.1):\n",
        "        super(CrossAttentionLayer, self).__init__()\n",
        "        self.src_cross_attention = Attention(hidden_size, nhead, dropout, ctx_dim=context_size)\n",
        "        self.context_cross_attention = Attention(context_size, nhead, dropout, ctx_dim=hidden_size)\n",
        "        self.self_attention = Attention(hidden_size + context_size, nhead, dropout)\n",
        "        self.fc = nn.Sequential(nn.Linear(hidden_size + context_size, hidden_size + context_size), nn.ReLU())\n",
        "        self.norm1 = nn.LayerNorm(hidden_size + context_size)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size + context_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, context, attention_mask=None):\n",
        "        new_src = self.src_cross_attention(src, context, attention_mask=attention_mask)\n",
        "        new_context = self.context_cross_attention(context, src, attention_mask=attention_mask)\n",
        "        \n",
        "        cross_src = torch.cat((new_src, new_context), dim=2)\n",
        "        \n",
        "        cross_src_1 = self.self_attention(cross_src, cross_src, attention_mask)\n",
        "        cross_src = cross_src + self.dropout1(cross_src_1)\n",
        "        cross_src = self.norm1(cross_src)\n",
        "\n",
        "        cross_src_2 = self.fc(cross_src)\n",
        "        cross_src = cross_src + self.dropout2(cross_src_2)\n",
        "        cross_src = self.norm2(cross_src)\n",
        "\n",
        "        return cross_src\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "class CrossAttentionEncoder(nn.Module):\n",
        "    def __init__(self, layer, num_layers):\n",
        "        super(CrossAttentionEncoder, self).__init__()\n",
        "        self.layers = _get_clones(layer, num_layers)\n",
        "\n",
        "    def forward(self, src, context, attention_mask=None):\n",
        "        src_dim = src.size()[2]\n",
        "        context_dim = context.size()[2]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(src, context, attention_mask=attention_mask)\n",
        "            new_src = output[:,:,0:src_dim]\n",
        "            new_context = output[:,:,src_dim:src_dim+context_dim]\n",
        "\n",
        "            src = src + new_src\n",
        "            context = context + new_context\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "#this version use multiple layer of cross attention\n",
        "class HKTMultiLayerCrossAttn(nn.Module):\n",
        "    \n",
        "    def __init__(self, text_model, visual_model, acoustic_model,hcf_model, args, dropout=0.1,fusion_dim=128):\n",
        "        \n",
        "        super(HKTMultiLayerCrossAttn, self).__init__()\n",
        "        self.newly_added_config=args\n",
        "        self.text_model = text_model\n",
        "        self.visual_model = visual_model\n",
        "        self.acoustic_model = acoustic_model\n",
        "        self.hcf_model = hcf_model\n",
        "        \n",
        "        L_AV_layer = CrossAttentionLayer((LANGUAGE_DIM+HCF_DIM), ACOUSTIC_DIM+VISUAL_DIM, nhead=args.cross_n_heads, dropout=args.dropout)\n",
        "        self.L_AV = CrossAttentionEncoder(L_AV_layer, args.cross_n_layers)\n",
        "        \n",
        "        total_dim = 2 * (LANGUAGE_DIM+HCF_DIM+ ACOUSTIC_DIM + VISUAL_DIM )\n",
        "        \n",
        "        self.fc = nn.Sequential(nn.Linear(total_dim, args.fusion_dim), \n",
        "                                nn.ReLU(), \n",
        "                                nn.Dropout(args.dropout), \n",
        "                                nn.Linear(args.fusion_dim, 1))\n",
        "        \n",
        "    def get_params(self):\n",
        "        \n",
        "        acoustic_params=list(self.acoustic_model.named_parameters())\n",
        "        visual_params=list(self.visual_model.named_parameters())\n",
        "        hcf_params=list(self.hcf_model.named_parameters())\n",
        "        \n",
        "        other_params=list(self.text_model.named_parameters())+list(self.L_AV.named_parameters())+list(self.fc.named_parameters())\n",
        "        \n",
        "        return acoustic_params,visual_params,hcf_params,other_params\n",
        "    \n",
        "\n",
        "    def forward(self, input_ids, visual, acoustic,hcf, attention_mask=None, token_type_ids=None):\n",
        "        \n",
        "        (text_output, _) = self.text_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        (_, _, visual_output) = self.visual_model(visual)\n",
        "        (_, _, acoustic_output) = self.acoustic_model(acoustic)\n",
        "        (_, _, hcf_output) = self.hcf_model(hcf)\n",
        "        \n",
        "        \n",
        "        text_hcf=torch.cat((text_output,hcf_output),dim=2)\n",
        "        \n",
        "        # attention mask conversion\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "        \n",
        "        av_output=torch.cat((acoustic_output,visual_output),dim=2)\n",
        "                \n",
        "        noverbal_text=self.L_AV(text_hcf, av_output, attention_mask=extended_attention_mask)\n",
        "        \n",
        "        # Extract embeddings\n",
        "        text_embedding = text_hcf[:,0,:] # [CLS] token\n",
        "        visual_embedding = F.max_pool1d(visual_output.permute(0,2,1).contiguous(), visual_output.shape[1]).squeeze(-1)\n",
        "        acoustic_embedding = F.max_pool1d(acoustic_output.permute(0,2,1).contiguous(),acoustic_output.shape[1]).squeeze(-1)\n",
        "        L_AV_embedding = F.max_pool1d(noverbal_text.permute(0,2,1).contiguous(),noverbal_text.shape[1]).squeeze(-1)\n",
        "        \n",
        "        \n",
        "        #print(weighted_vad_emb.shape)\n",
        "        fusion = (text_embedding, visual_embedding, acoustic_embedding,L_AV_embedding)\n",
        "        fused_hidden = torch.cat(fusion, dim=1)\n",
        "        \n",
        "        out = self.fc(fused_hidden)\n",
        "        \n",
        "        return (out, fused_hidden)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class HKT(nn.Module):\n",
        "    def __init__(self, text_model, visual_model, acoustic_model,hcf_model, args, dropout=0.1,fusion_dim=128):\n",
        "        super(HKT, self).__init__()\n",
        "        \n",
        "        self.newly_added_config=args\n",
        "        self.text_model = text_model\n",
        "        self.visual_model = visual_model\n",
        "        self.acoustic_model = acoustic_model\n",
        "        self.hcf_model = hcf_model\n",
        "        \n",
        "        self.L_AV = CrossAttentionLayer(LANGUAGE_DIM+HCF_DIM, ACOUSTIC_DIM+VISUAL_DIM, nhead=args.cross_n_heads, dropout=args.dropout)\n",
        "        \n",
        "        total_dim = 2 * (LANGUAGE_DIM+HCF_DIM + ACOUSTIC_DIM + VISUAL_DIM )\n",
        "        \n",
        "        self.fc = nn.Sequential(nn.Linear(total_dim, args.fusion_dim), \n",
        "                                nn.ReLU(), \n",
        "                                nn.Dropout(args.dropout), \n",
        "                                nn.Linear(args.fusion_dim, 1))\n",
        "        \n",
        "    def get_params(self):\n",
        "        \n",
        "        acoustic_params=list(self.acoustic_model.named_parameters())\n",
        "        visual_params=list(self.visual_model.named_parameters())\n",
        "        hcf_params=list(self.hcf_model.named_parameters())\n",
        "        \n",
        "        other_params=list(self.text_model.named_parameters())+list(self.L_AV.named_parameters())+list(self.fc.named_parameters())\n",
        "        \n",
        "        return acoustic_params,visual_params,hcf_params,other_params\n",
        "    \n",
        "    def forward(self, input_ids, visual, acoustic,hcf, attention_mask=None, token_type_ids=None):\n",
        "        # print('input ids shape : ', input_ids.shape)\n",
        "        # print('attention_mask shape : ', attention_mask.shape)\n",
        "        (text_output, _) = self.text_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        # text_output = self.text_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)['last_hidden_state']\n",
        "        (_, _, visual_output) = self.visual_model(visual)\n",
        "        (_, _, acoustic_output) = self.acoustic_model(acoustic)\n",
        "        (_, _, hcf_output) = self.hcf_model(hcf)\n",
        "        \n",
        "        # print('self text model : \\n')\n",
        "        # print(self.text_model)\n",
        "        # print()\n",
        "        # print('text output type : ', type(text_output))\n",
        "        # print('text_output shape : ', text_output.shape)\n",
        "        # print('hcf_output shape : ', hcf_output.shape)\n",
        "        text_hcf=torch.cat((text_output,hcf_output),dim=2)\n",
        "        \n",
        "        # attention mask conversion\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "        \n",
        "        av_output=torch.cat((acoustic_output,visual_output),dim=2)\n",
        "                \n",
        "        noverbal_text=self.L_AV(text_hcf, av_output, attention_mask=extended_attention_mask)\n",
        "        \n",
        "        # Extract embeddings\n",
        "        text_embedding = text_hcf[:,0,:] # [CLS] token\n",
        "        visual_embedding = F.max_pool1d(visual_output.permute(0,2,1).contiguous(), visual_output.shape[1]).squeeze(-1)\n",
        "        acoustic_embedding = F.max_pool1d(acoustic_output.permute(0,2,1).contiguous(),acoustic_output.shape[1]).squeeze(-1)\n",
        "        L_AV_embedding = F.max_pool1d(noverbal_text.permute(0,2,1).contiguous(),noverbal_text.shape[1]).squeeze(-1)\n",
        "        \n",
        "        \n",
        "        fusion = (text_embedding, visual_embedding, acoustic_embedding,L_AV_embedding)\n",
        "        fused_hidden = torch.cat(fusion, dim=1)\n",
        "        \n",
        "        out = self.fc(fused_hidden)\n",
        "        \n",
        "        return (out, fused_hidden)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "metadata": {
        "id": "SvIymQxFtpu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class InputFeatures(object):\n",
        "#     \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "#     def __init__(self, input_ids, input_mask, segment_ids, visual, acoustic,hcf,label_id):\n",
        "#         self.input_ids = input_ids\n",
        "#         self.input_mask = input_mask\n",
        "#         self.segment_ids = segment_ids\n",
        "#         self.visual = visual\n",
        "#         self.acoustic = acoustic\n",
        "#         self.hcf = hcf\n",
        "#         self.label_id = label_id\n",
        "\n",
        "# def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "#     \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "#     pop_count = 0\n",
        "#     while True:\n",
        "#         total_length = len(tokens_a) + len(tokens_b)\n",
        "#         if total_length <= max_length:\n",
        "#             break\n",
        "#         if len(tokens_a) == 0:\n",
        "#             tokens_b.pop()\n",
        "#         else:\n",
        "#             pop_count += 1\n",
        "#             tokens_a.pop(0)\n",
        "#     return pop_count\n",
        "\n",
        "# #albert tokenizer split words in to subwords. \"_\" marker helps to find thos sub words\n",
        "# #our acoustic and visual features are aligned on word level. So we just create copy the same \n",
        "# #visual/acoustic vectors that belong to same word.\n",
        "# def get_inversion(tokens, SPIECE_MARKER=\"▁\"):\n",
        "#     inversion_index = -1\n",
        "#     inversions = []\n",
        "#     for token in tokens:\n",
        "#         if SPIECE_MARKER in token:\n",
        "#             inversion_index += 1\n",
        "#         inversions.append(inversion_index)\n",
        "#     return inversions\n",
        "\n",
        "\n",
        "# def convert_humor_to_features(examples, tokenizer, punchline_only=False):\n",
        "#     features = []\n",
        "\n",
        "#     for (ex_index, example) in enumerate(examples):\n",
        "        \n",
        "#         #p denotes punchline, c deontes context\n",
        "#         #hid is the utterance unique id. these id's are provided by the authors of urfunny and mustard\n",
        "#         #label is either 1/0 . 1=humor, 0=not humor\n",
        "#         (\n",
        "#             (p_words, p_visual, p_acoustic, p_hcf),\n",
        "#             (c_words, c_visual, c_acoustic, c_hcf),\n",
        "#             hid,\n",
        "#             label\n",
        "#         ) = example\n",
        "                \n",
        "#         text_a = \". \".join(c_words)\n",
        "#         text_b = p_words + \".\"\n",
        "#         tokens_a = tokenizer.tokenize(text_a)\n",
        "#         tokens_b = tokenizer.tokenize(text_b)\n",
        "        \n",
        "#         inversions_a = get_inversion(tokens_a)\n",
        "#         inversions_b = get_inversion(tokens_b)\n",
        "\n",
        "#         pop_count = _truncate_seq_pair(tokens_a, tokens_b, args.max_seq_length - 3)\n",
        "\n",
        "#         inversions_a = inversions_a[pop_count:]\n",
        "#         inversions_b = inversions_b[: len(tokens_b)]\n",
        "\n",
        "#         visual_a = []\n",
        "#         acoustic_a = []\n",
        "#         hcf_a=[]        \n",
        "#         #our acoustic and visual features are aligned on word level. So we just \n",
        "#         #create copy of the same visual/acoustic vectors that belong to same word.\n",
        "#         #because ber tokenizer split word into subwords\n",
        "#         for inv_id in inversions_a:\n",
        "#             visual_a.append(c_visual[inv_id, :])\n",
        "#             acoustic_a.append(c_acoustic[inv_id, :])\n",
        "#             hcf_a.append(c_hcf[inv_id, :])\n",
        "            \n",
        "\n",
        "\n",
        "#         visual_a = np.array(visual_a)\n",
        "#         acoustic_a = np.array(acoustic_a)\n",
        "#         hcf_a = np.array(hcf_a)\n",
        "        \n",
        "#         visual_b = []\n",
        "#         acoustic_b = []\n",
        "#         hcf_b = []\n",
        "#         for inv_id in inversions_b:\n",
        "#             visual_b.append(p_visual[inv_id, :])\n",
        "#             acoustic_b.append(p_acoustic[inv_id, :])\n",
        "#             hcf_b.append(p_hcf[inv_id, :])\n",
        "        \n",
        "#         visual_b = np.array(visual_b)\n",
        "#         acoustic_b = np.array(acoustic_b)\n",
        "#         hcf_b = np.array(hcf_b)\n",
        "        \n",
        "#         tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
        "\n",
        "#         acoustic_zero = np.zeros((1, ACOUSTIC_DIM_ALL))\n",
        "#         if len(tokens_a) == 0:\n",
        "#             acoustic = np.concatenate(\n",
        "#                 (acoustic_zero, acoustic_zero, acoustic_b, acoustic_zero)\n",
        "#             )\n",
        "#         else:\n",
        "#             acoustic = np.concatenate(\n",
        "#                 (acoustic_zero, acoustic_a, acoustic_zero, acoustic_b, acoustic_zero)\n",
        "#             )\n",
        "\n",
        "#         visual_zero = np.zeros((1, VISUAL_DIM_ALL))\n",
        "#         if len(tokens_a) == 0:\n",
        "#             visual = np.concatenate((visual_zero, visual_zero, visual_b, visual_zero))\n",
        "#         else:\n",
        "#             visual = np.concatenate(\n",
        "#                 (visual_zero, visual_a, visual_zero, visual_b, visual_zero)\n",
        "#             )\n",
        "        \n",
        "        \n",
        "#         hcf_zero = np.zeros((1,4))\n",
        "#         if len(tokens_a) == 0:\n",
        "#             hcf = np.concatenate((hcf_zero, hcf_zero, hcf_b, hcf_zero))\n",
        "#         else:\n",
        "#             hcf = np.concatenate(\n",
        "#                 (hcf_zero, hcf_a, hcf_zero, hcf_b, hcf_zero)\n",
        "                \n",
        "#             )\n",
        "        \n",
        "#         input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "#         segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
        "#         input_mask = [1] * len(input_ids)\n",
        "            \n",
        "#         acoustic_padding = np.zeros(\n",
        "#             (args.max_seq_length - len(input_ids), acoustic.shape[1])\n",
        "#         )\n",
        "#         acoustic = np.concatenate((acoustic, acoustic_padding))\n",
        "#         #original urfunny acoustic feature dimension is 81.\n",
        "#         #we found many features are highly correllated. so we removed\n",
        "#         #highly correlated feature to reduce dimension\n",
        "#         acoustic=np.take(acoustic, acoustic_features_list,axis=1)\n",
        "        \n",
        "#         visual_padding = np.zeros(\n",
        "#             (args.max_seq_length - len(input_ids), visual.shape[1])\n",
        "#         )\n",
        "#         visual = np.concatenate((visual, visual_padding))\n",
        "#         #original urfunny visual feature dimension is more than 300.\n",
        "#         #we only considred the action unit and face shape parameter features\n",
        "#         visual = np.take(visual, visual_features_list,axis=1)\n",
        "        \n",
        "        \n",
        "#         hcf_padding= np.zeros(\n",
        "#             (args.max_seq_length - len(input_ids), hcf.shape[1])\n",
        "#         )\n",
        "        \n",
        "#         hcf = np.concatenate((hcf, hcf_padding))\n",
        "        \n",
        "#         padding = [0] * (args.max_seq_length - len(input_ids))\n",
        "\n",
        "#         input_ids += padding\n",
        "#         input_mask += padding\n",
        "#         segment_ids += padding\n",
        "\n",
        "#         assert len(input_ids) == args.max_seq_length\n",
        "#         assert len(input_mask) == args.max_seq_length\n",
        "#         assert len(segment_ids) == args.max_seq_length\n",
        "#         assert acoustic.shape[0] == args.max_seq_length\n",
        "#         assert visual.shape[0] == args.max_seq_length\n",
        "#         assert hcf.shape[0] == args.max_seq_length\n",
        "        \n",
        "#         label_id = float(label)\n",
        "        \n",
        "        \n",
        "#         features.append(\n",
        "#             InputFeatures(\n",
        "#                 input_ids=input_ids,\n",
        "#                 input_mask=input_mask,\n",
        "#                 segment_ids=segment_ids,\n",
        "#                 visual=visual,\n",
        "#                 acoustic=acoustic,\n",
        "#                 hcf=hcf,\n",
        "#                 label_id=label_id,\n",
        "#             )\n",
        "#         )\n",
        "            \n",
        "#     return features\n",
        "\n",
        "\n",
        "\n",
        "# def get_appropriate_dataset(data, tokenizer, parition):\n",
        "    \n",
        "\n",
        "#     features = convert_humor_to_features(data, tokenizer)\n",
        "#     all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "#     all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "#     all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "#     all_visual = torch.tensor([f.visual for f in features], dtype=torch.float)\n",
        "#     all_acoustic = torch.tensor([f.acoustic for f in features], dtype=torch.float)\n",
        "#     hcf = torch.tensor([f.hcf for f in features], dtype=torch.float)\n",
        "#     all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
        "    \n",
        "\n",
        "#     dataset = TensorDataset(\n",
        "#         all_input_ids,\n",
        "#         all_visual,\n",
        "#         all_acoustic,\n",
        "#         all_input_mask,\n",
        "#         all_segment_ids,\n",
        "#         hcf,\n",
        "#         all_label_ids,\n",
        "#     )\n",
        "    \n",
        "#     return dataset\n",
        "\n",
        "\n",
        "# def set_up_data_loader():\n",
        "#     if args.dataset==\"humor\":\n",
        "#         data_file = \"ur_funny.pkl\"\n",
        "#     elif args.dataset==\"sarcasm\":\n",
        "#         # data_file = \"mustard.pkl\"\n",
        "#         data_file = \"/content/drive/MyDrive/Colab Notebooks/32/HKT/dataset/our_mustard_split_final.p\"\n",
        "#         # data_file = \"/content/drive/MyDrive/Colab Notebooks/32/HKT/dataset/mustard.pkl\"\n",
        "        \n",
        "#     with open(\n",
        "#         # os.path.join(DATASET_LOCATION, data_file),\n",
        "#         data_file,\n",
        "#         \"rb\",\n",
        "#     ) as handle:\n",
        "#         all_data = pickle.load(handle)\n",
        "        \n",
        "#     train_data = all_data[\"train\"]\n",
        "#     dev_data = all_data[\"dev\"]\n",
        "#     test_data = all_data[\"test\"]\n",
        "\n",
        "#     # tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "\n",
        "#     train_dataset = get_appropriate_dataset(train_data, tokenizer, \"train\")\n",
        "    \n",
        "#     dev_dataset = get_appropriate_dataset(dev_data, tokenizer, \"dev\")\n",
        "#     print('Dev dataset : ', dev_dataset)\n",
        "#     test_dataset = get_appropriate_dataset(test_data, tokenizer, \"test\")\n",
        "\n",
        "#     train_dataloader = DataLoader(\n",
        "#         train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=1\n",
        "#     )\n",
        "\n",
        "#     dev_dataloader = DataLoader(\n",
        "#         dev_dataset, batch_size=args.batch_size, shuffle=True, num_workers=1\n",
        "#     )\n",
        "\n",
        "#     test_dataloader = DataLoader(\n",
        "#         test_dataset, batch_size=args.batch_size, shuffle=True, num_workers=1\n",
        "#     )\n",
        "    \n",
        "    \n",
        "#     return train_dataloader, dev_dataloader, test_dataloader\n",
        "\n",
        "# def train_epoch(model, train_dataloader, optimizer, scheduler, loss_fct):\n",
        "#     model.train()\n",
        "#     tr_loss = 0\n",
        "#     nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "#     for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "\n",
        "#         batch = tuple(t.to(DEVICE) for t in batch)\n",
        "#         (\n",
        "#             input_ids,\n",
        "#             visual,\n",
        "#             acoustic,\n",
        "#             input_mask,\n",
        "#             segment_ids,\n",
        "#             hcf,\n",
        "#             label_ids\n",
        "#         ) = batch\n",
        "        \n",
        "#         visual = torch.squeeze(visual, 1)\n",
        "#         acoustic = torch.squeeze(acoustic, 1)\n",
        "\n",
        "#         if args.model == \"language_only\":\n",
        "#             outputs = model(\n",
        "#                 input_ids,\n",
        "#                 token_type_ids=segment_ids,\n",
        "#                 attention_mask=input_mask,\n",
        "#                 labels=None,\n",
        "#             )\n",
        "#         elif args.model == \"acoustic_only\":\n",
        "#             outputs = model(\n",
        "#                 acoustic\n",
        "#             )\n",
        "#         elif args.model == \"visual_only\":\n",
        "#             outputs = model(\n",
        "#                 visual\n",
        "#             )\n",
        "#         elif args.model==\"hcf_only\":\n",
        "#             outputs=model(hcf)\n",
        "            \n",
        "#         elif args.model==\"HKT\":\n",
        "#             # print('\\nhello worlld\\n')\n",
        "#             # print('input ids shape : ', input_ids.shape)\n",
        "#             # print('visual shape : ', visual.shape)\n",
        "#             # print('hcf shape : ', hcf.shape)\n",
        "#             # print('segment ids shape : ', segment_ids.shape)\n",
        "#             # print('attention mask shape : ', input_mask.shape)\n",
        "#             outputs = model(input_ids, visual, acoustic,hcf, token_type_ids=segment_ids, attention_mask=input_mask,)\n",
        "        \n",
        "        \n",
        "            \n",
        "#         logits = outputs[0]\n",
        "        \n",
        "#         loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "\n",
        "#         tr_loss += loss.item()\n",
        "#         nb_tr_examples += input_ids.size(0)\n",
        "#         nb_tr_steps += 1\n",
        "\n",
        "#         loss.backward()\n",
        "        \n",
        "#         for o_i in range(len(optimizer)):\n",
        "#             optimizer[o_i].step()\n",
        "#             scheduler[o_i].step()\n",
        "        \n",
        "#         model.zero_grad()\n",
        "\n",
        "#     return tr_loss/nb_tr_steps\n",
        "\n",
        "\n",
        "\n",
        "# def eval_epoch(model, dev_dataloader, loss_fct):\n",
        "    \n",
        "#     model.eval()\n",
        "#     dev_loss = 0\n",
        "#     nb_dev_examples, nb_dev_steps = 0, 0\n",
        "    \n",
        "#     with torch.no_grad():\n",
        "#         for step, batch in enumerate(tqdm(dev_dataloader, desc=\"Iteration\")):\n",
        "#             batch = tuple(t.to(DEVICE) for t in batch)\n",
        "#             (\n",
        "#                 input_ids,\n",
        "#                 visual,\n",
        "#                 acoustic,\n",
        "#                 input_mask,\n",
        "#                 segment_ids,\n",
        "#                 hcf,\n",
        "#                 label_ids\n",
        "#             ) = batch\n",
        "                    \n",
        "#             visual = torch.squeeze(visual, 1)\n",
        "#             acoustic = torch.squeeze(acoustic, 1)\n",
        "    \n",
        "#             if args.model == \"language_only\":\n",
        "#                 outputs = model(\n",
        "#                     input_ids,\n",
        "#                     token_type_ids=segment_ids,\n",
        "#                     attention_mask=input_mask,\n",
        "#                     labels=None,\n",
        "#                 )\n",
        "#             elif args.model == \"acoustic_only\":\n",
        "#                 outputs = model(\n",
        "#                     acoustic\n",
        "#                 )\n",
        "#             elif args.model == \"visual_only\":\n",
        "#                 outputs = model(\n",
        "#                     visual\n",
        "#                 )\n",
        "#             elif args.model==\"hcf_only\":\n",
        "#                 outputs=model(hcf)\n",
        "                \n",
        "#             elif args.model==\"HKT\":\n",
        "#                 outputs = model(input_ids, visual, acoustic,hcf, token_type_ids=segment_ids, attention_mask=input_mask,)\n",
        "            \n",
        "            \n",
        "#             logits = outputs[0]\n",
        "#             loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "    \n",
        "#             dev_loss += loss.item()\n",
        "#             nb_dev_examples += input_ids.size(0)\n",
        "#             nb_dev_steps += 1\n",
        "\n",
        "#     return dev_loss/nb_dev_steps\n",
        "\n",
        "# def test_epoch(model, test_data_loader, loss_fct):\n",
        "#     \"\"\" Epoch operation in evaluation phase \"\"\"\n",
        "#     model.eval()\n",
        "\n",
        "#     eval_loss = 0.0\n",
        "#     nb_eval_steps = 0\n",
        "#     preds = []\n",
        "#     all_labels = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for step, batch in enumerate(tqdm(test_data_loader, desc=\"Iteration\")):\n",
        "            \n",
        "#             batch = tuple(t.to(DEVICE) for t in batch)\n",
        "\n",
        "#             (\n",
        "#                 input_ids,\n",
        "#                 visual,\n",
        "#                 acoustic,\n",
        "#                 input_mask,\n",
        "#                 segment_ids,\n",
        "#                 hcf,\n",
        "#                 label_ids\n",
        "#             ) = batch\n",
        "                    \n",
        "#             visual = torch.squeeze(visual, 1)\n",
        "#             acoustic = torch.squeeze(acoustic, 1)\n",
        "            \n",
        "#             if args.model == \"language_only\":\n",
        "#                 outputs = model(\n",
        "#                     input_ids,\n",
        "#                     token_type_ids=segment_ids,\n",
        "#                     attention_mask=input_mask,\n",
        "#                     labels=None,\n",
        "#                 )\n",
        "#             elif args.model == \"acoustic_only\":\n",
        "#                 outputs = model(\n",
        "#                     acoustic\n",
        "#                 )\n",
        "#             elif args.model == \"visual_only\":\n",
        "#                 outputs = model(\n",
        "#                     visual\n",
        "#                 )\n",
        "#             elif args.model==\"hcf_only\":\n",
        "#                 outputs=model(hcf)\n",
        "                \n",
        "#             elif args.model==\"HKT\":\n",
        "#                 # print('\\nhello worlld\\n')\n",
        "#                 # print('input ids shape : ', input_ids.shape)\n",
        "#                 # print('visual shape : ', visual.shape)\n",
        "#                 # print('hcf shape : ', hcf.shape)\n",
        "#                 # print('segment ids shape : ', segment_ids.shape)\n",
        "#                 # print('attention mask shape : ', input_mask.shape)\n",
        "#                 outputs = model(input_ids, visual, acoustic,hcf, token_type_ids=segment_ids, attention_mask=input_mask,)\n",
        "            \n",
        "            \n",
        "#             logits = outputs[0]\n",
        "            \n",
        "            \n",
        "#             tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "\n",
        "#             eval_loss += tmp_eval_loss.mean().item()\n",
        "#             nb_eval_steps += 1\n",
        "            \n",
        "#             logits = torch.sigmoid(logits)\n",
        "            \n",
        "#             if len(preds) == 0:\n",
        "#                 preds=logits.detach().cpu().numpy()\n",
        "#                 all_labels=label_ids.detach().cpu().numpy()\n",
        "#             else:\n",
        "#                 preds= np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "#                 all_labels = np.append(\n",
        "#                     all_labels, label_ids.detach().cpu().numpy(), axis=0\n",
        "#                 )\n",
        "                \n",
        "                \n",
        "                \n",
        "#         eval_loss = eval_loss / nb_eval_steps\n",
        "#         preds = np.squeeze(preds)\n",
        "#         all_labels = np.squeeze(all_labels)\n",
        "\n",
        "#     return preds, all_labels, eval_loss\n",
        "\n",
        "\n",
        "\n",
        "# def test_score_model(model, test_data_loader, loss_fct, exclude_zero=False):\n",
        "\n",
        "#     predictions, y_test, test_loss = test_epoch(model, test_data_loader, loss_fct)\n",
        "    \n",
        "#     predictions = predictions.round()\n",
        "\n",
        "#     f_score = f1_score(y_test, predictions, average=\"weighted\")\n",
        "#     accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "#     print(\"Accuracy:\", accuracy,\"F score:\", f_score)\n",
        "#     return accuracy, f_score, test_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def train(\n",
        "#     model,\n",
        "#     train_dataloader,\n",
        "#     dev_dataloader,\n",
        "#     test_dataloader,\n",
        "#     optimizer,\n",
        "#     scheduler,\n",
        "#     loss_fct,\n",
        "# ):\n",
        "       \n",
        "#     best_valid_loss = 9e+9\n",
        "#     # run_name = str(wandb.run.id)\n",
        "#     run_name = 'colab'\n",
        "#     valid_losses = []\n",
        "    \n",
        "#     n_epochs=args.epochs\n",
        "        \n",
        "    \n",
        "#     for epoch_i in range(n_epochs):\n",
        "        \n",
        "#         train_loss = train_epoch(\n",
        "#             model, train_dataloader, optimizer, scheduler, loss_fct\n",
        "#         )\n",
        "#         valid_loss = eval_epoch(model, dev_dataloader, loss_fct)\n",
        "\n",
        "#         print('\\n')\n",
        "#         valid_losses.append(valid_loss)\n",
        "#         print(\n",
        "#             \"\\nepoch:{}, train_loss : {}, valid_loss : {}\".format(\n",
        "#                 epoch_i, train_loss, valid_loss\n",
        "#             )\n",
        "#         )\n",
        "#         print('\\n')\n",
        "\n",
        "#         valid_accuracy, valid_f_score, valid_loss = test_score_model(\n",
        "#             model, dev_dataloader, loss_fct\n",
        "#         )\n",
        "\n",
        "#         print('Valid accuracy : ', valid_accuracy)\n",
        "#         print('Valid F score : ', valid_f_score)\n",
        "#         print('\\n')\n",
        "\n",
        "#         test_accuracy, test_f_score, test_loss = test_score_model(\n",
        "#             model, test_dataloader, loss_fct\n",
        "#         )\n",
        "        \n",
        "#         print('Test accuracy : ', test_accuracy)\n",
        "#         print('Test F score : ', test_f_score)    \n",
        "#         if(valid_loss <= best_valid_loss):\n",
        "#             best_valid_loss = valid_loss\n",
        "#             best_valid_test_accuracy = test_accuracy\n",
        "#             best_valid_test_fscore= test_f_score\n",
        "#             print(\"\\nBest model\\n\")\n",
        "#             print(\"best  test accuracy : \", best_valid_test_accuracy)\n",
        "#             print('best  test fscore : ', best_valid_test_fscore)\n",
        "#             torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/32/HKT/saved_model/epoch_'+str(epoch_i)+'_f1_'+str(best_valid_test_fscore))\n",
        "#             print(\"\\n\")\n",
        "            \n",
        "#             # if(args.save_weight == \"True\"):\n",
        "#             #     print(\"Best model\\n\")\n",
        "#             #     print(\"best valid test accuracy : \", best_valid_test_accuracy)\n",
        "#             #     print('best valid test fscore : ', best_valid_test_fscore)\n",
        "#                 # torch.save(model.state_dict(),'./best_weights/'+run_name+'.pt')\n",
        "        \n",
        "\n",
        "\n",
        "#         #we report test_accuracy of the best valid loss (best_valid_test_accuracy)\n",
        "#         # wandb.log(\n",
        "#         #     {\n",
        "#         #         \"train_loss\": train_loss,\n",
        "#         #         \"valid_loss\": valid_loss,\n",
        "#         #         \"test_loss\": test_loss,\n",
        "#         #         \"best_valid_loss\": best_valid_loss,\n",
        "#         #         \"best_valid_test_accuracy\": best_valid_test_accuracy,\n",
        "#         #         \"best_valid_test_fscore\":best_valid_test_fscore\n",
        "#         #     }\n",
        "#         # )\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "# def get_optimizer_scheduler(params,num_training_steps,learning_rate=1e-5):\n",
        "    \n",
        "#     no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "#     optimizer_grouped_parameters = [\n",
        "#         {\n",
        "#             \"params\": [\n",
        "#                 p for n, p in params if not any(nd in n for nd in no_decay)\n",
        "#             ],\n",
        "#             \"weight_decay\": 0.01,\n",
        "#         },\n",
        "#         {\n",
        "#             \"params\": [\n",
        "#                 p for n, p in params if any(nd in n for nd in no_decay)\n",
        "#             ],\n",
        "#             \"weight_decay\": 0.0,\n",
        "#         },\n",
        "#     ]\n",
        "    \n",
        "#     optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "#     scheduler = get_linear_schedule_with_warmup(\n",
        "#         optimizer,\n",
        "#         num_warmup_steps=int(num_training_steps * args.warmup_ratio),\n",
        "#         num_training_steps=num_training_steps,\n",
        "#     )\n",
        "    \n",
        "#     return optimizer,scheduler\n",
        "\n",
        "# def prep_for_training(num_training_steps):\n",
        "    \n",
        "    \n",
        "#     if args.model == \"language_only\":\n",
        "#         model = AlbertForSequenceClassification.from_pretrained(\n",
        "#             \"albert-base-v2\", num_labels=1\n",
        "#         )\n",
        "#     elif args.model == \"acoustic_only\":\n",
        "#         model = Transformer(ACOUSTIC_DIM, num_layers=args.n_layers, nhead=args.n_heads, dim_feedforward=args.fc_dim)\n",
        "        \n",
        "#     elif args.model == \"visual_only\":\n",
        "#         model = Transformer(VISUAL_DIM, num_layers=args.n_layers, nhead=args.n_heads, dim_feedforward=args.fc_dim)\n",
        "        \n",
        "#     elif args.model==\"hcf_only\":\n",
        "#         model=Transformer(HCF_DIM, num_layers=args.n_layers, nhead=args.n_heads, dim_feedforward=args.fc_dim)\n",
        "        \n",
        "#     elif args.model == \"HKT\" :\n",
        "#         #HKT model has 4 unimodal encoders. But the language one is ALBERT pretrained model. But other enocders are\n",
        "#         #trained from scratch with low level features. We have found that many times most of the the gardients flows to albert encoders only as it\n",
        "#         #already has rich contextual representation. So in the beginning the gradient flows ignores other encoders which are trained from low level features. \n",
        "#         # We found that if we intitalize the weights of the acoustic, visual and hcf encoders of HKT model from the best unimodal models that we already ran for ablation study then\n",
        "#         #the model converege faster. Other wise it takes very long time to converge. \n",
        "#         if args.dataset==\"humor\":\n",
        "#             visual_model = Transformer(VISUAL_DIM, num_layers=7, nhead=3, dim_feedforward= 128)\n",
        "#             visual_model.load_state_dict(torch.load(\"./model_weights/init/humor/humorVisualTransformer.pt\"))\n",
        "#             acoustic_model = Transformer(ACOUSTIC_DIM, num_layers=8, nhead=3, dim_feedforward = 256)\n",
        "#             acoustic_model.load_state_dict(torch.load(\"./model_weights/init/humor/humorAcousticTransformer.pt\"))\n",
        "#             hcf_model = Transformer(HCF_DIM, num_layers=3, nhead=2, dim_feedforward = 128)\n",
        "#             hcf_model.load_state_dict(torch.load(\"./model_weights/init/humor/humorHCFTransformer.pt\"))\n",
        "            \n",
        "#         elif args.dataset==\"sarcasm\":\n",
        "#             visual_model = Transformer(VISUAL_DIM, num_layers=8, nhead=4, dim_feedforward=1024)\n",
        "#             # visual_model.load_state_dict(torch.load(\"./model_weights/init/sarcasm/sarcasmVisualTransformer.pt\"))\n",
        "#             visual_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/init/sarcasm/sarcasmVisualTransformer.pt\"))\n",
        "#             acoustic_model = Transformer(ACOUSTIC_DIM, num_layers=1, nhead=3, dim_feedforward=512)\n",
        "#             # acoustic_model.load_state_dict(torch.load(\"./model_weights/init/sarcasm/sarcasmAcousticTransformer.pt\"))\n",
        "#             acoustic_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/init/sarcasm/sarcasmAcousticTransformer.pt\"))\n",
        "#             hcf_model = Transformer(HCF_DIM, num_layers=8, nhead=4, dim_feedforward=128)\n",
        "#             # hcf_model.load_state_dict(torch.load(\"./model_weights/init/sarcasm/sarcasmHCFTransformer.pt\"))\n",
        "#             hcf_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/init/sarcasm/sarcasmHCFTransformer.pt\"))\n",
        "        \n",
        "#         text_model = AlbertModel.from_pretrained('albert-base-v2')\n",
        "        \n",
        "#         # print('Text model\\n')\n",
        "#         # print(text_model)\n",
        "#         # print('\\n')\n",
        "#         num_parameters = sum(p.numel() for p in text_model.parameters())\n",
        "#         print('text model Number of parameters : ', num_parameters/1e6)\n",
        "#         model = HKT(text_model, visual_model, acoustic_model,hcf_model, args)\n",
        "\n",
        "#     else:\n",
        "#         raise ValueError(\"Requested model is not available\")\n",
        "\n",
        "#     model.to(DEVICE)\n",
        "    \n",
        "#     loss_fct = BCEWithLogitsLoss()\n",
        "    \n",
        "\n",
        "#     # Prepare optimizer\n",
        "#     # used different learning rates for different componenets.\n",
        "    \n",
        "#     if args.model == \"HKT\" :\n",
        "        \n",
        "#         acoustic_params,visual_params,hcf_params,other_params = model.get_params()\n",
        "#         optimizer_o,scheduler_o=get_optimizer_scheduler(other_params,num_training_steps,learning_rate=args.learning_rate)\n",
        "#         optimizer_h,scheduler_h=get_optimizer_scheduler(hcf_params,num_training_steps,learning_rate=args.learning_rate_h)\n",
        "#         optimizer_v,scheduler_v=get_optimizer_scheduler(visual_params,num_training_steps,learning_rate=args.learning_rate_v)\n",
        "#         optimizer_a,scheduler_a=get_optimizer_scheduler(acoustic_params,num_training_steps,learning_rate=args.learning_rate_a)\n",
        "        \n",
        "#         optimizers=[optimizer_o,optimizer_h,optimizer_v,optimizer_a]\n",
        "#         schedulers=[scheduler_o,scheduler_h,scheduler_v,scheduler_a]\n",
        "        \n",
        "#     else:\n",
        "#         params = list(model.named_parameters())\n",
        "\n",
        "#         optimizer_l, scheduler_l = get_optimizer_scheduler(\n",
        "#             params, num_training_steps, learning_rate=args.learning_rate\n",
        "#         )\n",
        "        \n",
        "#         optimizers=[optimizer_l]\n",
        "#         schedulers=[scheduler_l]\n",
        "        \n",
        "        \n",
        "#     return model, optimizers, schedulers,loss_fct\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def set_random_seed(seed):\n",
        "#     \"\"\"\n",
        "#     This function controls the randomness by setting seed in all the libraries we will use.\n",
        "#     \"\"\"\n",
        "#     random.seed(seed)\n",
        "#     os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "#     np.random.seed(seed)\n",
        "#     torch.manual_seed(seed)\n",
        "#     torch.cuda.manual_seed(seed)\n",
        "#     torch.cuda.manual_seed_all(seed)\n",
        "    \n",
        "#     torch.backends.cudnn.benchmark = False\n",
        "#     torch.backends.cudnn.enabled = False\n",
        "#     torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    \n",
        "\n",
        "# def main():\n",
        "    \n",
        "#     # wandb.init(project=\"HKT\")\n",
        "#     # wandb.config.update(args)\n",
        "    \n",
        "#     if(args.seed == -1):\n",
        "#         seed = random.randint(0, 9999)\n",
        "#         print(\"seed\",seed)\n",
        "#     else:\n",
        "#         seed = args.seed\n",
        "    \n",
        "#     # wandb.config.update({\"seed\": seed}, allow_val_change=True)\n",
        "    \n",
        "#     set_random_seed(seed)\n",
        "    \n",
        "#     train_dataloader,dev_dataloader,test_dataloader=set_up_data_loader()\n",
        "#     print(\"Dataset Loaded: \",args.dataset)\n",
        "#     print('\\n')\n",
        "#     num_training_steps = len(train_dataloader) * args.epochs\n",
        "    \n",
        "#     model, optimizers, schedulers, loss_fct = prep_for_training(\n",
        "#         num_training_steps\n",
        "#     )\n",
        "#     print(\"Model Loaded: \",args.model)\n",
        "\n",
        "#     print('train loader length : ', len(train_dataloader.dataset))\n",
        "#     print('dev loader length : ', len(dev_dataloader.dataset))\n",
        "#     print('test loader length : ', len(test_dataloader.dataset))\n",
        "#     train(\n",
        "#         model,\n",
        "#         train_dataloader,\n",
        "#         dev_dataloader,\n",
        "#         test_dataloader,\n",
        "#         optimizers,\n",
        "#         schedulers,\n",
        "#         loss_fct,\n",
        "#     )\n",
        "\n",
        "#     # print(\"Full model : \\n\")\n",
        "#     # print(model)\n",
        "   \n",
        "#     # z = torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/best/sarcasm/sarcasmHKT.pt\")\n",
        "#     # print(type(z))\n",
        "#     # for name in z:\n",
        "#     #   print(name)\n",
        "\n",
        "#     # model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/best/sarcasm/sarcasmHKT.pt\"))  \n",
        "#     # model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/best/sarcasm/sarcasmAcousticTransformer.pt\"))  \n",
        "#     # print(z)\n",
        "#     # test_accuracy, test_f_score, test_loss = test_score_model(\n",
        "#     #         model, test_dataloader, loss_fct\n",
        "#     #     )\n",
        "\n",
        "#     # test_accuracy, test_f_score, test_loss = test_score_model(\n",
        "#     #         model, train_dataloader, loss_fct\n",
        "#     #     )\n",
        "    \n",
        "#     # print('Test accuracy : ', test_accuracy)\n",
        "#     # print('Test F score : ', test_f_score)\n",
        "    \n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "id": "mNpywOtOUYud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained('albert-base-v2')\n",
        "# model = AlbertModel.from_pretrained(\"albert-base-v2\")\n"
      ],
      "metadata": {
        "id": "YffYRnGKeOrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "cX_Q6peNuWZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import argparse\n",
        "# import csv\n",
        "# import logging\n",
        "# import os\n",
        "# import random\n",
        "# import pickle\n",
        "# import sys\n",
        "# from global_config import *\n",
        "# import numpy as np\n",
        "\n",
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# from sklearn.metrics import precision_recall_fscore_support\n",
        "# from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "# from torch.utils.data.distributed import DistributedSampler\n",
        "# from tqdm import tqdm, trange\n",
        "\n",
        "# from torch.nn import CrossEntropyLoss, L1Loss, BCEWithLogitsLoss\n",
        "# from scipy.stats import pearsonr, spearmanr\n",
        "# from sklearn.metrics import matthews_corrcoef\n",
        "# from transformers import (\n",
        "#     AlbertConfig,\n",
        "#     AlbertTokenizer,\n",
        "#     AlbertForSequenceClassification,\n",
        "#     BertForNextSentencePrediction,\n",
        "#     BertTokenizer,\n",
        "#     get_linear_schedule_with_warmup,\n",
        "# )\n",
        "# # from models import *\n",
        "# from transformers.optimization import AdamW"
      ],
      "metadata": {
        "id": "fzStTsbf_Qdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()"
      ],
      "metadata": {
        "id": "UiM1RGMwCwc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_unk():\n",
        "    return 0\n",
        "\n",
        "\n",
        "parser.add_argument(\n",
        "    # \"--model\", type=str, choices=[\"language_only\", \"acoustic_only\", \"visual_only\",\"hcf_only\",\"HKT\"], default=\"HKT\",\n",
        "    \"--model\", type=str,  default=\"HKT\"\n",
        ")"
      ],
      "metadata": {
        "id": "qbFRbJIsCzIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e9a8cd-6eae-4000-df7e-60170c89cfdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--model'], dest='model', nargs=None, const=None, default='HKT', type=<class 'str'>, choices=None, help=None, metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# # parser.add_argument(\"--dataset\", type=str, choices=[\"humor\", \"sarcasm\"], default=\"sarcasm\")#humor=UR-FUNNY, sarcasm=MUsTARD\n",
        "parser.add_argument(\"--dataset\", type=str, choices=[\"sarcasm\"], default=\"sarcasm\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vS9ajhTz_TrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3d2758-8e5c-4bb1-f589-aaa9cfff3b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--dataset'], dest='dataset', nargs=None, const=None, default='sarcasm', type=<class 'str'>, choices=['sarcasm'], help=None, metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.add_argument(\"--batch_size\", type=int, default=16)\n",
        "parser.add_argument(\"--max_seq_length\", type=int, default=77)\n",
        "parser.add_argument(\"--max_concept_length\", type=int, default=5)\n",
        "parser.add_argument(\"--n_layers\", type=int, default=1)\n",
        "parser.add_argument(\"--n_heads\", type=int, default=1)\n",
        "parser.add_argument(\"--cross_n_layers\", type=int, default=1)\n",
        "parser.add_argument(\"--cross_n_heads\", type=int, default=2)\n",
        "parser.add_argument(\"--fusion_dim\", type=int, default=172)\n",
        "parser.add_argument(\"--dropout\", type=float, default=0.09379)\n",
        "parser.add_argument(\"--seed\", type=int, default=5149)\n",
        "\n"
      ],
      "metadata": {
        "id": "YyGiJIBLTlzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4f2cf5-2b1c-4c0d-d0bc-e949f1c7302f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--seed'], dest='seed', nargs=None, const=None, default=5149, type=<class 'int'>, choices=None, help=None, metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "args, unknown = parser.parse_known_args()"
      ],
      "metadata": {
        "id": "DG1HtWE6LkN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_train_data = []\n",
        "global_dev_data = []\n",
        "global_test_data = []"
      ],
      "metadata": {
        "id": "1husiv0h9Apa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, visual, acoustic,hcf,label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.visual = visual\n",
        "        self.acoustic = acoustic\n",
        "        self.hcf = hcf\n",
        "        self.label_id = label_id\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "    pop_count = 0\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) == 0:\n",
        "            tokens_b.pop()\n",
        "        else:\n",
        "            pop_count += 1\n",
        "            tokens_a.pop(0)\n",
        "    return pop_count\n",
        "\n",
        "\n",
        "def get_inversion(tokens, SPIECE_MARKER=\"▁\"):\n",
        "    inversion_index = -1\n",
        "    inversions = []\n",
        "    for token in tokens:\n",
        "        if SPIECE_MARKER in token:\n",
        "            inversion_index += 1\n",
        "        inversions.append(inversion_index)\n",
        "    return inversions\n",
        "\n",
        "\n",
        "def convert_humor_to_features(examples, tokenizer, punchline_only=False):\n",
        "    features = []\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        \n",
        "        (\n",
        "            (p_words, p_visual, p_acoustic, p_hcf),\n",
        "            (c_words, c_visual, c_acoustic, c_hcf),\n",
        "            hid,\n",
        "            label\n",
        "        ) = example\n",
        "                \n",
        "        text_a = \". \".join(c_words)\n",
        "        text_b = p_words + \".\"\n",
        "        tokens_a = tokenizer.tokenize(text_a)\n",
        "        tokens_b = tokenizer.tokenize(text_b)\n",
        "        \n",
        "        inversions_a = get_inversion(tokens_a)\n",
        "        inversions_b = get_inversion(tokens_b)\n",
        "\n",
        "        pop_count = _truncate_seq_pair(tokens_a, tokens_b, args.max_seq_length - 3)\n",
        "\n",
        "        inversions_a = inversions_a[pop_count:]\n",
        "        inversions_b = inversions_b[: len(tokens_b)]\n",
        "\n",
        "        visual_a = []\n",
        "        acoustic_a = []\n",
        "        hcf_a=[]        \n",
        "\n",
        "        for inv_id in inversions_a:\n",
        "            visual_a.append(c_visual[inv_id, :])\n",
        "            acoustic_a.append(c_acoustic[inv_id, :])\n",
        "            hcf_a.append(c_hcf[inv_id, :])\n",
        "            \n",
        "\n",
        "\n",
        "        visual_a = np.array(visual_a)\n",
        "        acoustic_a = np.array(acoustic_a)\n",
        "        hcf_a = np.array(hcf_a)\n",
        "        \n",
        "        visual_b = []\n",
        "        acoustic_b = []\n",
        "        hcf_b = []\n",
        "        for inv_id in inversions_b:\n",
        "            visual_b.append(p_visual[inv_id, :])\n",
        "            acoustic_b.append(p_acoustic[inv_id, :])\n",
        "            hcf_b.append(p_hcf[inv_id, :])\n",
        "        \n",
        "        visual_b = np.array(visual_b)\n",
        "        acoustic_b = np.array(acoustic_b)\n",
        "        hcf_b = np.array(hcf_b)\n",
        "        \n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
        "\n",
        "        acoustic_zero = np.zeros((1, ACOUSTIC_DIM_ALL))\n",
        "        if len(tokens_a) == 0:\n",
        "            acoustic = np.concatenate(\n",
        "                (acoustic_zero, acoustic_zero, acoustic_b, acoustic_zero)\n",
        "            )\n",
        "        else:\n",
        "            acoustic = np.concatenate(\n",
        "                (acoustic_zero, acoustic_a, acoustic_zero, acoustic_b, acoustic_zero)\n",
        "            )\n",
        "\n",
        "        visual_zero = np.zeros((1, VISUAL_DIM_ALL))\n",
        "        if len(tokens_a) == 0:\n",
        "            visual = np.concatenate((visual_zero, visual_zero, visual_b, visual_zero))\n",
        "        else:\n",
        "            visual = np.concatenate(\n",
        "                (visual_zero, visual_a, visual_zero, visual_b, visual_zero)\n",
        "            )\n",
        "        \n",
        "        \n",
        "        hcf_zero = np.zeros((1,4))\n",
        "        if len(tokens_a) == 0:\n",
        "            hcf = np.concatenate((hcf_zero, hcf_zero, hcf_b, hcf_zero))\n",
        "        else:\n",
        "            hcf = np.concatenate(\n",
        "                (hcf_zero, hcf_a, hcf_zero, hcf_b, hcf_zero)\n",
        "                \n",
        "            )\n",
        "        \n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        segment_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
        "        input_mask = [1] * len(input_ids)\n",
        "            \n",
        "        acoustic_padding = np.zeros(\n",
        "            (args.max_seq_length - len(input_ids), acoustic.shape[1])\n",
        "        )\n",
        "        acoustic = np.concatenate((acoustic, acoustic_padding))\n",
        "        acoustic=np.take(acoustic, acoustic_features_list,axis=1)\n",
        "        \n",
        "        visual_padding = np.zeros(\n",
        "            (args.max_seq_length - len(input_ids), visual.shape[1])\n",
        "        )\n",
        "        visual = np.concatenate((visual, visual_padding))\n",
        "        visual = np.take(visual, visual_features_list,axis=1)\n",
        "        \n",
        "        \n",
        "        hcf_padding= np.zeros(\n",
        "            (args.max_seq_length - len(input_ids), hcf.shape[1])\n",
        "        )\n",
        "        \n",
        "        hcf = np.concatenate((hcf, hcf_padding))\n",
        "        \n",
        "        padding = [0] * (args.max_seq_length - len(input_ids))\n",
        "\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == args.max_seq_length\n",
        "        assert len(input_mask) == args.max_seq_length\n",
        "        assert len(segment_ids) == args.max_seq_length\n",
        "        assert acoustic.shape[0] == args.max_seq_length\n",
        "        assert visual.shape[0] == args.max_seq_length\n",
        "        assert hcf.shape[0] == args.max_seq_length\n",
        "        \n",
        "        label_id = float(label)\n",
        "        \n",
        "        \n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids,\n",
        "                input_mask=input_mask,\n",
        "                segment_ids=segment_ids,\n",
        "                visual=visual,\n",
        "                acoustic=acoustic,\n",
        "                hcf=hcf,\n",
        "                label_id=label_id,\n",
        "            )\n",
        "        )\n",
        "            \n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "def get_appropriate_dataset(data, tokenizer, parition):\n",
        "    \n",
        "\n",
        "    features = convert_humor_to_features(data, tokenizer)\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    all_visual = torch.tensor([f.visual for f in features], dtype=torch.float)\n",
        "    all_acoustic = torch.tensor([f.acoustic for f in features], dtype=torch.float)\n",
        "    hcf = torch.tensor([f.hcf for f in features], dtype=torch.float)\n",
        "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
        "    \n",
        "\n",
        "    dataset = TensorDataset(\n",
        "        all_input_ids,\n",
        "        all_visual,\n",
        "        all_acoustic,\n",
        "        all_input_mask,\n",
        "        all_segment_ids,\n",
        "        hcf,\n",
        "        all_label_ids,\n",
        "    )\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "\n",
        "def set_up_data_loader():\n",
        "    if args.dataset==\"humor\":\n",
        "        data_file = \"ur_funny.pkl\"\n",
        "    elif args.dataset==\"sarcasm\":\n",
        "        # data_file = \"/content/drive/MyDrive/Colab Notebooks/32/HKT/dataset/mustard.pkl\"\n",
        "        data_file = \"/content/drive/MyDrive/Colab Notebooks/32/HKT/dataset/our_mustard_split_final.p\"\n",
        "        # data_file = \"mustard.pkl\"\n",
        "        \n",
        "    with open(\n",
        "        # os.path.join(DATASET_LOCATION, args.dataset, data_file),\n",
        "        data_file,\n",
        "        \"rb\",\n",
        "    ) as handle:\n",
        "        all_data = pickle.load(handle)\n",
        "    train_data = all_data[\"train\"]\n",
        "    dev_data = all_data[\"dev\"]\n",
        "    test_data = all_data[\"test\"]\n",
        "\n",
        "    print('Train data : ', len(train_data))\n",
        "    print('Dev data : ', len(dev_data))\n",
        "    print('Test data : ', len(test_data))\n",
        "\n",
        "    tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "\n",
        "    train_dataset = get_appropriate_dataset(train_data, tokenizer, \"train\")\n",
        "    dev_dataset = get_appropriate_dataset(dev_data, tokenizer, \"dev\")\n",
        "    test_dataset = get_appropriate_dataset(test_data, tokenizer, \"test\")\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=1\n",
        "    )\n",
        "\n",
        "    dev_dataloader = DataLoader(\n",
        "        dev_dataset, batch_size=args.batch_size, shuffle=False, num_workers=1\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=1\n",
        "    )\n",
        "    \n",
        "    return (train_dataloader, dev_dataloader, test_dataloader)\n",
        "\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    \n",
        "    if args.model == \"HKT\" :\n",
        "        \n",
        "        if args.dataset==\"humor\":\n",
        "            visual_model = Transformer(VISUAL_DIM, num_layers=7, nhead=3, dim_feedforward= 128)\n",
        "            acoustic_model = Transformer(ACOUSTIC_DIM, num_layers=8, nhead=3, dim_feedforward = 256)\n",
        "            hcf_model = Transformer(HCF_DIM, num_layers=3, nhead=2, dim_feedforward = 128)\n",
        "            text_model = AlbertModel.from_pretrained('albert-base-v2')\n",
        "            model = HKT(text_model, visual_model, acoustic_model,hcf_model, args)\n",
        "            model.load_state_dict(torch.load(\"./model_weights/best/humor/humorHKT.pt\"))\n",
        "        elif args.dataset==\"sarcasm\":\n",
        "            visual_model = Transformer(VISUAL_DIM, num_layers=8, nhead=4, dim_feedforward=1024)\n",
        "            acoustic_model = Transformer(ACOUSTIC_DIM, num_layers=1, nhead=3, dim_feedforward=512)\n",
        "            hcf_model = Transformer(HCF_DIM, num_layers=8, nhead=4, dim_feedforward=128)    \n",
        "            text_model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
        "            model = HKT(text_model, visual_model, acoustic_model, hcf_model, args)\n",
        "            # model.load_state_dict(torch.load(\"./model_weights/best/sarcasm/sarcasmHKT.pt\"))\n",
        "            # model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/model_weights/best/sarcasm/sarcasmHKT.pt\"))\n",
        "            model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/saved_model/epoch_7_f1_0.6720807726075505\"))\n",
        "            \n",
        "    \n",
        "    \n",
        "    model.to(DEVICE)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def test_epoch(model, data_loader, loss_fct):\n",
        "    \"\"\" Epoch operation in evaluation phase \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(\n",
        "            data_loader, mininterval=2, desc=\"  - (Validation)   \", leave=False\n",
        "        ):\n",
        "            batch = tuple(t.to(DEVICE) for t in batch)\n",
        "\n",
        "            (\n",
        "                input_ids,\n",
        "                visual,\n",
        "                acoustic,\n",
        "                input_mask,\n",
        "                segment_ids,\n",
        "                hcf,\n",
        "                label_ids\n",
        "            ) = batch\n",
        "                    \n",
        "            visual = torch.squeeze(visual, 1)\n",
        "            acoustic = torch.squeeze(acoustic, 1)\n",
        "\n",
        "            \n",
        "            if args.model == \"HKT\":\n",
        "                outputs = model(input_ids, visual, acoustic,hcf, token_type_ids=segment_ids, attention_mask=input_mask,)\n",
        "            \n",
        "            logits = outputs[0]\n",
        "            \n",
        "            \n",
        "            tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "            \n",
        "            logits = torch.sigmoid(logits)\n",
        "            \n",
        "            \n",
        "            if len(preds) == 0:\n",
        "                preds=logits.detach().cpu().numpy()\n",
        "                all_labels=label_ids.detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "                all_labels = np.append(\n",
        "                    all_labels, label_ids.detach().cpu().numpy(), axis=0\n",
        "                )\n",
        "            \n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        \n",
        "        preds = np.squeeze(preds)\n",
        "        all_labels = np.squeeze(all_labels)\n",
        "\n",
        "    return preds, all_labels, eval_loss\n",
        "\n",
        "def test_score_model(model, test_data_loader, loss_fct, exclude_zero=False):\n",
        "\n",
        "    predictions, y_test, test_loss = test_epoch(model, test_data_loader, loss_fct)\n",
        "    \n",
        "    predictions = predictions.round()\n",
        "\n",
        "    f_score = f1_score(y_test, predictions, average=\"weighted\")\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    print(\"Accuracy, F score\", accuracy, f_score)\n",
        "    return accuracy, f_score, y_test, predictions\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    \"\"\"\n",
        "    This function controls the randomness by setting seed in all the libraries we will use.\n",
        "    \"\"\"\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def main():\n",
        "   \n",
        "   set_random_seed(args.seed)\n",
        "   (\n",
        "        train_data_loader,\n",
        "        dev_data_loader,\n",
        "        test_data_loader,\n",
        "    ) = set_up_data_loader()\n",
        "   \n",
        "   model = get_model()\n",
        "   print(\"loaded\")\n",
        "   loss_fct = BCEWithLogitsLoss()\n",
        "   acc, f_score, test_gold, test_pred = test_score_model(model, test_data_loader, loss_fct)\n",
        "   dev_acc, dev_f_score, dev_gold, dev_pred = test_score_model(model, dev_data_loader, loss_fct)\n",
        "  #  acc, f_score = test_score_model(model, train_data_loader, loss_fct)\n",
        "   print('test accuracy : ', acc)\n",
        "   print('test f_score : ', f_score)\n",
        "\n",
        "   print('dev accuracy : ', dev_acc)\n",
        "   print('dev f score : ', dev_f_score)\n",
        "   \n",
        "   return test_gold, test_pred\n",
        "if __name__ == \"__main__\":\n",
        "    test_gold, test_pred = main()\n"
      ],
      "metadata": {
        "id": "P92CnHND5Yne",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "ee78220d8fa94b07a8039e909e59cd45",
            "6eefb0b83c0e42bca141f1d6b2232205",
            "98e2162572254dc491110c4fbb03a6a4",
            "aa82db9aef42486d8cc80b06ccecda51",
            "6b347dae670245939cd8c896784baeed",
            "43ea9d0aeaf942e4850d1f0572d12bc0",
            "b53c9960bdec470bad7efe07f872174b",
            "4372c74bbfea462fa7025415dd582e3f",
            "460fd763a36445f6a12f3d35ad374fcb",
            "dea6bc3d035e4b0f96394905dc8bb970",
            "41662771d3eb4fe7a45bb421dfc3c303",
            "c00a28e18ea149098c13e6109a941116",
            "4e9d472775f3473dabca50db53d8542a",
            "26ef5a0e52e84159800e5a87a6710a2f",
            "e0a149a9b0f54144bc7c7a9209cdac37",
            "263b28ed50504afc8eb595e7545038f7",
            "5e70e23e7f054bf08bd5983d7bd14f3c",
            "c0055876f39049edb404c17d13a0440f",
            "5ed5aad5ef8f49788fd2e0534c362985",
            "6e1e694bac1a4656a1ce884de073eb7f",
            "2da6bcc911974189a4b2eb98cf3add2b",
            "24db70fdcbed4234b7f1c14c17f7ee85",
            "ece487748b1f434784282a0825c23797",
            "dcdc9ed0744c4cc693b13c4616e7b3dc",
            "d2b6c342af43412285972f5752b0b1c4",
            "34d9059989f74876b63d4c34cdff9ead",
            "488b49976fd647c2999f496e40cb02ac",
            "a14925a9da3443e7a500b4e0438abc6c",
            "fb86522349c64781b2a888d5263b8f27",
            "1a76c6c2ca844c25ba802e0dca4f13ff",
            "0fb69e947aaf43f69ed3506f60ea3d1f",
            "3f6340b082524929bdcca9bb6a023d37",
            "188d09b27efc4273af6b326d2e522382"
          ]
        },
        "outputId": "39159f8a-03a4-464d-ddf0-c20122296b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data :  540\n",
            "Dev data :  68\n",
            "Test data :  67\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/760k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee78220d8fa94b07a8039e909e59cd45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-861261a25c63>:183: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  all_visual = torch.tensor([f.visual for f in features], dtype=torch.float)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c00a28e18ea149098c13e6109a941116"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ece487748b1f434784282a0825c23797"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy, F score 0.6716417910447762 0.6720807726075505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy, F score 0.75 0.7501622323166774\n",
            "test accuracy :  0.6716417910447762\n",
            "test f_score :  0.6720807726075505\n",
            "dev accuracy :  0.75\n",
            "dev f score :  0.7501622323166774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/32/HKT/dataset/our_mustard_split_final.p\", 'rb') as f:\n",
        "  dataset = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "MNI_Y2OP5yUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxuVmA0j7fNj",
        "outputId": "d771f7ee-9030-467c-e75a-2f437fc5b573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = dataset['test']\n",
        "len(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgER80eD72Pl",
        "outputId": "a20976e1-3379-48dd-c858-1f9a028b0751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "Q9wZv15dBvL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(test_gold, test_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h249_xAADHjH",
        "outputId": "4b29debc-826f-46c6-93d6-5ccc87fbb608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[22,  8],\n",
              "       [14, 23]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==0) and (test_gold[j]==1)):\n",
        "    print(j, \" : \", test_dataset[j][2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExDZ_dkV8AiH",
        "outputId": "d8412370-a1e0-4117-d007-930df005e562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7  :  2_15\n",
            "9  :  2_550\n",
            "10  :  1_3707\n",
            "12  :  1_971\n",
            "13  :  2_504\n",
            "15  :  1_12202\n",
            "21  :  1_8052\n",
            "24  :  1_8136\n",
            "31  :  2_573\n",
            "32  :  1_4967\n",
            "35  :  2_24\n",
            "39  :  2_282\n",
            "53  :  2_548\n",
            "57  :  1_5571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==1) and (test_gold[j]==0)):\n",
        "    print(j, \" : \", test_dataset[j][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8eXMNjyCShQ",
        "outputId": "f341fbb1-4daf-44de-d2c9-4f894835fa3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11  :  1_10190\n",
            "14  :  2_221\n",
            "19  :  1_5786\n",
            "34  :  1_11609\n",
            "47  :  1_3348\n",
            "50  :  2_125\n",
            "51  :  2_8\n",
            "63  :  2_467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==0) and (test_gold[j]==0)):\n",
        "    print(j, \" : \", test_dataset[j][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtO8l358DWaf",
        "outputId": "1335d1ed-ac8a-4557-86f4-b6ea8f332106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1  :  2_445\n",
            "3  :  2_180\n",
            "4  :  2_162\n",
            "5  :  1_2423\n",
            "8  :  2_205\n",
            "16  :  2_481\n",
            "23  :  2_13\n",
            "25  :  2_181\n",
            "26  :  2_387\n",
            "30  :  2_278\n",
            "33  :  2_44\n",
            "37  :  2_243\n",
            "41  :  2_210\n",
            "42  :  1_467\n",
            "46  :  2_439\n",
            "48  :  2_590\n",
            "49  :  1_7442\n",
            "56  :  2_134\n",
            "58  :  2_594\n",
            "60  :  2_353\n",
            "61  :  2_426\n",
            "65  :  2_253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==1) and (test_gold[j]==1)):\n",
        "    print(j, \" : \", test_dataset[j][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVm3OotUF_cx",
        "outputId": "69073432-1e32-4d73-e777-26a85a3e1ae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  :  1_6426\n",
            "2  :  2_508\n",
            "6  :  2_260\n",
            "17  :  1_3064\n",
            "18  :  1_8827\n",
            "20  :  2_390\n",
            "22  :  1_7357\n",
            "27  :  2_430\n",
            "28  :  2_140\n",
            "29  :  1_1973\n",
            "36  :  1_10748\n",
            "38  :  1_7281\n",
            "40  :  1_537\n",
            "43  :  1_213\n",
            "44  :  2_351\n",
            "45  :  1_7047\n",
            "52  :  1_6683\n",
            "54  :  1_3545\n",
            "55  :  2_141\n",
            "59  :  2_440\n",
            "62  :  2_491\n",
            "64  :  2_182\n",
            "66  :  1_105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Replace me by any text you'd like.\"\n",
        "# encoded_input = tokenizer(text, return_tensors='pt')\n"
      ],
      "metadata": {
        "id": "gV9YSOYH4iDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoded_input.keys()"
      ],
      "metadata": {
        "id": "gMdcX31e4kD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output = model(**encoded_input)"
      ],
      "metadata": {
        "id": "fHo3Hlrn4l3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output.keys()"
      ],
      "metadata": {
        "id": "F1Bo8sar4pR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a,b  = model(input_ids = encoded_input['input_ids'], attention_mask = encoded_input['attention_mask'], token_type_ids = encoded_input['token_type_ids'])"
      ],
      "metadata": {
        "id": "sVxKiyM84qPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a"
      ],
      "metadata": {
        "id": "j0RQRHYt5BzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b"
      ],
      "metadata": {
        "id": "bXlT3Djb5JVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oceRZyU15Ony"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}