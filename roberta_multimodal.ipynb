{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WHwHkm4rGCLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58629636-f673-48e2-94db-f9f4be2392b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VFg6mNX7Gdxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "227261f1-15da-40de-afb6-bdcd61601cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 21 19:41:18 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    49W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZQU-d0knGyhl"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import warnings\n",
        "import logging\n",
        "import gc\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "from typing import Optional\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import jaccard_score, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "suRa-hBHG_21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c4f01b7-9b29-4427-f897-5d584255a171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r_Ovlo42G4Kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e24c52-1ba7-479f-f5f2-0a6f359ff6bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "# from rouge_score.rouge_scorer import RougeScorer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from transformers import (\n",
        "    BartTokenizerFast,\n",
        "    AdamW\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print(\"Using GPU\")\n",
        "\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "foldNum = 0\n",
        "\n",
        "\n",
        "\n",
        "SOURCE_MAX_LEN = 500\n",
        "# TARGET_MAX_LEN = 50\n",
        "# MAX_UTTERANCES = 25\n",
        "\n",
        "ACOUSTIC_DIM = 768\n",
        "ACOUSTIC_MAX_LEN = 1000\n",
        "\n",
        "\n",
        "\n",
        "VISUAL_DIM = 2048\n",
        "VISUAL_MAX_LEN = 480\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "# LEARNING_RATE = 1e-5\n",
        "# LEARNING_RATE = 1e-3\n",
        "\n",
        "\n",
        "# VALID_LEN = 69\n",
        "\n",
        "# BASE_LEARNING_RATE = 5e-6\n",
        "# NEW_LEARNING_RATE = 5e-5\n",
        "# WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# NUM_BEAMS = 5\n",
        "# EARLY_STOPPING = True\n",
        "# NO_REPEAT_NGRAM_SIZE = 3\n",
        "\n",
        "# EARLY_STOPPING_THRESHOLD = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cMdRaTfAHF7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917deef8-eb3b-4a24-fae7-a972901e3df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed : 994\n"
          ]
        }
      ],
      "source": [
        "def set_random_seed(seed: int):\n",
        "    print(\"Seed : {}\".format(seed))\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# set_random_seed(42)\n",
        "# set_random_seed(123)\n",
        "# set_random_seed(12345)\n",
        "set_random_seed(994)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "from transformers.modeling_utils import PreTrainedModel, unwrap_model\n",
        "\n",
        "from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
        "# from transformers.models.roberta.tokenization_roberta import RobertaTokenizer\n",
        "from transformers.models.roberta.modeling_roberta import RobertaLayer, RobertaEmbeddings, RobertaPooler, RobertaPreTrainedModel\n",
        "# from transformers.models.roberta.modeling_roberta import RobertaEmbeddings, RobertaPreTrainedModel\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions\n",
        ")\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
        "from transformers.activations import ACT2FN"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jMtoMDKiVk0r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MuNh3gFEHQhY"
      },
      "outputs": [],
      "source": [
        "# from transformer_encoder import TransformerEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "-ML9b3x4axdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3feffe99-1469-45e9-bf6e-220e8a93a259"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_tokenizer  = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "# bert_model = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "# # bert_model"
      ],
      "metadata": {
        "id": "OtwuoBrcwzUN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e1XR3BlgHOY8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aEDR_7N6HWsu"
      },
      "outputs": [],
      "source": [
        "class ContextAwareAttention(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim_model : int,\n",
        "                 dim_context : int,\n",
        "                 dropout_rate : Optional[float] = 0.0 ):\n",
        "\n",
        "        super(ContextAwareAttention, self).__init__()\n",
        "\n",
        "        self.dim_model = dim_model\n",
        "        self.dim_context = dim_context\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attention_layer = nn.MultiheadAttention(embed_dim=self.dim_model,\n",
        "                                                     num_heads = 1,\n",
        "                                                     dropout = self.dropout_rate,\n",
        "                                                     bias = True,\n",
        "                                                    add_zero_attn=False,\n",
        "                                                    batch_first=True,\n",
        "                                                    device=DEVICE   \n",
        "        )\n",
        "\n",
        "        self.u_k = nn.Linear(self.dim_context, self.dim_model, bias = False)\n",
        "        self.w1_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "        self.w2_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "\n",
        "        self.u_v = nn.Linear(self.dim_context, self.dim_model, bias=False)\n",
        "        self.w1_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "        self.w2_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "\n",
        "    def forward(self, q, k, v, context):\n",
        "\n",
        "        # print(\"Context shape : \", context.shape)\n",
        "        # print(\"Dim context : \", self.dim_context, \" : Dim model : \", self.dim_model)\n",
        "        key_context = self.u_k(context)\n",
        "        # print(\"Context shape below key context : \", key_context.shape)\n",
        "        value_context = self.u_v(context)\n",
        "\n",
        "        lambda_k = F.sigmoid(self.w1_k(k) + self.w2_k(key_context))\n",
        "        lambda_v = F.sigmoid(self.w1_v(v) + self.w2_v(value_context))\n",
        "\n",
        "        k_cap = (1-lambda_k) * k + (lambda_k) * key_context\n",
        "        v_cap = (1-lambda_v) * v + (lambda_v) * value_context\n",
        "\n",
        "        attention_output, _ = self.attention_layer(query = q,\n",
        "                                                   key = k_cap,\n",
        "                                                   value = v_cap)\n",
        "  \n",
        "        return attention_output                                     \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MAF_acoustic(nn.Module):\n",
        "    def __init__(self, \n",
        "                dim_model,\n",
        "                dropout_rate):\n",
        "        super(MAF_acoustic, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "        # self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "        self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "                                                                dim_context=ACOUSTIC_DIM,\n",
        "                                                                dropout_rate=dropout_rate)\n",
        "\n",
        "        # self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "        #                                                     dim_context=VISUAL_DIM,\n",
        "        #                                                     dropout_rate=dropout_rate)\n",
        "\n",
        "        self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        # self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "        self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "    def forward(self,\n",
        "                text_input,\n",
        "                acoustic_context):\n",
        "\n",
        "        # print(\"Acoustic context shape (A) : \", acoustic_context.shape)        \n",
        "\n",
        "        acoustic_context = acoustic_context.permute(0,2,1)\n",
        "        acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "        acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "        audio_out = self.acoustic_context_attention(q=text_input,\n",
        "                                                    k=text_input,\n",
        "                                                    v=text_input,\n",
        "                                                    context=acoustic_context)\n",
        "        # print(\"Audio out (A) : \", audio_out.shape) \n",
        "\n",
        "        # print(\"Visual context shape : \", visual_context.shape)\n",
        "        # visual_context = visual_context.permute(0,2,1)\n",
        "        # visual_context = self.visual_context_transform(visual_context.float())\n",
        "        # visual_context = visual_context.permute(0,2,1)\n",
        "        \n",
        "        # video_out = self.visual_context_attention(q=text_input,\n",
        "        #                                             k=text_input,\n",
        "        #                                             v=text_input,\n",
        "        #                                             context=visual_context)\n",
        "\n",
        "        # print(\"Video out shape : \", video_out.shape)\n",
        "        # print(\"Text input shape : \", text_input.shape)\n",
        "        weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "        # weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "        # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "        output = self.final_layer_norm(text_input + weight_a * audio_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "2ZPn4U3Weq9I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAF_visual(nn.Module):\n",
        "    def __init__(self, \n",
        "                dim_model,\n",
        "                dropout_rate):\n",
        "        super(MAF_visual, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "        self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "        # self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "        #                                                         dim_context=ACOUSTIC_DIM,\n",
        "        #                                                         dropout_rate=dropout_rate)\n",
        "\n",
        "        self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "                                                            dim_context=VISUAL_DIM,\n",
        "                                                            dropout_rate=dropout_rate)\n",
        "\n",
        "        # self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "        self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "    def forward(self,\n",
        "                text_input,\n",
        "                visual_context):\n",
        "\n",
        "        # print(\"Acoustic context shape (A) : \", acoustic_context.shape)        \n",
        "\n",
        "        # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "        # acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "        # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "        # audio_out = self.acoustic_context_attention(q=text_input,\n",
        "        #                                             k=text_input,\n",
        "        #                                             v=text_input,\n",
        "        #                                             context=acoustic_context)\n",
        "        # print(\"Audio out (A) : \", audio_out.shape) \n",
        "\n",
        "        # print(\"Visual context shape : \", visual_context.shape)\n",
        "        visual_context = visual_context.permute(0,2,1)\n",
        "        visual_context = self.visual_context_transform(visual_context.float())\n",
        "        visual_context = visual_context.permute(0,2,1)\n",
        "        \n",
        "        video_out = self.visual_context_attention(q=text_input,\n",
        "                                                    k=text_input,\n",
        "                                                    v=text_input,\n",
        "                                                    context=visual_context)\n",
        "\n",
        "        # print(\"Video out shape : \", video_out.shape)\n",
        "        # print(\"Text input shape : \", text_input.shape)\n",
        "        # weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "        weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "        # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "        # q3 = weight_v * video_out\n",
        "        # print('weight_v shape : ', weight_v.shape)\n",
        "        # print('weight_v * video_out shape : ', q3.shape)\n",
        "\n",
        "        output = self.final_layer_norm(text_input  + weight_v * video_out)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "ZeZdIlcker1V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "s88m19VoHbj5"
      },
      "outputs": [],
      "source": [
        "class MultimodalRobertaEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.gradient_checkpointing = False\n",
        "        \n",
        "        self.fusion_at_layer9 = [8]\n",
        "        # self.fusion_at_layer10 = [11]\n",
        "\n",
        "        # self.fusion_at_layer9 = [19]\n",
        "        # self.fusion_at_layer10 = [20]\n",
        "\n",
        "        # self.MAF_layer9 = MAF_acoustic(dim_model=self.config.hidden_size,\n",
        "        #                      dropout_rate=0.2)\n",
        "        \n",
        "        # self.MAF_layer10 = MAF_visual(dim_model=self.config.hidden_size,\n",
        "        #                      dropout_rate=0.2)\n",
        "\n",
        "        self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "        self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "        self.acoustic_dim = nn.Linear(ACOUSTIC_DIM, 768, bias = False)\n",
        "        self.visual_dim = nn.Linear(VISUAL_DIM, 768, bias = False)\n",
        "\n",
        "        self.concat_linear = nn.Linear(3*768, 768, bias = False)\n",
        "        \n",
        "    def forward(self,\n",
        "               hidden_states : torch.Tensor,\n",
        "               attention_mask: Optional[torch.FloatTensor] = None,\n",
        "               acoustic_input: Optional[torch.FloatTensor] = None,\n",
        "               visual_input: Optional[torch.FloatTensor] = None, \n",
        "               head_mask: Optional[torch.FloatTensor] = None,\n",
        "               encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "               encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "               past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "               use_cache: Optional[bool] = False,\n",
        "               output_attentions: Optional[bool] = False,\n",
        "               output_hidden_states: Optional[bool] = False,\n",
        "               return_dict: Optional[bool] = True\n",
        "               ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
        "        \n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "        \n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            # print(\"i : \", i)\n",
        "            if i in self.fusion_at_layer9:\n",
        "                    # print(\"Inside layer 9\")\n",
        "                    # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "                    # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]   \n",
        "                    # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "                    # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "                    # print(\"====Idx inside fusion at layer :\", idx)\n",
        "                    \n",
        "                      acoustic_input = acoustic_input.permute(0,2,1)\n",
        "                      acoustic_input = self.acoustic_context_transform(acoustic_input.float())\n",
        "                      acoustic_input = acoustic_input.permute(0,2,1)\n",
        "\n",
        "                      acoustic_input = self.acoustic_dim(acoustic_input)\n",
        "\n",
        "                      # print(\"acoustic_input shape : \", acoustic_input.shape)\n",
        "\n",
        "                      visual_input = visual_input.permute(0,2,1)\n",
        "                      visual_input = self.visual_context_transform(visual_input.float())\n",
        "                      visual_input = visual_input.permute(0,2,1)\n",
        "\n",
        "                      visual_input = self.visual_dim(visual_input)\n",
        "\n",
        "                      # print(\"visual input shape : \", visual_input.shape)\n",
        "                      concat = torch.concat([hidden_states, acoustic_input, visual_input], dim = -1)\n",
        "\n",
        "                      # print(\"concat shape : \", concat.shape)\n",
        "\n",
        "                      hidden_states = self.concat_linear(concat)\n",
        "\n",
        "                      # print('hidden states shape : ', hidden_states.shape)\n",
        "            # if i in self.fusion_at_layer10:\n",
        "            #         # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "            #         # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]   \n",
        "            #         # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "            #         # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "            #         # print(\"====Idx inside fusion at layer :\", idx)\n",
        "            #         hidden_states = self.MAF_layer10(text_input = hidden_states,\n",
        "            #                                        visual_context = visual_input)  \n",
        "        \n",
        "           \n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "            \n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "            \n",
        "            if self.gradient_checkpointing and self.training:\n",
        "                \n",
        "                if use_cache:\n",
        "                    # logger.warning(\n",
        "                    #     \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "                    # )\n",
        "                    use_cache = False\n",
        "                \n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "                    \n",
        "                    return custom_forward\n",
        "                \n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask\n",
        "                )\n",
        "                \n",
        "            else:\n",
        "                # print(\"hidden states shape : \", hidden_states.shape)\n",
        "                # print('attention_mask shape : ', attention_mask.shape)\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                    \n",
        "                \n",
        "                )\n",
        "                \n",
        "            hidden_states = layer_outputs[0]\n",
        "            \n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "         \n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "            \n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                    \n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        \n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state = hidden_states,\n",
        "            past_key_values = next_decoder_cache,\n",
        "            hidden_states = all_hidden_states,\n",
        "            attentions = all_self_attentions,\n",
        "            cross_attentions = all_cross_attentions\n",
        "            \n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6F_nfly0HgKR"
      },
      "outputs": [],
      "source": [
        "class MultiModalRobertaClassification(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, num_classes)\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = torch.relu(hidden_states)\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        return hidden_states\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalRobertaModel(RobertaPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        \n",
        "        super().__init__(config)\n",
        "        \n",
        "        self.config = config\n",
        "        \n",
        "        self.embeddings = RobertaEmbeddings(config)\n",
        "        self.encoder = MultimodalRobertaEncoder(config)\n",
        "        # self.encoder = RobertaEncoder(config)\n",
        "        \n",
        "        self.output = MultiModalRobertaClassification(config.hidden_size, num_classes = 2)\n",
        "        \n",
        "        self.post_init()\n",
        "    \n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "   \n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "    \n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "            \n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        acoustic_input: Optional[torch.Tensor] = None,\n",
        "        visual_input: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None\n",
        "    )   -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
        "        \n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.output_hidden_states\n",
        "        \n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "        \n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError('You can not  specify both input_ids and input_embeds at the same time')\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError('You have to specify either input_ids or inputs_embeds')\n",
        "        \n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "        \n",
        "        past_key_value_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "        \n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_value_length)), device = device)\n",
        "        \n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, 'token_type_ids'):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype = torch.long, device = device)\n",
        "        \n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
        "\n",
        "        # print(\"attention mask shape : \", attention_mask.shape)\n",
        "        # print(\"extended attention mask shape : \", extended_attention_mask.shape)\n",
        "        \n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device = device)\n",
        "            \n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        \n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "        \n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "        \n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids = input_ids,\n",
        "            position_ids = position_ids,\n",
        "            token_type_ids = token_type_ids,\n",
        "            inputs_embeds = inputs_embeds,\n",
        "            past_key_values_length = past_key_value_length\n",
        "        )\n",
        "        \n",
        "        # print(\"attention mask shape 2 : \", attention_mask.shape)\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask = extended_attention_mask,\n",
        "            acoustic_input = acoustic_input,\n",
        "            visual_input = visual_input,\n",
        "            head_mask = head_mask,\n",
        "            encoder_hidden_states = encoder_hidden_states,\n",
        "            encoder_attention_mask = encoder_extended_attention_mask,\n",
        "            past_key_values = past_key_values,\n",
        "            use_cache = use_cache,\n",
        "            output_attentions = output_attentions,\n",
        "            output_hidden_states = output_hidden_states,\n",
        "            return_dict = return_dict\n",
        "        )\n",
        "        \n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.output(sequence_output) \n",
        "        \n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "        \n",
        "        # print(\"pooled output shape : \", pooled_output.shape)\n",
        "        # print(\"labels shape : \", labels.shape)\n",
        "        pooled_output = pooled_output[:, 0, :]\n",
        "        loss = loss_fn(pooled_output, labels)\n",
        "        \n",
        "        temp_dict = {}\n",
        "        \n",
        "        temp_dict['logits'] = pooled_output\n",
        "        temp_dict['loss'] = loss\n",
        "        \n",
        "        return temp_dict\n",
        "        "
      ],
      "metadata": {
        "id": "es1L0jZkUFTj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "m4lcNczUHpOH"
      },
      "outputs": [],
      "source": [
        "# def audio_video_broadcast(x):\n",
        "# #     z = torch.empty()\n",
        "#     temp_all = torch.Tensor()\n",
        "#     for j in range(x.shape[0]):\n",
        "#         print(\"j : \", j)\n",
        "#         temp_x = x[j,:]\n",
        "# #         print(\"Temp x shape : \", temp_x.shape)\n",
        "#         temp_x = torch.tensor(temp_x, dtype=torch.float)\n",
        "#         temp_x = torch.broadcast_to(temp_x, (SOURCE_MAX_LEN, temp_x.shape[0]))\n",
        "# #         print(\"Temp x shape : \", temp_x.shape)\n",
        "#         temp_x = temp_x.unsqueeze(0)\n",
        "# #         print(\"Temp x shape : \", temp_x.shape)\n",
        "        \n",
        "#         if(j==0):\n",
        "#             temp_all = temp_x\n",
        "#         else:\n",
        "#             temp_all = torch.cat([temp_all, temp_x], dim = 0)\n",
        "        \n",
        "        \n",
        "#     return temp_all "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "inWKX8K6GV50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60f9381d-4e77-46af-9692-aec6150569f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "foldNum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/train_audio_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  train_audio_data_utterance1 = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "RunFr5b0Ni1u"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_audio_data_utterance1)"
      ],
      "metadata": {
        "id": "pGFk4HCwOo-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ced09e8-a065-4612-a1ea-08ec4c5c466b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_audio_data_utterance1)"
      ],
      "metadata": {
        "id": "sK4GbT8ZOq2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14e060c-5a97-436a-e643-913e00a174e4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "552"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X73Hp5HxOsEl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yM1zXLalOifj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/test_audio_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  test_audio_data_utterance1 = pickle.load(f)"
      ],
      "metadata": {
        "id": "tvBQbRyFN5zu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_audio_data_utterance1)"
      ],
      "metadata": {
        "id": "ZctmFGmNQsMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3820537-488c-4950-ffcf-dfa01f498b74"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_audio_data_utterance1)"
      ],
      "metadata": {
        "id": "Dz2xdgV7PzbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e5ca3a-c2db-49ab-9aa3-520bf8a77e4e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/train_video_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  train_image_data_utterance1 = pickle.load(f)"
      ],
      "metadata": {
        "id": "Yw0sc0aaNzOO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_image_data_utterance1)"
      ],
      "metadata": {
        "id": "0yT8QM77QvJS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cdf16ff-c64a-4a6a-e8fe-c4570c9f7db0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "552"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/test_video_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  test_image_data_utterance1 = pickle.load(f)"
      ],
      "metadata": {
        "id": "aUEvmMBNOPFE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_image_data_utterance1)"
      ],
      "metadata": {
        "id": "gm6Z6Fc0QxSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deeb1e34-7df9-4402-b337-e2ece1709714"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tp = torch.ones(4,5)\n",
        "tp"
      ],
      "metadata": {
        "id": "KIEMeHJ6RoXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4da293d-509b-4b79-d9df-a02cc95a01c9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros(5 - tp.shape[0], 5)"
      ],
      "metadata": {
        "id": "o8vsmq4FS48j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d37cad7a-4dfc-4bc2-f221-6e5702ac4742"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat([tp, torch.zeros(5 - tp.shape[0], 5)])"
      ],
      "metadata": {
        "id": "6CNdKCeQRrN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c29b1b-f974-478f-db7e-a95063fbe44a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_seq(tensor, dim, max_len):\n",
        "  if max_len > tensor.shape[0] :\n",
        "    return torch.cat([tensor, torch.zeros(max_len - tensor.shape[0], dim)])\n",
        "  else:\n",
        "    return tensor[:max_len]  "
      ],
      "metadata": {
        "id": "qO9elV8EQAi1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ACOUSTIC_DIM"
      ],
      "metadata": {
        "id": "HmBt6xZ1TElw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f30eb99-94dc-4087-a2d1-d297a8967d95"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_audio_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = ACOUSTIC_DIM,\n",
        "                                                   max_len = ACOUSTIC_MAX_LEN)\n",
        "                                                  for a in train_audio_data_utterance1], 0)\n",
        "train_audio_data_utterance1.shape"
      ],
      "metadata": {
        "id": "4cUPe2LxQ04S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1c1aa1-0d57-4595-8db6-f956d809e4f6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552, 1000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_audio_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = ACOUSTIC_DIM,\n",
        "                                                   max_len = ACOUSTIC_MAX_LEN)\n",
        "                                                  for a in test_audio_data_utterance1], 0)\n",
        "test_audio_data_utterance1.shape"
      ],
      "metadata": {
        "id": "x6-frFIpTzrW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e43bc65-7794-4a7b-c76f-d871f6cb28ee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138, 1000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VISUAL_DIM"
      ],
      "metadata": {
        "id": "WEDVn1TaUicC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec9b35f-e011-4cfb-bb30-611fce940902"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2048"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VISUAL_MAX_LEN"
      ],
      "metadata": {
        "id": "7ArX7k9VUkWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc869ab-8937-4869-940b-e52410e5182f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "480"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = VISUAL_DIM,\n",
        "                                                   max_len = VISUAL_MAX_LEN)\n",
        "                                                  for a in train_image_data_utterance1], 0)\n",
        "train_image_data_utterance1.shape"
      ],
      "metadata": {
        "id": "PO2OLh5yUTF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ce9ddb-e233-4f46-d5d1-76f6868fe0d9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552, 480, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = VISUAL_DIM,\n",
        "                                                   max_len = VISUAL_MAX_LEN)\n",
        "                                                  for a in test_image_data_utterance1], 0)\n",
        "test_image_data_utterance1.shape"
      ],
      "metadata": {
        "id": "9BAAQoCcUv34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fd86a1-ef2e-442d-f085-91b02120cdb0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138, 480, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTMNyPhSUdBs"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "j3V8UO6HH0xE"
      },
      "outputs": [],
      "source": [
        "# path = \"/content/drive/MyDrive/Colab Notebooks/32/datasetTrue_original/sarcasmDataset_speaker_dependent_True.npz\"\n",
        "# data2 = np.load(path, mmap_mode=True)   \n",
        "\n",
        "# train_audio_data_utterance1 = data2['feautesUA_train'][foldNum]\n",
        "# train_image_data_utterance1 = data2['feautesUV_train'][foldNum]\n",
        "\n",
        "# test_audio_data_utterance1 = data2['feautesUA_test'][foldNum]\n",
        "# test_image_data_utterance1 = data2['feautesUV_test'][foldNum]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "pdf5hGlVH5Zw"
      },
      "outputs": [],
      "source": [
        "# model =  MultiModalRobertaModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
        "# print(model)\n",
        "\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# print(tokenizer)\n",
        "\n",
        "# num_param = sum(p.numel() for p in model.parameters())\n",
        "# print(\"Total parameters : \", num_param/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiModalRobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "\n",
        "\n",
        "print(model)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "print(tokenizer)\n",
        "\n",
        "num_param = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total trainanable parameters : \", num_param/1e6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3d636d685b494381b413c4e2d6229957",
            "8bf053b245084502a06d28e009fedb2c",
            "209d4c2c076c42aaabff98794d6b4b59",
            "38f4d50ddbd54929aaaf51960ca59a04",
            "a2516511c98249e5ac16f50729717ddb",
            "cabece2f70a24383b19a98ec160faf1a",
            "621eab262d33456e904c69e95a706907",
            "f1e634e5de7b46f287708f39e54cb666",
            "84f92592d052475a9f316db702a4e960",
            "2dfa8862676a4b11b7e0e50cef5d2114",
            "11506f543bb04da1ae5e7323f7dd82be",
            "b4790bb39f3e443b9880bbe156fccf1d",
            "cd71035656de42d1b5e5f1fbb583bfd2",
            "961fd4c230bd4aedb8d72e13d87cfb72",
            "e6a0304da41a4a5bafb1cdbed7b5f2b5",
            "90e170f25dfb4c3b8058206e52ff5f33",
            "b24d2bb0c217470a9254b640f7cbcc0b",
            "2038067354f6439d8c14c8de0fe64384",
            "63e098d2ef77446a962a9065dd899b43",
            "1e17f398d6854d7d955c2bae73a53e7f",
            "4751fd1595a84005b576f293ce5a0e49",
            "dfb5bd0a91e04e81aacf85052915b3ea",
            "b9b78963f9bb4a02a5f9747b1ceb5fdf",
            "16de6d4a6b3b449faf69e13c05a5d6c6",
            "107e2bb9a6d04dc38fc7bf5019cbd20f",
            "d00c602bfce34e31a54f61d19a049db8",
            "bff24afddd4941a2b0f1fda7651dd12d",
            "8cba9a7e16fd427f838a734f676c7664",
            "1a620d98c374446d92c790d382c9892b",
            "6ac438395055426ba0975622fb17f46d",
            "1856dfa6c88f405a9ce2fa8cedc11f83",
            "e6da7cb773264502bcdd1aff73e38522",
            "10f58335a5fb44e88bcdf5e336bc0f78",
            "9edc64bfadff41c197533a25305bcf7b",
            "b21c8a0d4d6f4da38f156cab7b4033b1",
            "8ecf4c3ac2414608a1a2254b51297898",
            "f016e6274bb94583a859ffa991357d6f",
            "706ca8702c094c6aa1edeb982f1babbb",
            "c93daebb86f74a3cabf94cbfa0bf4460",
            "75d35d6ce16b48629fc0fb33fbbbb844",
            "8c27099814e2401d9d13949fcb150460",
            "6ce1ad69ab6b45a0bb870d01d04d0c71",
            "248bac307c324cec855bac7809325415",
            "627f00aa067744d9a2a0d720679f30a8"
          ]
        },
        "id": "nYjq5Tz4ceZa",
        "outputId": "0988a367-dff5-4afb-db89-c1a9a9edc1c9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d636d685b494381b413c4e2d6229957"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4790bb39f3e443b9880bbe156fccf1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing MultiModalRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing MultiModalRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MultiModalRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of MultiModalRobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'roberta.encoder.concat_linear.weight', 'roberta.encoder.visual_dim.weight', 'roberta.encoder.acoustic_context_transform.weight', 'roberta.output.dense.weight', 'roberta.encoder.visual_context_transform.weight', 'roberta.encoder.acoustic_dim.weight', 'roberta.output.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiModalRobertaModel(\n",
            "  (embeddings): RobertaEmbeddings(\n",
            "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "    (token_type_embeddings): Embedding(1, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): MultimodalRobertaEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0): RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): RobertaLayer(\n",
            "        (attention): RobertaAttention(\n",
            "          (self): RobertaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): RobertaSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): RobertaIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): RobertaOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (acoustic_context_transform): Linear(in_features=1000, out_features=500, bias=False)\n",
            "    (visual_context_transform): Linear(in_features=480, out_features=500, bias=False)\n",
            "    (acoustic_dim): Linear(in_features=768, out_features=768, bias=False)\n",
            "    (visual_dim): Linear(in_features=2048, out_features=768, bias=False)\n",
            "    (concat_linear): Linear(in_features=2304, out_features=768, bias=False)\n",
            "  )\n",
            "  (output): MultiModalRobertaClassification(\n",
            "    (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9b78963f9bb4a02a5f9747b1ceb5fdf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9edc64bfadff41c197533a25305bcf7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaTokenizer(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n",
            "Total trainanable parameters :  128.728738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7t2C-xFIQXla"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "n_N4HcMQ9hXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a22c22-3f8e-47b1-d427-c76546ffe379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count :  0  name :  embeddings.word_embeddings.weight\n",
            "Count :  1  name :  embeddings.position_embeddings.weight\n",
            "Count :  2  name :  embeddings.token_type_embeddings.weight\n",
            "Count :  3  name :  embeddings.LayerNorm.weight\n",
            "Count :  4  name :  embeddings.LayerNorm.bias\n",
            "Count :  5  name :  encoder.layer.0.attention.self.query.weight\n",
            "Count :  6  name :  encoder.layer.0.attention.self.query.bias\n",
            "Count :  7  name :  encoder.layer.0.attention.self.key.weight\n",
            "Count :  8  name :  encoder.layer.0.attention.self.key.bias\n",
            "Count :  9  name :  encoder.layer.0.attention.self.value.weight\n",
            "Count :  10  name :  encoder.layer.0.attention.self.value.bias\n",
            "Count :  11  name :  encoder.layer.0.attention.output.dense.weight\n",
            "Count :  12  name :  encoder.layer.0.attention.output.dense.bias\n",
            "Count :  13  name :  encoder.layer.0.attention.output.LayerNorm.weight\n",
            "Count :  14  name :  encoder.layer.0.attention.output.LayerNorm.bias\n",
            "Count :  15  name :  encoder.layer.0.intermediate.dense.weight\n",
            "Count :  16  name :  encoder.layer.0.intermediate.dense.bias\n",
            "Count :  17  name :  encoder.layer.0.output.dense.weight\n",
            "Count :  18  name :  encoder.layer.0.output.dense.bias\n",
            "Count :  19  name :  encoder.layer.0.output.LayerNorm.weight\n",
            "Count :  20  name :  encoder.layer.0.output.LayerNorm.bias\n",
            "Count :  21  name :  encoder.layer.1.attention.self.query.weight\n",
            "Count :  22  name :  encoder.layer.1.attention.self.query.bias\n",
            "Count :  23  name :  encoder.layer.1.attention.self.key.weight\n",
            "Count :  24  name :  encoder.layer.1.attention.self.key.bias\n",
            "Count :  25  name :  encoder.layer.1.attention.self.value.weight\n",
            "Count :  26  name :  encoder.layer.1.attention.self.value.bias\n",
            "Count :  27  name :  encoder.layer.1.attention.output.dense.weight\n",
            "Count :  28  name :  encoder.layer.1.attention.output.dense.bias\n",
            "Count :  29  name :  encoder.layer.1.attention.output.LayerNorm.weight\n",
            "Count :  30  name :  encoder.layer.1.attention.output.LayerNorm.bias\n",
            "Count :  31  name :  encoder.layer.1.intermediate.dense.weight\n",
            "Count :  32  name :  encoder.layer.1.intermediate.dense.bias\n",
            "Count :  33  name :  encoder.layer.1.output.dense.weight\n",
            "Count :  34  name :  encoder.layer.1.output.dense.bias\n",
            "Count :  35  name :  encoder.layer.1.output.LayerNorm.weight\n",
            "Count :  36  name :  encoder.layer.1.output.LayerNorm.bias\n",
            "Count :  37  name :  encoder.layer.2.attention.self.query.weight\n",
            "Count :  38  name :  encoder.layer.2.attention.self.query.bias\n",
            "Count :  39  name :  encoder.layer.2.attention.self.key.weight\n",
            "Count :  40  name :  encoder.layer.2.attention.self.key.bias\n",
            "Count :  41  name :  encoder.layer.2.attention.self.value.weight\n",
            "Count :  42  name :  encoder.layer.2.attention.self.value.bias\n",
            "Count :  43  name :  encoder.layer.2.attention.output.dense.weight\n",
            "Count :  44  name :  encoder.layer.2.attention.output.dense.bias\n",
            "Count :  45  name :  encoder.layer.2.attention.output.LayerNorm.weight\n",
            "Count :  46  name :  encoder.layer.2.attention.output.LayerNorm.bias\n",
            "Count :  47  name :  encoder.layer.2.intermediate.dense.weight\n",
            "Count :  48  name :  encoder.layer.2.intermediate.dense.bias\n",
            "Count :  49  name :  encoder.layer.2.output.dense.weight\n",
            "Count :  50  name :  encoder.layer.2.output.dense.bias\n",
            "Count :  51  name :  encoder.layer.2.output.LayerNorm.weight\n",
            "Count :  52  name :  encoder.layer.2.output.LayerNorm.bias\n",
            "Count :  53  name :  encoder.layer.3.attention.self.query.weight\n",
            "Count :  54  name :  encoder.layer.3.attention.self.query.bias\n",
            "Count :  55  name :  encoder.layer.3.attention.self.key.weight\n",
            "Count :  56  name :  encoder.layer.3.attention.self.key.bias\n",
            "Count :  57  name :  encoder.layer.3.attention.self.value.weight\n",
            "Count :  58  name :  encoder.layer.3.attention.self.value.bias\n",
            "Count :  59  name :  encoder.layer.3.attention.output.dense.weight\n",
            "Count :  60  name :  encoder.layer.3.attention.output.dense.bias\n",
            "Count :  61  name :  encoder.layer.3.attention.output.LayerNorm.weight\n",
            "Count :  62  name :  encoder.layer.3.attention.output.LayerNorm.bias\n",
            "Count :  63  name :  encoder.layer.3.intermediate.dense.weight\n",
            "Count :  64  name :  encoder.layer.3.intermediate.dense.bias\n",
            "Count :  65  name :  encoder.layer.3.output.dense.weight\n",
            "Count :  66  name :  encoder.layer.3.output.dense.bias\n",
            "Count :  67  name :  encoder.layer.3.output.LayerNorm.weight\n",
            "Count :  68  name :  encoder.layer.3.output.LayerNorm.bias\n",
            "Count :  69  name :  encoder.layer.4.attention.self.query.weight\n",
            "Count :  70  name :  encoder.layer.4.attention.self.query.bias\n",
            "Count :  71  name :  encoder.layer.4.attention.self.key.weight\n",
            "Count :  72  name :  encoder.layer.4.attention.self.key.bias\n",
            "Count :  73  name :  encoder.layer.4.attention.self.value.weight\n",
            "Count :  74  name :  encoder.layer.4.attention.self.value.bias\n",
            "Count :  75  name :  encoder.layer.4.attention.output.dense.weight\n",
            "Count :  76  name :  encoder.layer.4.attention.output.dense.bias\n",
            "Count :  77  name :  encoder.layer.4.attention.output.LayerNorm.weight\n",
            "Count :  78  name :  encoder.layer.4.attention.output.LayerNorm.bias\n",
            "Count :  79  name :  encoder.layer.4.intermediate.dense.weight\n",
            "Count :  80  name :  encoder.layer.4.intermediate.dense.bias\n",
            "Count :  81  name :  encoder.layer.4.output.dense.weight\n",
            "Count :  82  name :  encoder.layer.4.output.dense.bias\n",
            "Count :  83  name :  encoder.layer.4.output.LayerNorm.weight\n",
            "Count :  84  name :  encoder.layer.4.output.LayerNorm.bias\n",
            "Count :  85  name :  encoder.layer.5.attention.self.query.weight\n",
            "Count :  86  name :  encoder.layer.5.attention.self.query.bias\n",
            "Count :  87  name :  encoder.layer.5.attention.self.key.weight\n",
            "Count :  88  name :  encoder.layer.5.attention.self.key.bias\n",
            "Count :  89  name :  encoder.layer.5.attention.self.value.weight\n",
            "Count :  90  name :  encoder.layer.5.attention.self.value.bias\n",
            "Count :  91  name :  encoder.layer.5.attention.output.dense.weight\n",
            "Count :  92  name :  encoder.layer.5.attention.output.dense.bias\n",
            "Count :  93  name :  encoder.layer.5.attention.output.LayerNorm.weight\n",
            "Count :  94  name :  encoder.layer.5.attention.output.LayerNorm.bias\n",
            "Count :  95  name :  encoder.layer.5.intermediate.dense.weight\n",
            "Count :  96  name :  encoder.layer.5.intermediate.dense.bias\n",
            "Count :  97  name :  encoder.layer.5.output.dense.weight\n",
            "Count :  98  name :  encoder.layer.5.output.dense.bias\n",
            "Count :  99  name :  encoder.layer.5.output.LayerNorm.weight\n",
            "Count :  100  name :  encoder.layer.5.output.LayerNorm.bias\n",
            "Count :  101  name :  encoder.layer.6.attention.self.query.weight\n",
            "Count :  102  name :  encoder.layer.6.attention.self.query.bias\n",
            "Count :  103  name :  encoder.layer.6.attention.self.key.weight\n",
            "Count :  104  name :  encoder.layer.6.attention.self.key.bias\n",
            "Count :  105  name :  encoder.layer.6.attention.self.value.weight\n",
            "Count :  106  name :  encoder.layer.6.attention.self.value.bias\n",
            "Count :  107  name :  encoder.layer.6.attention.output.dense.weight\n",
            "Count :  108  name :  encoder.layer.6.attention.output.dense.bias\n",
            "Count :  109  name :  encoder.layer.6.attention.output.LayerNorm.weight\n",
            "Count :  110  name :  encoder.layer.6.attention.output.LayerNorm.bias\n",
            "Count :  111  name :  encoder.layer.6.intermediate.dense.weight\n",
            "Count :  112  name :  encoder.layer.6.intermediate.dense.bias\n",
            "Count :  113  name :  encoder.layer.6.output.dense.weight\n",
            "Count :  114  name :  encoder.layer.6.output.dense.bias\n",
            "Count :  115  name :  encoder.layer.6.output.LayerNorm.weight\n",
            "Count :  116  name :  encoder.layer.6.output.LayerNorm.bias\n",
            "Count :  117  name :  encoder.layer.7.attention.self.query.weight\n",
            "Count :  118  name :  encoder.layer.7.attention.self.query.bias\n",
            "Count :  119  name :  encoder.layer.7.attention.self.key.weight\n",
            "Count :  120  name :  encoder.layer.7.attention.self.key.bias\n",
            "Count :  121  name :  encoder.layer.7.attention.self.value.weight\n",
            "Count :  122  name :  encoder.layer.7.attention.self.value.bias\n",
            "Count :  123  name :  encoder.layer.7.attention.output.dense.weight\n",
            "Count :  124  name :  encoder.layer.7.attention.output.dense.bias\n",
            "Count :  125  name :  encoder.layer.7.attention.output.LayerNorm.weight\n",
            "Count :  126  name :  encoder.layer.7.attention.output.LayerNorm.bias\n",
            "Count :  127  name :  encoder.layer.7.intermediate.dense.weight\n",
            "Count :  128  name :  encoder.layer.7.intermediate.dense.bias\n",
            "Count :  129  name :  encoder.layer.7.output.dense.weight\n",
            "Count :  130  name :  encoder.layer.7.output.dense.bias\n",
            "Count :  131  name :  encoder.layer.7.output.LayerNorm.weight\n",
            "Count :  132  name :  encoder.layer.7.output.LayerNorm.bias\n",
            "Count :  133  name :  encoder.layer.8.attention.self.query.weight\n",
            "Count :  134  name :  encoder.layer.8.attention.self.query.bias\n",
            "Count :  135  name :  encoder.layer.8.attention.self.key.weight\n",
            "Count :  136  name :  encoder.layer.8.attention.self.key.bias\n",
            "Count :  137  name :  encoder.layer.8.attention.self.value.weight\n",
            "Count :  138  name :  encoder.layer.8.attention.self.value.bias\n",
            "Count :  139  name :  encoder.layer.8.attention.output.dense.weight\n",
            "Count :  140  name :  encoder.layer.8.attention.output.dense.bias\n",
            "Count :  141  name :  encoder.layer.8.attention.output.LayerNorm.weight\n",
            "Count :  142  name :  encoder.layer.8.attention.output.LayerNorm.bias\n",
            "Count :  143  name :  encoder.layer.8.intermediate.dense.weight\n",
            "Count :  144  name :  encoder.layer.8.intermediate.dense.bias\n",
            "Count :  145  name :  encoder.layer.8.output.dense.weight\n",
            "Count :  146  name :  encoder.layer.8.output.dense.bias\n",
            "Count :  147  name :  encoder.layer.8.output.LayerNorm.weight\n",
            "Count :  148  name :  encoder.layer.8.output.LayerNorm.bias\n",
            "Count :  149  name :  encoder.layer.9.attention.self.query.weight\n",
            "Count :  150  name :  encoder.layer.9.attention.self.query.bias\n",
            "Count :  151  name :  encoder.layer.9.attention.self.key.weight\n",
            "Count :  152  name :  encoder.layer.9.attention.self.key.bias\n",
            "Count :  153  name :  encoder.layer.9.attention.self.value.weight\n",
            "Count :  154  name :  encoder.layer.9.attention.self.value.bias\n",
            "Count :  155  name :  encoder.layer.9.attention.output.dense.weight\n",
            "Count :  156  name :  encoder.layer.9.attention.output.dense.bias\n",
            "Count :  157  name :  encoder.layer.9.attention.output.LayerNorm.weight\n",
            "Count :  158  name :  encoder.layer.9.attention.output.LayerNorm.bias\n",
            "Count :  159  name :  encoder.layer.9.intermediate.dense.weight\n",
            "Count :  160  name :  encoder.layer.9.intermediate.dense.bias\n",
            "Count :  161  name :  encoder.layer.9.output.dense.weight\n",
            "Count :  162  name :  encoder.layer.9.output.dense.bias\n",
            "Count :  163  name :  encoder.layer.9.output.LayerNorm.weight\n",
            "Count :  164  name :  encoder.layer.9.output.LayerNorm.bias\n",
            "Count :  165  name :  encoder.layer.10.attention.self.query.weight\n",
            "Count :  166  name :  encoder.layer.10.attention.self.query.bias\n",
            "Count :  167  name :  encoder.layer.10.attention.self.key.weight\n",
            "Count :  168  name :  encoder.layer.10.attention.self.key.bias\n",
            "Count :  169  name :  encoder.layer.10.attention.self.value.weight\n",
            "Count :  170  name :  encoder.layer.10.attention.self.value.bias\n",
            "Count :  171  name :  encoder.layer.10.attention.output.dense.weight\n",
            "Count :  172  name :  encoder.layer.10.attention.output.dense.bias\n",
            "Count :  173  name :  encoder.layer.10.attention.output.LayerNorm.weight\n",
            "Count :  174  name :  encoder.layer.10.attention.output.LayerNorm.bias\n",
            "Count :  175  name :  encoder.layer.10.intermediate.dense.weight\n",
            "Count :  176  name :  encoder.layer.10.intermediate.dense.bias\n",
            "Count :  177  name :  encoder.layer.10.output.dense.weight\n",
            "Count :  178  name :  encoder.layer.10.output.dense.bias\n",
            "Count :  179  name :  encoder.layer.10.output.LayerNorm.weight\n",
            "Count :  180  name :  encoder.layer.10.output.LayerNorm.bias\n",
            "Count :  181  name :  encoder.layer.11.attention.self.query.weight\n",
            "Count :  182  name :  encoder.layer.11.attention.self.query.bias\n",
            "Count :  183  name :  encoder.layer.11.attention.self.key.weight\n",
            "Count :  184  name :  encoder.layer.11.attention.self.key.bias\n",
            "Count :  185  name :  encoder.layer.11.attention.self.value.weight\n",
            "Count :  186  name :  encoder.layer.11.attention.self.value.bias\n",
            "Count :  187  name :  encoder.layer.11.attention.output.dense.weight\n",
            "Count :  188  name :  encoder.layer.11.attention.output.dense.bias\n",
            "Count :  189  name :  encoder.layer.11.attention.output.LayerNorm.weight\n",
            "Count :  190  name :  encoder.layer.11.attention.output.LayerNorm.bias\n",
            "Count :  191  name :  encoder.layer.11.intermediate.dense.weight\n",
            "Count :  192  name :  encoder.layer.11.intermediate.dense.bias\n",
            "Count :  193  name :  encoder.layer.11.output.dense.weight\n",
            "Count :  194  name :  encoder.layer.11.output.dense.bias\n",
            "Count :  195  name :  encoder.layer.11.output.LayerNorm.weight\n",
            "Count :  196  name :  encoder.layer.11.output.LayerNorm.bias\n",
            "Count :  197  name :  encoder.acoustic_context_transform.weight\n",
            "Count :  198  name :  encoder.visual_context_transform.weight\n",
            "Count :  199  name :  encoder.acoustic_dim.weight\n",
            "Count :  200  name :  encoder.visual_dim.weight\n",
            "Count :  201  name :  encoder.concat_linear.weight\n",
            "Count :  202  name :  output.dense.weight\n",
            "Count :  203  name :  output.dense.bias\n"
          ]
        }
      ],
      "source": [
        "cnt = 0\n",
        "for name, param in model.named_parameters():\n",
        "    print(\"Count : \", cnt, \" name : \", name)\n",
        "    cnt+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "feA-7jSwIBQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e7a4ef-a0fc-448d-d678-737c30f869e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count :  197  name :  encoder.acoustic_context_transform.weight\n",
            "Count :  198  name :  encoder.visual_context_transform.weight\n",
            "Count :  199  name :  encoder.acoustic_dim.weight\n",
            "Count :  200  name :  encoder.visual_dim.weight\n",
            "Count :  201  name :  encoder.concat_linear.weight\n",
            "Count :  202  name :  output.dense.weight\n",
            "Count :  203  name :  output.dense.bias\n",
            "Total trainanable parameters :  4.673698\n"
          ]
        }
      ],
      "source": [
        "cnt = 0\n",
        "for name, param in model.named_parameters():\n",
        "    \n",
        "    if(cnt>=197):\n",
        "    # if(cnt>=389):  \n",
        "    \n",
        "        param.requires_grad = True\n",
        "        print(\"Count : \", cnt, \" name : \", name)\n",
        "\n",
        "    else:\n",
        "        param.requires_grad = False    \n",
        "    cnt+=1\n",
        "        \n",
        "\n",
        "num_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total trainanable parameters : \", num_param/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "foldNum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amBD8a4_rhUM",
        "outputId": "4e7224e1-7040-47b4-c24f-269fb598ffcf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1NGerbQcymT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/32/json_file_fold.p\", \"rb\") as f:\n",
        "    text_file = pickle.load(f)\n",
        "\n",
        "train_text = text_file[\"json_file_list_\" + str(foldNum) +\"_train\"]\n",
        "test_text =  text_file[\"json_file_list_\" + str(foldNum) +\"_test\"]"
      ],
      "metadata": {
        "id": "46OM_4HZjKNp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainlen = 483"
      ],
      "metadata": {
        "id": "HaH0ocF_zD0O"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_audio_data_utterance1))\n",
        "print(train_audio_data_utterance1.shape)"
      ],
      "metadata": {
        "id": "sL8tkEu3kCrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3be0b774-485f-4238-a765-236c42dfbd6a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([552, 1000, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_train_data = []\n",
        "# for j in range(len(train_text)):\n",
        "#   temp_text = train_text[j]\n",
        "#   temp_audio = train_audio_data_utterance1[j]\n",
        "#   temp_image = train_image_data_utterance1[j]\n",
        "\n",
        "#   temp_list = []\n",
        "#   temp_list.append(temp_text)\n",
        "#   temp_list.append(temp_audio)\n",
        "#   temp_list.append(temp_image)\n",
        "\n",
        "#   combined_train_data.append(temp_list)\n",
        "\n",
        "# random.shuffle(combined_train_data)"
      ],
      "metadata": {
        "id": "DRRQRZeBNYum"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_text2 = []\n",
        "# train_audio_utterance2 = []\n",
        "# train_image_utterance2 = []\n",
        "\n",
        "# for j in combined_train_data:\n",
        "#   train_text2.append(j[0])\n",
        "#   train_audio_utterance2.append(j[1])\n",
        "#   train_image_utterance2.append(j[2])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZGx2mZfPOWjo"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(train_text2))\n",
        "# print(len(train_audio_utterance2))\n",
        "# print(len(train_image_utterance2))"
      ],
      "metadata": {
        "id": "Qe5xfQYURWrs"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_audio_data_utterance2 = torch.tensor(train_audio_utterance2)\n",
        "# train_audio_data_utterance2.shape"
      ],
      "metadata": {
        "id": "nPa_FWppRfpn"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_image_data_utterance2 = torch.tensor(train_image_utterance2)\n",
        "# train_image_data_utterance2.shape"
      ],
      "metadata": {
        "id": "uuHaeA7JiRQz"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_audio_data_utterance = torch.tensor(train_audio_data_utterance2)[:Trainlen]\n",
        "# # train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# train_audio_data_utterance.shape"
      ],
      "metadata": {
        "id": "mQCW6oZTifhd"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_audio_data_utterance = torch.tensor(train_audio_data_utterance1)\n",
        "# train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "train_audio_data_utterance.shape"
      ],
      "metadata": {
        "id": "984ZSUXVWQfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a44f06-e544-43f4-c5d7-ad8bd8bb0bdc"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552, 1000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_audio_data_utterance = torch.tensor(train_audio_data_utterance1)[:Trainlen]\n",
        "# # train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# train_audio_data_utterance.shape"
      ],
      "metadata": {
        "id": "eYe20QnlkJwg"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_image_data_utterance = torch.tensor(train_image_data_utterance2)[:Trainlen]\n",
        "# # train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "# train_image_data_utterance.shape"
      ],
      "metadata": {
        "id": "VLqtw_6-irl3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_data_utterance = torch.tensor(train_image_data_utterance1)\n",
        "# train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "train_image_data_utterance.shape"
      ],
      "metadata": {
        "id": "AleXnjQNWTKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9cde06b-830f-488c-b553-575824396540"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552, 480, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_image_data_utterance = torch.tensor(train_image_data_utterance1)[:Trainlen]\n",
        "# # train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "# train_image_data_utterance.shape"
      ],
      "metadata": {
        "id": "G1NXg6uikX5P"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_audio_data_utterance = torch.tensor(train_audio_data_utterance2)[Trainlen:]\n",
        "# valid_audio_data_utterance.shape"
      ],
      "metadata": {
        "id": "YkC6buI-iu39"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_text)"
      ],
      "metadata": {
        "id": "NU5wt_vv998E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e003c3a-034f-42f2-8d4d-d746d514df5a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_data_utterance1.shape"
      ],
      "metadata": {
        "id": "F4yP2ap7Pl27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e3dbfdc-eb4f-49eb-d0fa-17f47945212f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138, 480, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_audio_data_utterance1.shape"
      ],
      "metadata": {
        "id": "g5rke66XPvU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5dbad2f-8799-42ec-d2d0-989c590313b6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138, 1000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_test_data = []\n",
        "\n",
        "# for j in range(len(test_text)):\n",
        "#   temp_text = test_text[j]\n",
        "#   temp_audio = test_audio_data_utterance1[j]\n",
        "#   temp_image = test_image_data_utterance1[j]\n",
        "\n",
        "#   temp_list = []\n",
        "#   temp_list.append(temp_text)\n",
        "#   temp_list.append(temp_audio)\n",
        "#   temp_list.append(temp_image)\n",
        "\n",
        "#   combined_test_data.append(temp_list)\n",
        "\n",
        "# random.shuffle(combined_test_data)\n"
      ],
      "metadata": {
        "id": "fyi5s9iS9jej"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_text2 = []\n",
        "# test_audio_utterance2 = []\n",
        "# test_image_utterance2 = []\n",
        "\n",
        "# for j in combined_test_data:\n",
        "#   test_text2.append(j[0])\n",
        "#   test_audio_utterance2.append(j[1])\n",
        "#   test_image_utterance2.append(j[2])\n"
      ],
      "metadata": {
        "id": "YvgGLs8S-cW9"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDLEN = 69"
      ],
      "metadata": {
        "id": "UgCracq6--vg"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(test_audio_utterance2)"
      ],
      "metadata": {
        "id": "UarPmcdo_uaU"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_audio_utterance2[0].shape"
      ],
      "metadata": {
        "id": "CgOQVaRV_-iV"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.stack(test_audio_utterance2).shape"
      ],
      "metadata": {
        "id": "AsLJeYkvAdoK"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_audio_data_utterance = test_audio_data_utterance1[:VALIDLEN]\n",
        "# valid_audio_data_utterance.shape"
      ],
      "metadata": {
        "id": "bDznHLJbzAbl"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_audio_data_utterance = test_audio_data_utterance1[VALIDLEN:]\n",
        "# test_audio_data_utterance.shape"
      ],
      "metadata": {
        "id": "6XNjZNpBAYd_"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_image_data_utterance = torch.tensor(train_image_data_utterance2)[Trainlen:]\n",
        "# valid_image_data_utterance.shape"
      ],
      "metadata": {
        "id": "Y-kqcb9tiyB8"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_image_data_utterance = test_image_data_utterance1[:VALIDLEN]\n",
        "# valid_image_data_utterance.shape"
      ],
      "metadata": {
        "id": "z8KNyWBm2Ie4"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_image_data_utterance = test_image_data_utterance1[VALIDLEN:]\n",
        "# test_image_data_utterance.shape"
      ],
      "metadata": {
        "id": "B5rPLXq6At1F"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_audio_data_utterance = torch.tensor(test_audio_data_utterance1)\n",
        "# # test_audio_data_utterance = test_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# test_audio_data_utterance.shape"
      ],
      "metadata": {
        "id": "YCxFywiVko_v"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_image_data_utterance = torch.tensor(test_image_data_utterance1)\n",
        "# # test_image_data_utterance = test_image_data_utterance.unsqueeze(dim = 1)\n",
        "# test_image_data_utterance.shape"
      ],
      "metadata": {
        "id": "hxqP1M_2k2Y_"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "PKt4UobTIJQY"
      },
      "outputs": [],
      "source": [
        "   \n",
        "\n",
        "# # print(len(text_file[train_text]))\n",
        "# # print(train_text)\n",
        "# # print(len(text_file[test_text]))\n",
        "\n",
        "# train_audio_broadcast_utterance = audio_video_broadcast(train_audio_data_utterance)\n",
        "\n",
        "# print(\"train_audio_broadcast_utterance complete : \", train_audio_broadcast_utterance.shape)\n",
        "# train_image_broadcast_utterance = audio_video_broadcast(train_image_data_utterance)\n",
        "# print(\"train_image_broadcast_utterance complete : \",train_image_broadcast_utterance.shape)\n",
        "\n",
        "# valid_audio_broadcast_utterance = audio_video_broadcast(valid_audio_data_utterance)\n",
        "# print(\"valid_audio_broadcast_utterance complete : \", valid_audio_broadcast_utterance.shape)\n",
        "\n",
        "# valid_image_broadcast_utterance = audio_video_broadcast(valid_image_data_utterance)\n",
        "# print('valid_image_broadcast_utterance complete : ', valid_image_broadcast_utterance.shape)\n",
        "\n",
        "# test_audio_broadcast_utterance = audio_video_broadcast(test_audio_data_utterance)\n",
        "# print(\"test_audio_broadcast_utterance complete : \",test_audio_broadcast_utterance.shape)\n",
        "# test_image_broadcast_utterance = audio_video_broadcast(test_image_data_utterance)\n",
        "# print(\"test_image_broadcast_utterance complete : \",test_image_broadcast_utterance.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer"
      ],
      "metadata": {
        "id": "9Ik_vBo7x6Rm"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# p = {\n",
        "#         'additional_special_tokens' : ['[CONTEXT]', '[UTTERANCE]']\n",
        "#     }\n",
        "\n",
        "# tokenizer.add_special_tokens(p)\n",
        "\n",
        "# tokenizer"
      ],
      "metadata": {
        "id": "X0y5DYklxuUT"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "P1Ctmwokx-T2"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = {\n",
        "        'additional_special_tokens' : ['[CONTEXT]', '[UTTERANCE]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(p)\n",
        "\n",
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uExwN4zRe0Wq",
        "outputId": "45120b7a-c3a4-46d0-e696-1ea35d2de19d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaTokenizer(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[CONTEXT]', '[UTTERANCE]']})"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer2"
      ],
      "metadata": {
        "id": "XRsujb2Nmwks"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_h3zgFvfrGo",
        "outputId": "f62a3537-794f-4070-bcea-458a34efa587"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50267, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg1kq4a5e3Cp",
        "outputId": "5ad24fac-eebd-4704-bb32-26965e791e12"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaTokenizer(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[CONTEXT]', '[UTTERANCE]']})"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def prepare_dataset_context(text_data):\n",
        "                    \n",
        "\n",
        "#             context = []\n",
        "#             # labels = []\n",
        "#             for i in range(len(text_data)):\n",
        "#                 data_point = text_data[i]\n",
        "\n",
        "#                 # example_speaker = data_point['speaker']\n",
        "#                 # example_utterance = data_point['utterance']\n",
        "#                 # temp_label = int(data_point['sarcasm'])\n",
        "\n",
        "#                 # example_context = '[CONTEXT] '\n",
        "#                 example_context = ''\n",
        "\n",
        "#                 temp_len = len(data_point['context_speakers'])\n",
        "#                 cnt = 0\n",
        "#                 print(\"Temp len : \", temp_len)\n",
        "#                 for speaker, utterance in list(zip(data_point['context_speakers'], data_point['context'])):\n",
        "#                     print(\"count : \", cnt)\n",
        "#                     if(cnt == temp_len - 1):\n",
        "#                       example_context = example_context + speaker.upper() + \" : \" + utterance\n",
        "#                     else:\n",
        "#                       example_context = example_context + speaker.upper() + \" : \" + utterance + \" , \"\n",
        "#                     cnt+=1\n",
        "\n",
        "                    \n",
        "\n",
        "                \n",
        "#                 # print(example_dialog)\n",
        "#                 example_context = re.sub(' +', ' ', example_context)\n",
        "\n",
        "#                 context.append(example_context)\n",
        "#                 # labels.append(temp_label)\n",
        "\n",
        "#             # df = pd.DataFrame(dialog, columns=['dialog'])\n",
        "\n",
        "#             # labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "            \n",
        "\n",
        "#             enc = bert_tokenizer(context, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "\n",
        "#             # df['audio_features'] = acoustic_data\n",
        "#             # df['visual_features'] = visual_data\n",
        "\n",
        "#             return torch.tensor(enc['input_ids'], dtype=torch.long), torch.tensor(enc['attention_mask'], dtype=torch.bool)\n"
      ],
      "metadata": {
        "id": "o7mKmBRp2XqP"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "HiN-BMSgISy7"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(text_data):\n",
        "                    \n",
        "\n",
        "            dialog = []\n",
        "            labels = []\n",
        "            for i in range(len(text_data)):\n",
        "                data_point = text_data[i]\n",
        "\n",
        "                example_speaker = data_point['speaker']\n",
        "                example_utterance = data_point['utterance']\n",
        "                temp_label = int(data_point['sarcasm'])\n",
        "\n",
        "                # example_dialog = '[CONTEXT] '\n",
        "                # example_dialog = '[TARGET] '\n",
        "                example_dialog = '[CONTEXT] '\n",
        "\n",
        "\n",
        "                for speaker, utterance in list(zip(data_point['context_speakers'], data_point['context'])):\n",
        "                    example_dialog = example_dialog + speaker.upper() + \" : \" + utterance + \" | \"\n",
        "\n",
        "                example_dialog = example_dialog + ' [UTTERANCE] ' + example_speaker + \" : \" + example_utterance + \" | \"\n",
        "                # example_dialog = example_dialog + example_speaker + \" : \" + example_utterance\n",
        "                # example_dialog = example_dialog + example_speaker + \" : \" + example_utterance \n",
        "                # print(example_dialog)\n",
        "                example_dialog = re.sub(' +', ' ', example_dialog)\n",
        "\n",
        "                dialog.append(example_dialog)\n",
        "                labels.append(temp_label)\n",
        "\n",
        "            # df = pd.DataFrame(dialog, columns=['dialog'])\n",
        "\n",
        "            labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "            \n",
        "\n",
        "            # enc = tokenizer(dialog, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "            enc = tokenizer(dialog, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "\n",
        "            # df['audio_features'] = acoustic_data\n",
        "            # df['visual_features'] = visual_data\n",
        "\n",
        "            return torch.tensor(enc['input_ids'], dtype=torch.long), torch.tensor(enc['attention_mask'], dtype=torch.bool), labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2dU7MfiZaDq"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "H7VlrC_WI9ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa5c8d3-2abf-4acb-93e1-ac851e3e1c73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaTokenizer(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[CONTEXT]', '[UTTERANCE]']})"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_text)"
      ],
      "metadata": {
        "id": "MgoPnVD4qfO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd81a6bb-daeb-45a2-8d85-a180ec44d661"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "552"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_input_ids1, train_text_attention_mask1, train_ground_truth1 = prepare_dataset(train_text)\n",
        "train_ground_truth1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7p229zVGBUG",
        "outputId": "22d430e5-31d1-453f-e560-318653080ed5"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "WdGsoVBDJxLw"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids1, train_text_attention_mask1, train_ground_truth1 = prepare_dataset_utterance(train_text)\n",
        "# train_ground_truth1.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_input_ids = train_text_input_ids1\n",
        "train_text_input_ids.shape"
      ],
      "metadata": {
        "id": "wQdk9Tk8WYzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f412d3-0625-4c33-832f-e4b522963393"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_text_input_ids = train_text_input_ids1[:Trainlen]\n",
        "# train_text_input_ids.shape"
      ],
      "metadata": {
        "id": "_rqaEYpWZMVx"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_attention_mask = train_text_attention_mask1\n",
        "train_text_attention_mask.shape"
      ],
      "metadata": {
        "id": "BIYgUkhfWcL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae5e13b-1a75-497c-ff2d-593ec5e8a1ce"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_text_attention_mask = train_text_attention_mask1[:Trainlen]\n",
        "# train_text_attention_mask.shape"
      ],
      "metadata": {
        "id": "MRRAuyVaaOun"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ground_truth = train_ground_truth1\n",
        "train_ground_truth.shape"
      ],
      "metadata": {
        "id": "q819IVudYtDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7670af0b-d9ba-4783-8f68-a7aafc4d365d"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_ground_truth = train_ground_truth1[:Trainlen]\n",
        "# train_ground_truth.shape"
      ],
      "metadata": {
        "id": "U8iuUm6vWeN-"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_ground_truth = train_ground_truth1[:Trainlen]\n",
        "# train_ground_truth.shape"
      ],
      "metadata": {
        "id": "pKXPrrh_jR_O"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_ground_truth"
      ],
      "metadata": {
        "id": "JAcvsTpeaeyB"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# context_input_ids, context_attention_mask = prepare_dataset_context(train_text)\n",
        "# print(context_input_ids.shape)"
      ],
      "metadata": {
        "id": "bctWHziiWqR0"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# context_attention_mask.shape"
      ],
      "metadata": {
        "id": "ScKZf52FX6AD"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "P0C9PnF9JypC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358a24c9-5ac7-4dfc-88dd-488c696da01c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TYPE : train_text_input_ids :  <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "print(\"TYPE : train_text_input_ids : \", type(train_text_input_ids))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# train_context_input_ids = context_input_ids\n",
        "# train_context_input_ids.shape"
      ],
      "metadata": {
        "id": "yzsrAIieWgvT"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# train_context_input_ids = context_input_ids[:Trainlen]\n",
        "# train_context_input_ids.shape"
      ],
      "metadata": {
        "id": "815UHYi_juEa"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_context_attention_mask = context_attention_mask\n",
        "# train_context_attention_mask.shape"
      ],
      "metadata": {
        "id": "KPScSL-aWiyC"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_context_attention_mask = context_attention_mask[:Trainlen]\n",
        "# train_context_attention_mask.shape"
      ],
      "metadata": {
        "id": "tEYf1vyblLxm"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_text_input_ids = train_text_input_ids1[Trainlen:]\n",
        "# valid_text_input_ids.shape"
      ],
      "metadata": {
        "id": "cZECJ_D6YwSF"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# valid_text_attention_mask = train_text_attention_mask1[Trainlen:]\n",
        "# valid_text_attention_mask.shape"
      ],
      "metadata": {
        "id": "6Iqag88ipliS"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_ground_truth = train_ground_truth1[Trainlen:]\n",
        "# valid_ground_truth.shape"
      ],
      "metadata": {
        "id": "bbjcYjztp523"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "NDIcnT9t-G1l"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids, valid_text_attention_mask, valid_ground_truth = prepare_dataset_utterance(valid_text)\n",
        "# valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_context_input_ids = context_input_ids[Trainlen:]\n",
        "# valid_context_input_ids.shape"
      ],
      "metadata": {
        "id": "NkfN5cn6YYoz"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_context_attention_mask = context_attention_mask[Trainlen:]\n",
        "# valid_context_attention_mask.shape"
      ],
      "metadata": {
        "id": "1ogVRlSWrgJf"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "AuNLAd5CIWLd"
      },
      "outputs": [],
      "source": [
        "# test_text_input_ids, test_text_attention_mask, test_ground_truth = prepare_dataset_utterance(test_text)\n",
        "# test_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text_input_ids, test_text_attention_mask, test_ground_truth = prepare_dataset(test_text)\n",
        "test_ground_truth.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vB39rShGL4K",
        "outputId": "9526ce88-362a-4026-e6d3-5ec08cb7995e"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_id = test_text_input_ids[:VALID_LEN]\n",
        "# valid_id.shape"
      ],
      "metadata": {
        "id": "6dIyrYyhGDFH"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_id = test_text_input_ids[VALIDLEN:]\n",
        "# test_id.shape"
      ],
      "metadata": {
        "id": "LmfvmZTEGbiQ"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_mask = test_text_attention_mask[:VALIDLEN]\n",
        "# valid_mask.shape"
      ],
      "metadata": {
        "id": "0Wi2Kyr5Gm52"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_mask = test_text_attention_mask[VALIDLEN:]\n",
        "# test_mask.shape"
      ],
      "metadata": {
        "id": "Z7V0wmGiHAB_"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_truth = test_ground_truth[:VALIDLEN]\n",
        "# valid_truth.shape"
      ],
      "metadata": {
        "id": "JFZQBCCTHFzt"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_truth = test_ground_truth[VALIDLEN:]\n",
        "# test_truth.shape"
      ],
      "metadata": {
        "id": "5wiu7X_lHWFi"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_context_input_ids, test_context_attention_mask = prepare_dataset_context(test_text)\n",
        "# test_context_input_ids.shape"
      ],
      "metadata": {
        "id": "E-ovTep1tWmK"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_context_id = test_context_input_ids[:VALIDLEN]\n",
        "# valid_context_id.shape"
      ],
      "metadata": {
        "id": "iMO8hPfyHkja"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_context_id = test_context_input_ids[VALIDLEN:]\n",
        "# test_context_id.shape"
      ],
      "metadata": {
        "id": "IG3A9ASzHwW3"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_context_attention_mask.shape"
      ],
      "metadata": {
        "id": "I1iZshp8tld_"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_context_mask = test_context_attention_mask[:VALIDLEN]\n",
        "# valid_context_mask.shape"
      ],
      "metadata": {
        "id": "doTkSyhKIMOt"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_context_mask = test_context_attention_mask[VALIDLEN:]\n",
        "# test_context_mask.shape"
      ],
      "metadata": {
        "id": "UGa0EsBVIpzX"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "_0kqUoFDKPkS"
      },
      "outputs": [],
      "source": [
        "# tokenizer.add_tokens(['[CONTEXT]', '[TARGET]'], special_tokens = True)\n",
        "# model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_audio_data_utterance1.shape"
      ],
      "metadata": {
        "id": "6EEHQlvrRM3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05f3bb9-0ba5-4321-c29b-2a0135f0aa3d"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138, 1000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_data_utterance1.shape"
      ],
      "metadata": {
        "id": "5PdHAFkTRsyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9fe961-bc87-4073-b942-8235f361f57a"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138, 480, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_text_input_ids.shape)\n",
        "print(test_text_attention_mask.shape)\n",
        "print(test_ground_truth.shape)"
      ],
      "metadata": {
        "id": "2FilVGy_RxCw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020b6c11-c575-4fd7-e896-5f9f9788bd47"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([138, 500])\n",
            "torch.Size([138, 500])\n",
            "torch.Size([138])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(test_context_input_ids.shape)\n",
        "# print(test_context_attention_mask.shape)\n"
      ],
      "metadata": {
        "id": "Icba89ykR_5g"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_input_data = []\n",
        "\n",
        "\n",
        "# for j in range(test_ground_truth.shape[0]):\n",
        "#   temp_list = []\n",
        "#   temp_list.append(test_text_input_ids[j])\n",
        "#   temp_list.append(test_text_attention_mask[j])\n",
        "#   temp_list.append(test_context_input_ids[j])\n",
        "#   temp_list.append(test_context_attention_mask[j])\n",
        "#   temp_list.append(test_audio_data_utterance1[j])\n",
        "#   temp_list.append(test_image_data_utterance1[j])\n",
        "\n",
        "#   test_input_data.append(temp_list)\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "kXqJ0fcjSJ4I"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(type(test_input_data))\n",
        "# print(len(test_input_data))"
      ],
      "metadata": {
        "id": "Z9swXKn_TQsW"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_output_data = test_ground_truth.tolist()\n",
        "# print(type(test_output_data))\n",
        "# print(len(test_output_data))"
      ],
      "metadata": {
        "id": "cjn1S99ZS_SX"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_valid, X_test, Y_valid, Y_test = train_test_split(\n",
        "#     test_input_data, test_output_data, test_size = 0.5, stratify = test_output_data\n",
        "# )"
      ],
      "metadata": {
        "id": "ecXLmWNsT6h-"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(X_valid)"
      ],
      "metadata": {
        "id": "GsXIaU_jUUwH"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(X_test)"
      ],
      "metadata": {
        "id": "J6lG18S9UcWx"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(Y_valid)"
      ],
      "metadata": {
        "id": "IfleSSo6UfWk"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(Y_test)"
      ],
      "metadata": {
        "id": "vi0Vpd2lUhR7"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_text_input_ids = []\n",
        "# valid_text_attention_mask = []\n",
        "# valid_context_input_ids = []\n",
        "# valid_context_attention_mask = []\n",
        "\n",
        "# valid_audio_data = []\n",
        "# valid_image_data = []\n",
        "\n",
        "# for j in range(len(X_valid)):\n",
        "#   valid_text_input_ids.append(X_valid[j][0])\n",
        "#   valid_text_attention_mask.append(X_valid[j][1])\n",
        "  \n",
        "#   valid_context_input_ids.append(X_valid[j][2])\n",
        "#   valid_context_attention_mask.append(X_valid[j][3])\n",
        "  \n",
        "#   valid_audio_data.append(X_valid[j][4])\n",
        "\n",
        "#   valid_image_data.append(X_valid[j][5])\n",
        "\n",
        "# print(len(valid_text_input_ids))\n",
        "# print(len(valid_text_attention_mask))\n",
        "# print(len(valid_context_input_ids))\n",
        "# print(len(valid_context_attention_mask))\n",
        "# print(len(valid_audio_data))\n",
        "# print(len(valid_image_data))"
      ],
      "metadata": {
        "id": "MnzHh4kBUk6f"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_text_input_ids[0].shape"
      ],
      "metadata": {
        "id": "xyqnoPspY06u"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_text_input_ids = torch.stack(valid_text_input_ids)\n",
        "# print(valid_text_input_ids.shape)\n",
        "# valid_text_attention_mask = torch.stack(valid_text_attention_mask)\n",
        "# print(valid_text_attention_mask.shape)\n",
        "# valid_context_input_ids = torch.stack(valid_context_input_ids)\n",
        "# print(valid_context_input_ids.shape)\n",
        "# valid_context_attention_mask = torch.stack(valid_context_attention_mask)\n",
        "# print(valid_context_attention_mask.shape)\n",
        "# valid_audio_data = torch.stack(valid_audio_data)\n",
        "# print(valid_audio_data.shape)\n",
        "# valid_image_data = torch.stack(valid_image_data)\n",
        "# print(valid_image_data.shape)"
      ],
      "metadata": {
        "id": "XoyGWRN9V6C9"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_ground_truth = torch.tensor(Y_valid)\n",
        "# valid_ground_truth.shape"
      ],
      "metadata": {
        "id": "ipNEkx0wV2Wz"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_text_id = []\n",
        "# test_text_mask = []\n",
        "\n",
        "# test_context_id = []\n",
        "# test_context_mask = []\n",
        "\n",
        "# test_audio_data = []\n",
        "# test_image_data = []\n",
        "\n",
        "# for j in range(len(X_test)):\n",
        "#   test_text_id.append(X_test[j][0])\n",
        "#   test_text_mask.append(X_test[j][1])\n",
        "\n",
        "#   test_context_id.append(X_test[j][2])\n",
        "#   test_context_mask.append(X_test[j][3])\n",
        "\n",
        "#   test_audio_data.append(X_test[j][4])\n",
        "\n",
        "#   test_image_data.append(X_test[j][5])\n",
        "\n",
        "# test_text_id = torch.stack(test_text_id)\n",
        "# print(test_text_id.shape)\n",
        "# test_text_mask = torch.stack(test_text_mask)\n",
        "# print(test_text_mask.shape)\n",
        "# test_context_id = torch.stack(test_context_id)\n",
        "# print(test_context_id.shape)\n",
        "# test_context_mask = torch.stack(test_context_mask)\n",
        "# print(test_context_mask.shape)\n",
        "# test_audio_data = torch.stack(test_audio_data)\n",
        "# print(test_audio_data.shape)\n",
        "# test_image_data = torch.stack(test_image_data)\n",
        "# print(test_image_data.shape)  "
      ],
      "metadata": {
        "id": "4FM6TlZIatoC"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_ground_truth = torch.tensor(Y_test)\n",
        "# print(test_ground_truth.shape)"
      ],
      "metadata": {
        "id": "RiJSqVpFckAT"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "TMN_GJV1Jjps"
      },
      "outputs": [],
      "source": [
        "\n",
        "# tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "SY3KgbQUKoFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a94ed992-8ba1-42f8-b2f7-ecb6c8a15f1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '</s>', '<unk>', '<pad>', '<mask>', '[CONTEXT]', '[UTTERANCE]']"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ],
      "source": [
        "tokenizer.all_special_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "M7uPY9VkLRcr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "oGASW03jLFtd"
      },
      "outputs": [],
      "source": [
        "# tokenizer.all_special_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "JVqVQYAULBg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1ad56b-0b8c-4b03-9a82-1c56ce7f262c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 2, 3, 1, 50264, 50265, 50266]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.all_special_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "hNcRXEYZLK7w"
      },
      "outputs": [],
      "source": [
        "# train_audio_broadcast_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "eP1Y_my0-btZ"
      },
      "outputs": [],
      "source": [
        "# valid_audio_data_utterance = test_audio_data_utterance[:VALID_LEN, :, :]\n",
        "# valid_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "K4c_xiCD-tn0"
      },
      "outputs": [],
      "source": [
        "# valid_image_data_utterance = test_image_data_utterance[:VALID_LEN, :, :]\n",
        "# valid_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "QwLEXwtB-6KN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# test_audio_data_utterance = test_audio_data_utterance[VALID_LEN:, :, :]\n",
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "fF_ErwW5_EsF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# test_image_data_utterance = test_image_data_utterance[VALID_LEN:, :, :]\n",
        "# test_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_data = []\n",
        "\n",
        "\n",
        "for j in range(test_ground_truth.shape[0]):\n",
        "  temp_list = []\n",
        "  temp_list.append(test_text_input_ids[j])\n",
        "  temp_list.append(test_text_attention_mask[j])\n",
        "\n",
        "  temp_list.append(test_audio_data_utterance1[j])\n",
        "  temp_list.append(test_image_data_utterance1[j])\n",
        "\n",
        "  test_input_data.append(temp_list)\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "q-7eDQZmEhNZ"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(test_input_data))\n",
        "print(len(test_input_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5UZkOdvEl6f",
        "outputId": "fae30ae2-dd61-45ed-9f90-69438595e641"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_output_data = test_ground_truth.tolist()\n",
        "print(type(test_output_data))\n",
        "print(len(test_output_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnW2xGe6Enfw",
        "outputId": "d7857336-71fe-470b-eb80-28ed75de1582"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid, X_test, Y_valid, Y_test = train_test_split(\n",
        "    test_input_data, test_output_data, test_size = 0.5, stratify = test_output_data\n",
        ")"
      ],
      "metadata": {
        "id": "jO18wKiHEoto"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFOS59YEKznY"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fegJT6LBEsM8",
        "outputId": "a77c4e4d-5b08-4a2b-a501-59f75467f787"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRaVQfSEEtmp",
        "outputId": "d873d8e2-ecdf-433b-f9ed-caafdee5ccf2"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(Y_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_Q2A4vkEu2w",
        "outputId": "1adf8263-fe34-4e6e-e5d3-a54631e7b321"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCjoibAAEwTD",
        "outputId": "d1568716-141e-49cc-d4ba-6057b02bbba4"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_text_input_ids = []\n",
        "valid_text_attention_mask = []\n",
        "valid_context_input_ids = []\n",
        "valid_context_attention_mask = []\n",
        "\n",
        "valid_audio_data = []\n",
        "valid_image_data = []\n",
        "\n",
        "for j in range(len(X_valid)):\n",
        "  valid_text_input_ids.append(X_valid[j][0])\n",
        "  valid_text_attention_mask.append(X_valid[j][1])\n",
        "  \n",
        "  \n",
        "  valid_audio_data.append(X_valid[j][2])\n",
        "\n",
        "  valid_image_data.append(X_valid[j][3])\n",
        "\n",
        "print(len(valid_text_input_ids))\n",
        "print(len(valid_text_attention_mask))\n",
        "\n",
        "print(len(valid_audio_data))\n",
        "print(len(valid_image_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bccK__bzExmv",
        "outputId": "a998a73b-6f30-4877-e907-eaa94c79d8a3"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69\n",
            "69\n",
            "69\n",
            "69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_text_input_ids[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK8TO60JEy_x",
        "outputId": "a4eba0c4-3dd3-4e91-b7e7-3091e46f50ed"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([500])"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_text_input_ids = torch.stack(valid_text_input_ids)\n",
        "print(valid_text_input_ids.shape)\n",
        "valid_text_attention_mask = torch.stack(valid_text_attention_mask)\n",
        "print(valid_text_attention_mask.shape)\n",
        "\n",
        "valid_audio_data = torch.stack(valid_audio_data)\n",
        "print(valid_audio_data.shape)\n",
        "valid_image_data = torch.stack(valid_image_data)\n",
        "print(valid_image_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XECiB9_qE0KB",
        "outputId": "a981a517-7652-4aae-dcdf-5d265d8fdfa9"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([69, 500])\n",
            "torch.Size([69, 500])\n",
            "torch.Size([69, 1000, 768])\n",
            "torch.Size([69, 480, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_ground_truth = torch.tensor(Y_valid)\n",
        "valid_ground_truth.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqE-5ALLE1en",
        "outputId": "cd919ca8-5b13-42f2-e8cf-cae379729d84"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69])"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text_id = []\n",
        "test_text_mask = []\n",
        "\n",
        "test_context_id = []\n",
        "test_context_mask = []\n",
        "\n",
        "test_audio_data = []\n",
        "test_image_data = []\n",
        "\n",
        "for j in range(len(X_test)):\n",
        "  test_text_id.append(X_test[j][0])\n",
        "  test_text_mask.append(X_test[j][1])\n",
        "\n",
        "\n",
        "  test_audio_data.append(X_test[j][2])\n",
        "\n",
        "  test_image_data.append(X_test[j][3])\n",
        "\n",
        "test_text_id = torch.stack(test_text_id)\n",
        "print(test_text_id.shape)\n",
        "test_text_mask = torch.stack(test_text_mask)\n",
        "print(test_text_mask.shape)\n",
        "\n",
        "test_audio_data = torch.stack(test_audio_data)\n",
        "print(test_audio_data.shape)\n",
        "test_image_data = torch.stack(test_image_data)\n",
        "print(test_image_data.shape)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTV-5u5IE2zS",
        "outputId": "73b21767-a0af-4d8d-f769-1baf5e0f0d68"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([69, 500])\n",
            "torch.Size([69, 500])\n",
            "torch.Size([69, 1000, 768])\n",
            "torch.Size([69, 480, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ground_truth = torch.tensor(Y_test)\n",
        "print(test_ground_truth.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s73tO--wE4aD",
        "outputId": "0bae7b7c-6952-4b3c-edd6-092a2568abb8"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([69])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "qrligkveIb4f"
      },
      "outputs": [],
      "source": [
        "class MultimodalSarcasmDataset(Dataset):\n",
        "    # def __init__(self, utterance_input_ids, utterance_attention_mask, context_input_ids, context_attention_mask, acoustic_data, visual_data, labels):\n",
        "    def __init__(self, utterance_input_ids, utterance_attention_mask, acoustic_data, visual_data, labels):\n",
        "\n",
        "        self.utterance_input_ids = utterance_input_ids\n",
        "        self.utterance_attention_mask = utterance_attention_mask\n",
        "        # self.context_input_ids = context_input_ids\n",
        "        # self.context_attention_mask = context_attention_mask\n",
        "        # self.context_attention_mask\n",
        "        self.acoustic_data = acoustic_data\n",
        "        self.visual_data = visual_data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.utterance_input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # return self.utterance_input_ids[idx], self.utterance_attention_mask[idx], self.context_input_ids[idx], self.context_attention_mask[idx], self.acoustic_data[idx], self.visual_data[idx], self.labels[idx]\n",
        "        return self.utterance_input_ids[idx], self.utterance_attention_mask[idx],  self.acoustic_data[idx], self.visual_data[idx], self.labels[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_text_input_ids.shape"
      ],
      "metadata": {
        "id": "8IhvAU6M_0mh"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_context_attention_mask.dtype"
      ],
      "metadata": {
        "id": "8UJo5LQ3RjCx"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_image_data_utterance.shape"
      ],
      "metadata": {
        "id": "1oQY9qKjXbPU"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V1wRtyIFYS2m"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_text_input_ids.shape"
      ],
      "metadata": {
        "id": "ILQZRYToYYDd"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_text_attention_mask.shape"
      ],
      "metadata": {
        "id": "U3SQyApZYbH7"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_context_input_ids.shape"
      ],
      "metadata": {
        "id": "_JFdrEk1Yd-J"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_context_attention_mask.shape"
      ],
      "metadata": {
        "id": "sYiUBLjTYgdj"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_audio_data_utterance.shape"
      ],
      "metadata": {
        "id": "lRpLc5f7Yiyz"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_image_data_utterance.shape"
      ],
      "metadata": {
        "id": "6c8Wsp6wYlcV"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_ground_truth.shape"
      ],
      "metadata": {
        "id": "mBLinpQcYo_4"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_context_mask.shape"
      ],
      "metadata": {
        "id": "gya_VngQJFlx"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_context_id.shape"
      ],
      "metadata": {
        "id": "cvLOlLI1JJmE"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_truth.shape"
      ],
      "metadata": {
        "id": "YOwejwPbJPwB"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_audio_data_utterance.shape"
      ],
      "metadata": {
        "id": "-KcNwn5qJcGa"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_context_input_ids.shape"
      ],
      "metadata": {
        "id": "R_5HoO4YdY0U"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_audio_data_utterance1.shape\n"
      ],
      "metadata": {
        "id": "tYWmjXQZ5y2W"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOHMQ9HIFRnh",
        "outputId": "ac6e8421-db6c-4a33-fbe0-a19105b157c9"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69, 480, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_audio_data.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C6e9caDFSy_",
        "outputId": "2dbd323f-0eb8-464d-cf67-b9decbcb7d65"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69, 1000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text_id.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEIpohRVFUO4",
        "outputId": "53d2dd1b-934e-4a24-f543-84ab77a8d432"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text_mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jEKs32xFVsZ",
        "outputId": "5c2cf06d-0033-4d1f-ed9d-cf82838064e9"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_ground_truth.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBTmMfh6FXAx",
        "outputId": "ed55ed85-0a18-4199-c49e-e4ffd1a95d70"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69])"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ground_truth.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux-4pOaOFYYk",
        "outputId": "155bb478-213f-40d8-92b5-bf7b298ac22b"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69])"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "5j_r6ZqtIeD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877a300e-40c9-4fdb-8a4f-c3936bc7aac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f76f0dc5ac0>\n"
          ]
        }
      ],
      "source": [
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask, train_context_input_ids, train_context_attention_mask,train_audio_broadcast_utterance, train_image_broadcast_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "# valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask, valid_audio_broadcast_utterance, valid_image_broadcast_utterance, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask, test_context_input_ids, test_context_attention_mask, test_audio_broadcast_utterance, test_image_broadcast_utterance, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask, train_context_input_ids, train_context_attention_mask, train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "# # valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask,   valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask, test_context_input_ids, test_context_attention_mask,  test_audio_data_utterance1, test_image_data_utterance1, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask,  train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "# # valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask,   valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask,   test_audio_data_utterance1, test_image_data_utterance1, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask,  train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=4, shuffle = True)\n",
        "# # valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask,   valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask,   test_audio_data_utterance1, test_image_data_utterance1, test_ground_truth), batch_size=4, shuffle = False)\n",
        "\n",
        "train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask,  train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask,    valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "test_loader = DataLoader(MultimodalSarcasmDataset(test_text_id, test_text_mask,   test_audio_data, test_image_data, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "\n",
        "print(test_loader)\n",
        "\n",
        "# print(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader.dataset)"
      ],
      "metadata": {
        "id": "XJYdQp5vwRim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37da028-4bc4-4f95-b2af-0ba7a1b6776b"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "552"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(valid_loader.dataset)"
      ],
      "metadata": {
        "id": "ZYX5DVe_wYtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775fcbcc-a8fc-4158-9649-b41d253fc6cf"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_loader.dataset)"
      ],
      "metadata": {
        "id": "NPTKQs5uwdsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f5c940-dcf5-45a4-9317-90b66cd7848f"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "44TjmFmSIh9D"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "O1EC_JPPUpq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b78b8365-b3d1-42d3-83bf-7712156a30dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaConfig {\n",
              "  \"_name_or_path\": \"roberta-base\",\n",
              "  \"architectures\": [\n",
              "    \"RobertaForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"max_position_embeddings\": 514,\n",
              "  \"model_type\": \"roberta\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.26.1\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50267\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "AbrSjHApAMiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e0c5bf-2a42-4529-dcd4-461b1343a6c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ],
      "source": [
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "TF3U8SkdZJ9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1504e797-3074-43d4-d86a-e6059da2256d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiModalRobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50267, 768)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): MultimodalRobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (acoustic_context_transform): Linear(in_features=1000, out_features=500, bias=False)\n",
              "    (visual_context_transform): Linear(in_features=480, out_features=500, bias=False)\n",
              "    (acoustic_dim): Linear(in_features=768, out_features=768, bias=False)\n",
              "    (visual_dim): Linear(in_features=2048, out_features=768, bias=False)\n",
              "    (concat_linear): Linear(in_features=2304, out_features=768, bias=False)\n",
              "  )\n",
              "  (output): MultiModalRobertaClassification(\n",
              "    (dense): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ],
      "source": [
        "model = model.to(DEVICE)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "i5MxPWWyIotE"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader):\n",
        "      model.train()\n",
        "      epoch_train_loss = 0.0\n",
        "    \n",
        "      \n",
        "      for step, batch in enumerate(tqdm(data_loader, desc = 'Training Iteration')):\n",
        "        # for i, t in enumerate(batch):\n",
        "        #     print(\"Inside hello\")\n",
        "        #     print(i, \" : \", type(t))\n",
        "        batch = tuple(t.to(DEVICE) for t in batch)\n",
        "        # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "        input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "        # print('\\ninput ids shape : ', input_ids.shape)\n",
        "        # print(\"attention mask shape : \", attention_mask.shape)\n",
        "        # print('acoustic_input shape : ', acoustic_input.shape)\n",
        "        # print('visual_input shape : ', visual_input.shape)\n",
        "        optimizer.zero_grad()\n",
        "        # print(\"Input ids shape : \", input_ids.shape)\n",
        "        # print(\"Input ids shape : \", input_ids.shape)\n",
        "        outputs = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask,\n",
        "                        \n",
        "                        # context_input_ids = context_input_ids,\n",
        "                        # context_attention_mask = context_attention_mask,\n",
        "                        acoustic_input = acoustic_input,\n",
        "                        visual_input = visual_input,\n",
        "                        labels = labels)\n",
        "\n",
        "        # outputs = model2(input_ids = input_ids,\n",
        "        #                 attention_mask = attention_mask,\n",
        "        #                 )\n",
        "        # last_hidden_state = outputs['last_hidde,n_state']\n",
        "        # print('last hidden_state shape : ', last_hidden_state.shape)\n",
        "        loss = outputs['loss']\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "        # print(\"Batch wise loss : \", epoch_train_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    \n",
        "\n",
        "      print(\"Epoch train loss : \", epoch_train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "D0H6Hk2w62xu"
      },
      "outputs": [],
      "source": [
        "def valid_epoch(model, data_loader):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  gold = []\n",
        "\n",
        "  valid_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(tqdm(data_loader)):\n",
        "      batch = tuple(t.to(DEVICE) for t in batch)\n",
        "      # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "      input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "      \n",
        "\n",
        "      outputs = model(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            # context_input_ids = context_input_ids,\n",
        "                            # context_attention_mask = context_attention_mask,\n",
        "                            acoustic_input = acoustic_input,\n",
        "                            visual_input = visual_input,\n",
        "                            labels = labels)\n",
        "      \n",
        "      logits = outputs['logits']\n",
        "      loss = outputs['loss']\n",
        "\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "      pred = logits.argmax(dim = -1)\n",
        "\n",
        "      predictions.extend(pred.tolist())\n",
        "      gold.extend(labels.tolist())\n",
        "\n",
        "  return valid_loss, predictions, gold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "93nGID9-Imr-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def test_epoch(model, data_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    gold = []\n",
        "\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(tqdm(data_loader)):\n",
        "            batch = tuple(t.to(DEVICE) for t in batch)\n",
        "            # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "            input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "\n",
        "            print(\"attention mask shape : \", attention_mask.shape)\n",
        "\n",
        "            outputs = model(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            # context_input_ids = context_input_ids,\n",
        "                            # context_attention_mask = context_attention_mask,\n",
        "                      \n",
        "                            acoustic_input = acoustic_input,\n",
        "                            visual_input = visual_input,\n",
        "                            labels = labels)\n",
        "\n",
        "            logits = outputs['logits']\n",
        "\n",
        "            pred = logits.argmax(dim = -1)\n",
        "\n",
        "            predictions.extend(pred.tolist())\n",
        "\n",
        "            gold.extend(labels.tolist())\n",
        "\n",
        "            correct += int((pred == labels).sum())\n",
        "\n",
        "    return correct/len(data_loader.dataset), predictions, gold "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "s5QauBgKBtYS"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "  def __init__(self, patience, min_delta):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.min_validation = np.inf\n",
        "\n",
        "  def early_stop(self, valid_loss):\n",
        "    if valid_loss < self.min_validation:\n",
        "      self.min_validation = valid_loss\n",
        "      self.counter = 0\n",
        "    elif valid_loss > (self.min_validation + self.min_delta):\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        return True\n",
        "    return False          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "8dHmyfM2Cc9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e4cfac5-102d-4834-941b-1658763dfdc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.EarlyStopping at 0x7f76f0d50910>"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ],
      "source": [
        "early_stopper = EarlyStopping(patience = 15, min_delta = 0.2)\n",
        "early_stopper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "B0HB4PIc6ixU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_and_validation(model, train_loader, valid_loader):\n",
        "  # lowest_loss = 1e6\n",
        "  best_f1 = 0.0\n",
        "  for epoch in range(30):\n",
        "    print(\"\\n=============Epoch : \", epoch)\n",
        "    train_epoch(model, train_loader)\n",
        "    valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "    if early_stopper.early_stop(valid_loss):\n",
        "      break\n",
        "\n",
        "    print(\"Length of predictions : \", len(valid_pred))\n",
        "    print(\"Length of gold : \", len(valid_gold))\n",
        "    print(\"Valid loss : \", valid_loss)\n",
        "    print(\"\\n Valid Accuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "    print(\"\\n Valid Precision : \", precision_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "    print(\"\\n Valid Recall : \", recall_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "    print(\"\\nValid F1 score : \", f1_score(valid_gold, valid_pred, average = 'weighted')) \n",
        "\n",
        "    \n",
        "    curr_f1 = f1_score(valid_gold, valid_pred, average = 'weighted')\n",
        "\n",
        "    curr_loss = valid_loss\n",
        "    # if((curr_f1 > best_f1) and (epoch>=4)):\n",
        "    if(curr_f1 > best_f1):  \n",
        "    # if(curr_loss < lowest_loss):    \n",
        "      best_f1 = curr_f1\n",
        "      # print(\"Valid pred : \", valid_pred)\n",
        "      # print('valid_gold : ', valid_gold)\n",
        "      # torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/roberta/new_seed/best_model_epoch_'+str(epoch)+'_best_f1_'+str(int(best_f1*100))+'_foldNum_'+str(foldNum)+'.pt')\n",
        "      torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/32/saved_model/roberta_only/best_model_epoch_'+str(epoch)+'_best_f1_'+str(int(best_f1*100))+'_foldNum_'+str(foldNum)+'.pt')\n",
        "\n",
        "      print(\"model saved\\n\")\n",
        "      # print(\"best model\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "RMJL4HbkEBh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c58d051b-0cc1-4821-c196-88c1cf6d1f1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=============Epoch :  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:13<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  12.492942035198212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  2.054832935333252\n",
            "\n",
            " Valid Accuracy :  0.5652173913043478\n",
            "\n",
            " Valid Precision :  0.7755960729312763\n",
            "\n",
            " Valid Recall :  0.5652173913043478\n",
            "\n",
            "Valid F1 score :  0.48637625094609366\n",
            "model saved\n",
            "\n",
            "\n",
            "=============Epoch :  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  12.21616941690445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.9246580004692078\n",
            "\n",
            " Valid Accuracy :  0.6376811594202898\n",
            "\n",
            " Valid Precision :  0.6573545806282379\n",
            "\n",
            " Valid Recall :  0.6376811594202898\n",
            "\n",
            "Valid F1 score :  0.6127566805911421\n",
            "model saved\n",
            "\n",
            "\n",
            "=============Epoch :  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  11.285974860191345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.6123742163181305\n",
            "\n",
            " Valid Accuracy :  0.8115942028985508\n",
            "\n",
            " Valid Precision :  0.8122529644268774\n",
            "\n",
            " Valid Recall :  0.8115942028985508\n",
            "\n",
            "Valid F1 score :  0.8117530275957912\n",
            "model saved\n",
            "\n",
            "\n",
            "=============Epoch :  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  10.797932863235474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.4393902719020844\n",
            "\n",
            " Valid Accuracy :  0.782608695652174\n",
            "\n",
            " Valid Precision :  0.782387244408356\n",
            "\n",
            " Valid Recall :  0.782608695652174\n",
            "\n",
            "Valid F1 score :  0.7823326432022084\n",
            "\n",
            "=============Epoch :  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  8.98695820569992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.272358626127243\n",
            "\n",
            " Valid Accuracy :  0.7681159420289855\n",
            "\n",
            " Valid Precision :  0.76999147485081\n",
            "\n",
            " Valid Recall :  0.7681159420289855\n",
            "\n",
            "Valid F1 score :  0.7684087249304641\n",
            "\n",
            "=============Epoch :  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  6.908990651369095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.101100116968155\n",
            "\n",
            " Valid Accuracy :  0.7971014492753623\n",
            "\n",
            " Valid Precision :  0.7972500929022668\n",
            "\n",
            " Valid Recall :  0.7971014492753623\n",
            "\n",
            "Valid F1 score :  0.7964986097783027\n",
            "\n",
            "=============Epoch :  6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  5.052379250526428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.2154718488454819\n",
            "\n",
            " Valid Accuracy :  0.7681159420289855\n",
            "\n",
            " Valid Precision :  0.7736382256099141\n",
            "\n",
            " Valid Recall :  0.7681159420289855\n",
            "\n",
            "Valid F1 score :  0.7648175912043979\n",
            "\n",
            "=============Epoch :  7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  3.7357277274131775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.1668111868202686\n",
            "\n",
            " Valid Accuracy :  0.7246376811594203\n",
            "\n",
            " Valid Precision :  0.7242439900592997\n",
            "\n",
            " Valid Recall :  0.7246376811594203\n",
            "\n",
            "Valid F1 score :  0.7242880147227974\n",
            "\n",
            "=============Epoch :  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  2.6103259548544884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.691084586083889\n",
            "\n",
            " Valid Accuracy :  0.7391304347826086\n",
            "\n",
            " Valid Precision :  0.8102139406487232\n",
            "\n",
            " Valid Recall :  0.7391304347826086\n",
            "\n",
            "Valid F1 score :  0.7292353823088455\n",
            "\n",
            "=============Epoch :  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  2.232686899602413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.3447156697511673\n",
            "\n",
            " Valid Accuracy :  0.7681159420289855\n",
            "\n",
            " Valid Precision :  0.773678817157078\n",
            "\n",
            " Valid Recall :  0.7681159420289855\n",
            "\n",
            "Valid F1 score :  0.7683108025819023\n",
            "\n",
            "=============Epoch :  10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  1.3751594200730324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.4125224649906158\n",
            "\n",
            " Valid Accuracy :  0.782608695652174\n",
            "\n",
            " Valid Precision :  0.7908710340775559\n",
            "\n",
            " Valid Recall :  0.782608695652174\n",
            "\n",
            "Valid F1 score :  0.7826086956521738\n",
            "\n",
            "=============Epoch :  11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.9802061785012484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.5798505516722798\n",
            "\n",
            " Valid Accuracy :  0.7536231884057971\n",
            "\n",
            " Valid Precision :  0.7543551456594935\n",
            "\n",
            " Valid Recall :  0.7536231884057971\n",
            "\n",
            "Valid F1 score :  0.7538308822406499\n",
            "\n",
            "=============Epoch :  12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  1.412614369764924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.7550670932978392\n",
            "\n",
            " Valid Accuracy :  0.7101449275362319\n",
            "\n",
            " Valid Precision :  0.7130227372896736\n",
            "\n",
            " Valid Recall :  0.7101449275362319\n",
            "\n",
            "Valid F1 score :  0.7060219890054972\n",
            "\n",
            "=============Epoch :  13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.9536694474518299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.7490187659859657\n",
            "\n",
            " Valid Accuracy :  0.7391304347826086\n",
            "\n",
            " Valid Precision :  0.749760094485864\n",
            "\n",
            " Valid Recall :  0.7391304347826086\n",
            "\n",
            "Valid F1 score :  0.7388016075995616\n",
            "\n",
            "=============Epoch :  14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.868172250688076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.9677083725109696\n",
            "\n",
            " Valid Accuracy :  0.782608695652174\n",
            "\n",
            " Valid Precision :  0.8185424282492029\n",
            "\n",
            " Valid Recall :  0.782608695652174\n",
            "\n",
            "Valid F1 score :  0.7795721187025536\n",
            "\n",
            "=============Epoch :  15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.6131956949830055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.9609907250851393\n",
            "\n",
            " Valid Accuracy :  0.7681159420289855\n",
            "\n",
            " Valid Precision :  0.773678817157078\n",
            "\n",
            " Valid Recall :  0.7681159420289855\n",
            "\n",
            "Valid F1 score :  0.7683108025819023\n",
            "\n",
            "=============Epoch :  16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.5072803599759936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.9815506963059306\n",
            "\n",
            " Valid Accuracy :  0.7536231884057971\n",
            "\n",
            " Valid Precision :  0.7615795142969055\n",
            "\n",
            " Valid Recall :  0.7536231884057971\n",
            "\n",
            "Valid F1 score :  0.7536231884057971\n",
            "\n",
            "=============Epoch :  17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.5668753311038017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.9526262660510838\n",
            "\n",
            " Valid Accuracy :  0.7536231884057971\n",
            "\n",
            " Valid Precision :  0.7533156172338279\n",
            "\n",
            " Valid Recall :  0.7536231884057971\n",
            "\n",
            "Valid F1 score :  0.7533103289625029\n",
            "\n",
            "=============Epoch :  18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.3486924283206463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  2.0125811106991023\n",
            "\n",
            " Valid Accuracy :  0.7536231884057971\n",
            "\n",
            " Valid Precision :  0.7615795142969055\n",
            "\n",
            " Valid Recall :  0.7536231884057971\n",
            "\n",
            "Valid F1 score :  0.7536231884057971\n",
            "\n",
            "=============Epoch :  19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.4302315656095743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.9269901476800442\n",
            "\n",
            " Valid Accuracy :  0.7681159420289855\n",
            "\n",
            " Valid Precision :  0.7680416202155333\n",
            "\n",
            " Valid Recall :  0.7681159420289855\n",
            "\n",
            "Valid F1 score :  0.7674269826037745\n",
            "\n",
            "=============Epoch :  20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.45496140886098146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  2.0980061162263155\n",
            "\n",
            " Valid Accuracy :  0.7391304347826086\n",
            "\n",
            " Valid Precision :  0.7433304814497939\n",
            "\n",
            " Valid Recall :  0.7391304347826086\n",
            "\n",
            "Valid F1 score :  0.7354197901049475\n",
            "\n",
            "=============Epoch :  21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.3229979267343879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.9374915123917162\n",
            "\n",
            " Valid Accuracy :  0.782608695652174\n",
            "\n",
            " Valid Precision :  0.7861161856046767\n",
            "\n",
            " Valid Recall :  0.782608695652174\n",
            "\n",
            "Valid F1 score :  0.782882891116981\n",
            "\n",
            "=============Epoch :  22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.2707266937941313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.46it/s]\n"
          ]
        }
      ],
      "source": [
        "# train_and_validation(model, train_loader, test_loader)\n",
        "train_and_validation(model, train_loader, valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model_epoch_12_best_f1_72_foldNum_0.pt\n",
        "# best_model_epoch_9_best_f1_69_foldNum_1.pt\n",
        "# best_model_epoch_9_best_f1_79_foldNum_2.pt\n",
        "# best_model_epoch_9_best_f1_73_foldNum_3.pt\n",
        "# best_model_epoch_7_best_f1_75_foldNum_4.pt"
      ],
      "metadata": {
        "id": "3o5p6PC96ZJQ"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/32/saved_model/roberta_only/best_model_epoch_2_best_f1_81_foldNum_0.pt'\n"
      ],
      "metadata": {
        "id": "DO0Qsq9ufvhi"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PATH_0 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_12_f1_76.pt'\n",
        "# PATH_1 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_13_f1_78_foldNum_1.pt'\n",
        "# PATH_2 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_9_f1_64_foldNum_2.pt'\n",
        "# PATH_3 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_5_f1_73_foldNum_3.pt'\n",
        "# PATH_4 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_74_foldNum_4.pt'"
      ],
      "metadata": {
        "id": "NAZqaFeUAiMW"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PATH_0 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_6_f1_50_foldNum_0.pt'\n",
        "# PATH_1 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_70_foldNum_1.pt'\n",
        "# PATH_2 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_11_f1_63_foldNum_2.pt'\n",
        "# PATH_3 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_44_foldNum_3.pt'\n",
        "# PATH_4 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_62_foldNum_4.pt'"
      ],
      "metadata": {
        "id": "NU9qavVgEwJ0"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foldNum"
      ],
      "metadata": {
        "id": "5PMNVLL3BM-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e3537b-2bc3-41c5-da0a-472de6b6e7e4"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "_J_qDFj5_ie3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c77ecc-8ea3-4e03-fd87-e42648a97773"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2*3"
      ],
      "metadata": {
        "id": "11LQROttIrMQ"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "print(acc)\n",
        "\n",
        "print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "print(\"\\nPrecision : \", precision_score(test_gold, test_pred, average = 'weighted'))\n",
        "print(\"\\nRecall : \", recall_score(test_gold, test_pred, average = 'weighted'))\n",
        "print(\"\\nF1 score : \", f1_score(test_gold, test_pred, average = 'weighted'))\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                                                 \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UgEpzAt-_mSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2042db43-3c1b-4e92-c253-185ac45a4f34"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention mask shape :  torch.Size([32, 500])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 1/3 [00:00<00:00,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention mask shape :  torch.Size([32, 500])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention mask shape :  torch.Size([5, 500])\n",
            "0.6666666666666666\n",
            "\n",
            "Accuracy :  0.6666666666666666\n",
            "\n",
            "Precision :  0.6675084175084174\n",
            "\n",
            "Recall :  0.6666666666666666\n",
            "\n",
            "F1 score :  0.6669476642079382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "50ctOavcBUOt"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred, average = 'weighted'))"
      ],
      "metadata": {
        "id": "LTd5vAY4HOp6"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0eD8Mc-spvSz"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred))\n",
        "# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred))\n",
        "# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred))"
      ],
      "metadata": {
        "id": "s3K5movSJPIf"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred, average = 'micro'))\n",
        "# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred, average = 'micro'))\n",
        "# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred, average = 'micro'))"
      ],
      "metadata": {
        "id": "wWNb08EkJPu0"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "Ixy_k2L9GgEV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(test_gold, test_pred))\n",
        "# print(\"\\nRecall : \", recall_score(test_gold, test_pred))\n",
        "# print(\"\\nF1 score : \", f1_score(test_gold, test_pred))\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                                                 \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H3XQZOkxJ4eP"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(test_gold, test_pred, average = 'micro'))\n",
        "# print(\"\\nRecall : \", recall_score(test_gold, test_pred, average = 'micro'))\n",
        "# print(\"\\nF1 score : \", f1_score(test_gold, test_pred, average = 'micro'))\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                                                 \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jryD1kV-J5MG"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "993-qFHhZxPp"
      },
      "outputs": [],
      "source": [
        "# test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "ZPaJ3qk0isrD"
      },
      "outputs": [],
      "source": [
        "# test_gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "94i6uFJtivrG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d636d685b494381b413c4e2d6229957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bf053b245084502a06d28e009fedb2c",
              "IPY_MODEL_209d4c2c076c42aaabff98794d6b4b59",
              "IPY_MODEL_38f4d50ddbd54929aaaf51960ca59a04"
            ],
            "layout": "IPY_MODEL_a2516511c98249e5ac16f50729717ddb"
          }
        },
        "8bf053b245084502a06d28e009fedb2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cabece2f70a24383b19a98ec160faf1a",
            "placeholder": "​",
            "style": "IPY_MODEL_621eab262d33456e904c69e95a706907",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "209d4c2c076c42aaabff98794d6b4b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1e634e5de7b46f287708f39e54cb666",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84f92592d052475a9f316db702a4e960",
            "value": 481
          }
        },
        "38f4d50ddbd54929aaaf51960ca59a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dfa8862676a4b11b7e0e50cef5d2114",
            "placeholder": "​",
            "style": "IPY_MODEL_11506f543bb04da1ae5e7323f7dd82be",
            "value": " 481/481 [00:00&lt;00:00, 30.9kB/s]"
          }
        },
        "a2516511c98249e5ac16f50729717ddb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cabece2f70a24383b19a98ec160faf1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "621eab262d33456e904c69e95a706907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1e634e5de7b46f287708f39e54cb666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84f92592d052475a9f316db702a4e960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2dfa8862676a4b11b7e0e50cef5d2114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11506f543bb04da1ae5e7323f7dd82be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4790bb39f3e443b9880bbe156fccf1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd71035656de42d1b5e5f1fbb583bfd2",
              "IPY_MODEL_961fd4c230bd4aedb8d72e13d87cfb72",
              "IPY_MODEL_e6a0304da41a4a5bafb1cdbed7b5f2b5"
            ],
            "layout": "IPY_MODEL_90e170f25dfb4c3b8058206e52ff5f33"
          }
        },
        "cd71035656de42d1b5e5f1fbb583bfd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b24d2bb0c217470a9254b640f7cbcc0b",
            "placeholder": "​",
            "style": "IPY_MODEL_2038067354f6439d8c14c8de0fe64384",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "961fd4c230bd4aedb8d72e13d87cfb72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63e098d2ef77446a962a9065dd899b43",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e17f398d6854d7d955c2bae73a53e7f",
            "value": 501200538
          }
        },
        "e6a0304da41a4a5bafb1cdbed7b5f2b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4751fd1595a84005b576f293ce5a0e49",
            "placeholder": "​",
            "style": "IPY_MODEL_dfb5bd0a91e04e81aacf85052915b3ea",
            "value": " 501M/501M [00:01&lt;00:00, 403MB/s]"
          }
        },
        "90e170f25dfb4c3b8058206e52ff5f33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b24d2bb0c217470a9254b640f7cbcc0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2038067354f6439d8c14c8de0fe64384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63e098d2ef77446a962a9065dd899b43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e17f398d6854d7d955c2bae73a53e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4751fd1595a84005b576f293ce5a0e49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfb5bd0a91e04e81aacf85052915b3ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9b78963f9bb4a02a5f9747b1ceb5fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16de6d4a6b3b449faf69e13c05a5d6c6",
              "IPY_MODEL_107e2bb9a6d04dc38fc7bf5019cbd20f",
              "IPY_MODEL_d00c602bfce34e31a54f61d19a049db8"
            ],
            "layout": "IPY_MODEL_bff24afddd4941a2b0f1fda7651dd12d"
          }
        },
        "16de6d4a6b3b449faf69e13c05a5d6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cba9a7e16fd427f838a734f676c7664",
            "placeholder": "​",
            "style": "IPY_MODEL_1a620d98c374446d92c790d382c9892b",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "107e2bb9a6d04dc38fc7bf5019cbd20f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ac438395055426ba0975622fb17f46d",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1856dfa6c88f405a9ce2fa8cedc11f83",
            "value": 898823
          }
        },
        "d00c602bfce34e31a54f61d19a049db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6da7cb773264502bcdd1aff73e38522",
            "placeholder": "​",
            "style": "IPY_MODEL_10f58335a5fb44e88bcdf5e336bc0f78",
            "value": " 899k/899k [00:01&lt;00:00, 805kB/s]"
          }
        },
        "bff24afddd4941a2b0f1fda7651dd12d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cba9a7e16fd427f838a734f676c7664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a620d98c374446d92c790d382c9892b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ac438395055426ba0975622fb17f46d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1856dfa6c88f405a9ce2fa8cedc11f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6da7cb773264502bcdd1aff73e38522": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10f58335a5fb44e88bcdf5e336bc0f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9edc64bfadff41c197533a25305bcf7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b21c8a0d4d6f4da38f156cab7b4033b1",
              "IPY_MODEL_8ecf4c3ac2414608a1a2254b51297898",
              "IPY_MODEL_f016e6274bb94583a859ffa991357d6f"
            ],
            "layout": "IPY_MODEL_706ca8702c094c6aa1edeb982f1babbb"
          }
        },
        "b21c8a0d4d6f4da38f156cab7b4033b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c93daebb86f74a3cabf94cbfa0bf4460",
            "placeholder": "​",
            "style": "IPY_MODEL_75d35d6ce16b48629fc0fb33fbbbb844",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "8ecf4c3ac2414608a1a2254b51297898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c27099814e2401d9d13949fcb150460",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ce1ad69ab6b45a0bb870d01d04d0c71",
            "value": 456318
          }
        },
        "f016e6274bb94583a859ffa991357d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_248bac307c324cec855bac7809325415",
            "placeholder": "​",
            "style": "IPY_MODEL_627f00aa067744d9a2a0d720679f30a8",
            "value": " 456k/456k [00:01&lt;00:00, 408kB/s]"
          }
        },
        "706ca8702c094c6aa1edeb982f1babbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c93daebb86f74a3cabf94cbfa0bf4460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75d35d6ce16b48629fc0fb33fbbbb844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c27099814e2401d9d13949fcb150460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ce1ad69ab6b45a0bb870d01d04d0c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "248bac307c324cec855bac7809325415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "627f00aa067744d9a2a0d720679f30a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}