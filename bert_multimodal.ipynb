{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WHwHkm4rGCLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85ce2bb-8b17-4228-db06-77369f072227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VFg6mNX7Gdxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0343316a-2e92-4d0e-f204-68ae3ede5845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 21 19:29:38 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZQU-d0knGyhl"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import warnings\n",
        "import logging\n",
        "import gc\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "from typing import Optional\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import jaccard_score, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "suRa-hBHG_21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c722d90e-3b9a-48a4-fcf8-5ba0ed721e33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r_Ovlo42G4Kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee73234a-7ac3-43e8-a680-2273415aeeae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "# from rouge_score.rouge_scorer import RougeScorer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from transformers import (\n",
        "    BartTokenizerFast,\n",
        "    AdamW\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print(\"Using GPU\")\n",
        "\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "foldNum = 0\n",
        "\n",
        "\n",
        "\n",
        "SOURCE_MAX_LEN = 500\n",
        "# TARGET_MAX_LEN = 50\n",
        "# MAX_UTTERANCES = 25\n",
        "\n",
        "ACOUSTIC_DIM = 768\n",
        "ACOUSTIC_MAX_LEN = 1000\n",
        "\n",
        "\n",
        "\n",
        "VISUAL_DIM = 2048\n",
        "VISUAL_MAX_LEN = 480\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "# LEARNING_RATE = 1e-5\n",
        "# LEARNING_RATE = 1e-3\n",
        "\n",
        "\n",
        "# VALID_LEN = 69\n",
        "\n",
        "# BASE_LEARNING_RATE = 5e-6\n",
        "# NEW_LEARNING_RATE = 5e-5\n",
        "# WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# NUM_BEAMS = 5\n",
        "# EARLY_STOPPING = True\n",
        "# NO_REPEAT_NGRAM_SIZE = 3\n",
        "\n",
        "# EARLY_STOPPING_THRESHOLD = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cMdRaTfAHF7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598a5c57-bf46-4435-d93e-e3c78d65edff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed : 994\n"
          ]
        }
      ],
      "source": [
        "def set_random_seed(seed: int):\n",
        "    print(\"Seed : {}\".format(seed))\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# set_random_seed(42)\n",
        "# set_random_seed(123)\n",
        "# set_random_seed(12345)\n",
        "set_random_seed(994)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "from transformers.modeling_utils import PreTrainedModel, unwrap_model\n",
        "\n",
        "# from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
        "# # from transformers.models.roberta.tokenization_roberta import RobertaTokenizer\n",
        "# from transformers.models.roberta.modeling_roberta import RobertaLayer, RobertaEmbeddings, RobertaPooler, RobertaPreTrainedModel\n",
        "# # from transformers.models.roberta.modeling_roberta import RobertaEmbeddings, RobertaPreTrainedModel\n",
        "# from transformers import RobertaTokenizer, RobertaModel\n",
        "# from transformers.modeling_outputs import (\n",
        "#     BaseModelOutputWithPastAndCrossAttentions,\n",
        "#     BaseModelOutputWithPoolingAndCrossAttentions\n",
        "# )\n",
        "\n",
        "from transformers.models.bert.configuration_bert import BertConfig\n",
        "\n",
        "from transformers.models.bert.modeling_bert import BertLayer, BertEmbeddings, BertPooler, BertPreTrainedModel\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions\n",
        ")\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
        "from transformers.activations import ACT2FN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jMtoMDKiVk0r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MuNh3gFEHQhY"
      },
      "outputs": [],
      "source": [
        "# from transformer_encoder import TransformerEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-ML9b3x4axdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d6b0ae3-e2af-44e5-b62a-775ff5d932df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OtwuoBrcwzUN"
      },
      "outputs": [],
      "source": [
        "# bert_tokenizer  = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "# bert_model = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "# # bert_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e1XR3BlgHOY8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aEDR_7N6HWsu"
      },
      "outputs": [],
      "source": [
        "# class ContextAwareAttention(nn.Module):\n",
        "\n",
        "#     def __init__(self,\n",
        "#                  dim_model : int,\n",
        "#                  dim_context : int,\n",
        "#                  dropout_rate : Optional[float] = 0.0 ):\n",
        "\n",
        "#         super(ContextAwareAttention, self).__init__()\n",
        "\n",
        "#         self.dim_model = dim_model\n",
        "#         self.dim_context = dim_context\n",
        "#         self.dropout_rate = dropout_rate\n",
        "#         self.attention_layer = nn.MultiheadAttention(embed_dim=self.dim_model,\n",
        "#                                                      num_heads = 1,\n",
        "#                                                      dropout = self.dropout_rate,\n",
        "#                                                      bias = True,\n",
        "#                                                     add_zero_attn=False,\n",
        "#                                                     batch_first=True,\n",
        "#                                                     device=DEVICE   \n",
        "#         )\n",
        "\n",
        "#         self.u_k = nn.Linear(self.dim_context, self.dim_model, bias = False)\n",
        "#         self.w1_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "#         self.w2_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "\n",
        "#         self.u_v = nn.Linear(self.dim_context, self.dim_model, bias=False)\n",
        "#         self.w1_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "#         self.w2_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "\n",
        "#     def forward(self, q, k, v, context):\n",
        "\n",
        "#         # print(\"Context shape : \", context.shape)\n",
        "#         # print(\"Dim context : \", self.dim_context, \" : Dim model : \", self.dim_model)\n",
        "#         key_context = self.u_k(context)\n",
        "#         # print(\"Context shape below key context : \", key_context.shape)\n",
        "#         value_context = self.u_v(context)\n",
        "\n",
        "#         lambda_k = F.sigmoid(self.w1_k(k) + self.w2_k(key_context))\n",
        "#         lambda_v = F.sigmoid(self.w1_v(v) + self.w2_v(value_context))\n",
        "\n",
        "#         k_cap = (1-lambda_k) * k + (lambda_k) * key_context\n",
        "#         v_cap = (1-lambda_v) * v + (lambda_v) * value_context\n",
        "\n",
        "#         attention_output, _ = self.attention_layer(query = q,\n",
        "#                                                    key = k_cap,\n",
        "#                                                    value = v_cap)\n",
        "  \n",
        "#         return attention_output                                     \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2ZPn4U3Weq9I"
      },
      "outputs": [],
      "source": [
        "# class MAF_acoustic(nn.Module):\n",
        "#     def __init__(self, \n",
        "#                 dim_model,\n",
        "#                 dropout_rate):\n",
        "#         super(MAF_acoustic, self).__init__()\n",
        "#         self.dropout_rate = dropout_rate\n",
        "\n",
        "#         self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "#         # self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "#         self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#                                                                 dim_context=ACOUSTIC_DIM,\n",
        "#                                                                 dropout_rate=dropout_rate)\n",
        "\n",
        "#         # self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#         #                                                     dim_context=VISUAL_DIM,\n",
        "#         #                                                     dropout_rate=dropout_rate)\n",
        "\n",
        "#         self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         # self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "#         self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "#     def forward(self,\n",
        "#                 text_input,\n",
        "#                 acoustic_context):\n",
        "\n",
        "#         # print(\"Acoustic context shape (A) : \", acoustic_context.shape)        \n",
        "\n",
        "#         acoustic_context = acoustic_context.permute(0,2,1)\n",
        "#         acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "#         acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "#         audio_out = self.acoustic_context_attention(q=text_input,\n",
        "#                                                     k=text_input,\n",
        "#                                                     v=text_input,\n",
        "#                                                     context=acoustic_context)\n",
        "#         # print(\"Audio out (A) : \", audio_out.shape) \n",
        "\n",
        "#         # print(\"Visual context shape : \", visual_context.shape)\n",
        "#         # visual_context = visual_context.permute(0,2,1)\n",
        "#         # visual_context = self.visual_context_transform(visual_context.float())\n",
        "#         # visual_context = visual_context.permute(0,2,1)\n",
        "        \n",
        "#         # video_out = self.visual_context_attention(q=text_input,\n",
        "#         #                                             k=text_input,\n",
        "#         #                                             v=text_input,\n",
        "#         #                                             context=visual_context)\n",
        "\n",
        "#         # print(\"Video out shape : \", video_out.shape)\n",
        "#         # print(\"Text input shape : \", text_input.shape)\n",
        "#         weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "#         # weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "#         # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "#         output = self.final_layer_norm(text_input + weight_a * audio_out)\n",
        "\n",
        "#         return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZeZdIlcker1V"
      },
      "outputs": [],
      "source": [
        "# class MAF_visual(nn.Module):\n",
        "#     def __init__(self, \n",
        "#                 dim_model,\n",
        "#                 dropout_rate):\n",
        "#         super(MAF_visual, self).__init__()\n",
        "#         self.dropout_rate = dropout_rate\n",
        "\n",
        "#         # self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "#         self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "#         # self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#         #                                                         dim_context=ACOUSTIC_DIM,\n",
        "#         #                                                         dropout_rate=dropout_rate)\n",
        "\n",
        "#         self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "#                                                             dim_context=VISUAL_DIM,\n",
        "#                                                             dropout_rate=dropout_rate)\n",
        "\n",
        "#         # self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "#         self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "#         self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "#     def forward(self,\n",
        "#                 text_input,\n",
        "#                 visual_context):\n",
        "\n",
        "#         # print(\"Acoustic context shape (A) : \", acoustic_context.shape)        \n",
        "\n",
        "#         # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "#         # acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "#         # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "#         # audio_out = self.acoustic_context_attention(q=text_input,\n",
        "#         #                                             k=text_input,\n",
        "#         #                                             v=text_input,\n",
        "#         #                                             context=acoustic_context)\n",
        "#         # print(\"Audio out (A) : \", audio_out.shape) \n",
        "\n",
        "#         # print(\"Visual context shape : \", visual_context.shape)\n",
        "#         visual_context = visual_context.permute(0,2,1)\n",
        "#         visual_context = self.visual_context_transform(visual_context.float())\n",
        "#         visual_context = visual_context.permute(0,2,1)\n",
        "        \n",
        "#         video_out = self.visual_context_attention(q=text_input,\n",
        "#                                                     k=text_input,\n",
        "#                                                     v=text_input,\n",
        "#                                                     context=visual_context)\n",
        "\n",
        "#         # print(\"Video out shape : \", video_out.shape)\n",
        "#         # print(\"Text input shape : \", text_input.shape)\n",
        "#         # weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "#         weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "#         # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "#         # q3 = weight_v * video_out\n",
        "#         # print('weight_v shape : ', weight_v.shape)\n",
        "#         # print('weight_v * video_out shape : ', q3.shape)\n",
        "\n",
        "#         output = self.final_layer_norm(text_input  + weight_v * video_out)\n",
        "\n",
        "#         return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s88m19VoHbj5"
      },
      "outputs": [],
      "source": [
        "class MultimodalBertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.gradient_checkpointing = False\n",
        "        \n",
        "        self.fusion_at_layer9 = [8]\n",
        "        # self.fusion_at_layer10 = [11]\n",
        "\n",
        "        # self.fusion_at_layer9 = [19]\n",
        "        # self.fusion_at_layer10 = [20]\n",
        "\n",
        "\n",
        "        self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "        self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "        self.acoustic_dim = nn.Linear(ACOUSTIC_DIM, 768, bias = False)\n",
        "        self.visual_dim = nn.Linear(VISUAL_DIM, 768, bias = False)\n",
        "\n",
        "        self.concat_linear = nn.Linear(3*768, 768, bias = False)\n",
        "\n",
        "        # self.MAF_layer9 = MAF_acoustic(dim_model=self.config.hidden_size,\n",
        "        #                      dropout_rate=0.2)\n",
        "        \n",
        "        # self.MAF_layer10 = MAF_visual(dim_model=self.config.hidden_size,\n",
        "        #                      dropout_rate=0.2)\n",
        "        \n",
        "    def forward(self,\n",
        "               hidden_states : torch.Tensor,\n",
        "               attention_mask: Optional[torch.FloatTensor] = None,\n",
        "               acoustic_input: Optional[torch.FloatTensor] = None,\n",
        "               visual_input: Optional[torch.FloatTensor] = None, \n",
        "               head_mask: Optional[torch.FloatTensor] = None,\n",
        "               encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "               encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "               past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "               use_cache: Optional[bool] = False,\n",
        "               output_attentions: Optional[bool] = False,\n",
        "               output_hidden_states: Optional[bool] = False,\n",
        "               return_dict: Optional[bool] = True\n",
        "               ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
        "        \n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "        \n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            # print(\"i : \", i)\n",
        "            if i in self.fusion_at_layer9:\n",
        "                    # print(\"Inside layer 9\")\n",
        "                    # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "                    # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]   \n",
        "                    # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "                    # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "                    # print(\"====Idx inside fusion at layer :\", idx)\n",
        "                    \n",
        "                      acoustic_input = acoustic_input.permute(0,2,1)\n",
        "                      acoustic_input = self.acoustic_context_transform(acoustic_input.float())\n",
        "                      acoustic_input = acoustic_input.permute(0,2,1)\n",
        "\n",
        "                      acoustic_input = self.acoustic_dim(acoustic_input)\n",
        "\n",
        "                      # print(\"acoustic_input shape : \", acoustic_input.shape)\n",
        "\n",
        "                      visual_input = visual_input.permute(0,2,1)\n",
        "                      visual_input = self.visual_context_transform(visual_input.float())\n",
        "                      visual_input = visual_input.permute(0,2,1)\n",
        "\n",
        "                      visual_input = self.visual_dim(visual_input)\n",
        "\n",
        "                      # print(\"visual input shape : \", visual_input.shape)\n",
        "                      concat = torch.concat([hidden_states, acoustic_input, visual_input], dim = -1)\n",
        "\n",
        "                      # print(\"concat shape : \", concat.shape)\n",
        "\n",
        "                      hidden_states = self.concat_linear(concat)\n",
        "\n",
        "                      # print('hidden states shape : ', hidden_states.shape)\n",
        "\n",
        "\n",
        "\n",
        "                      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # if i in self.fusion_at_layer10:\n",
        "            #         # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "            #         # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]   \n",
        "            #         # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "            #         # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "            #         # print(\"====Idx inside fusion at layer :\", idx)\n",
        "            #         hidden_states = self.MAF_layer10(text_input = hidden_states,\n",
        "            #                                        visual_context = visual_input)  \n",
        "        \n",
        "           \n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "            \n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "            \n",
        "            if self.gradient_checkpointing and self.training:\n",
        "                \n",
        "                if use_cache:\n",
        "                    # logger.warning(\n",
        "                    #     \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "                    # )\n",
        "                    use_cache = False\n",
        "                \n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "                    \n",
        "                    return custom_forward\n",
        "                \n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask\n",
        "                )\n",
        "                \n",
        "            else:\n",
        "                # print(\"hidden states shape : \", hidden_states.shape)\n",
        "                # print('attention_mask shape : ', attention_mask.shape)\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                    \n",
        "                \n",
        "                )\n",
        "                \n",
        "            hidden_states = layer_outputs[0]\n",
        "            \n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "         \n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "            \n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                    \n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        \n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state = hidden_states,\n",
        "            past_key_values = next_decoder_cache,\n",
        "            hidden_states = all_hidden_states,\n",
        "            attentions = all_self_attentions,\n",
        "            cross_attentions = all_cross_attentions\n",
        "            \n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6F_nfly0HgKR"
      },
      "outputs": [],
      "source": [
        "class MultiModalBertClassification(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, num_classes)\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = torch.relu(hidden_states)\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        return hidden_states\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "es1L0jZkUFTj"
      },
      "outputs": [],
      "source": [
        "class MultiModalBertModel(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        \n",
        "        super().__init__(config)\n",
        "        \n",
        "        self.config = config\n",
        "        \n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = MultimodalBertEncoder(config)\n",
        "        # self.encoder = BertEncoder(config)\n",
        "        \n",
        "        self.output = MultiModalBertClassification(config.hidden_size, num_classes = 2)\n",
        "        \n",
        "        self.post_init()\n",
        "    \n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "   \n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "    \n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "            \n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        acoustic_input: Optional[torch.Tensor] = None,\n",
        "        visual_input: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None\n",
        "    )   -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
        "        \n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.output_hidden_states\n",
        "        \n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "        \n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError('You can not  specify both input_ids and input_embeds at the same time')\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError('You have to specify either input_ids or inputs_embeds')\n",
        "        \n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "        \n",
        "        past_key_value_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "        \n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_value_length)), device = device)\n",
        "        \n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, 'token_type_ids'):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype = torch.long, device = device)\n",
        "        \n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
        "\n",
        "        # print(\"attention mask shape : \", attention_mask.shape)\n",
        "        # print(\"extended attention mask shape : \", extended_attention_mask.shape)\n",
        "        \n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device = device)\n",
        "            \n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        \n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "        \n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "        \n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids = input_ids,\n",
        "            position_ids = position_ids,\n",
        "            token_type_ids = token_type_ids,\n",
        "            inputs_embeds = inputs_embeds,\n",
        "            past_key_values_length = past_key_value_length\n",
        "        )\n",
        "        \n",
        "        # print(\"attention mask shape 2 : \", attention_mask.shape)\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask = extended_attention_mask,\n",
        "            acoustic_input = acoustic_input,\n",
        "            visual_input = visual_input,\n",
        "            head_mask = head_mask,\n",
        "            encoder_hidden_states = encoder_hidden_states,\n",
        "            encoder_attention_mask = encoder_extended_attention_mask,\n",
        "            past_key_values = past_key_values,\n",
        "            use_cache = use_cache,\n",
        "            output_attentions = output_attentions,\n",
        "            output_hidden_states = output_hidden_states,\n",
        "            return_dict = return_dict\n",
        "        )\n",
        "        \n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.output(sequence_output) \n",
        "        \n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "        \n",
        "        # print(\"pooled output shape : \", pooled_output.shape)\n",
        "        # print(\"labels shape : \", labels.shape)\n",
        "        pooled_output = pooled_output[:, 0, :]\n",
        "        loss = loss_fn(pooled_output, labels)\n",
        "        \n",
        "        temp_dict = {}\n",
        "        \n",
        "        temp_dict['logits'] = pooled_output\n",
        "        temp_dict['loss'] = loss\n",
        "        \n",
        "        return temp_dict\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m4lcNczUHpOH"
      },
      "outputs": [],
      "source": [
        "# def audio_video_broadcast(x):\n",
        "# #     z = torch.empty()\n",
        "#     temp_all = torch.Tensor()\n",
        "#     for j in range(x.shape[0]):\n",
        "#         print(\"j : \", j)\n",
        "#         temp_x = x[j,:]\n",
        "# #         print(\"Temp x shape : \", temp_x.shape)\n",
        "#         temp_x = torch.tensor(temp_x, dtype=torch.float)\n",
        "#         temp_x = torch.broadcast_to(temp_x, (SOURCE_MAX_LEN, temp_x.shape[0]))\n",
        "# #         print(\"Temp x shape : \", temp_x.shape)\n",
        "#         temp_x = temp_x.unsqueeze(0)\n",
        "# #         print(\"Temp x shape : \", temp_x.shape)\n",
        "        \n",
        "#         if(j==0):\n",
        "#             temp_all = temp_x\n",
        "#         else:\n",
        "#             temp_all = torch.cat([temp_all, temp_x], dim = 0)\n",
        "        \n",
        "        \n",
        "#     return temp_all "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "inWKX8K6GV50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f2d947-bcd6-49a8-d4d8-bcfe7de17aa8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "foldNum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RunFr5b0Ni1u"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/train_audio_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  train_audio_data_utterance1 = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pGFk4HCwOo-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa3cd1c5-d48c-487b-f2ab-64b7e12fc8e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "type(train_audio_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sK4GbT8ZOq2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e77eef-cdf7-421c-d7a2-236e098f80fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "552"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(train_audio_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "X73Hp5HxOsEl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yM1zXLalOifj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tvBQbRyFN5zu"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/test_audio_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  test_audio_data_utterance1 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZctmFGmNQsMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd1e568-06aa-4011-be7a-2986f197d1f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "len(test_audio_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Dz2xdgV7PzbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362e61ae-f461-4e34-aff3-278226cbb535"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "len(test_audio_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Yw0sc0aaNzOO"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/train_video_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  train_image_data_utterance1 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yT8QM77QvJS",
        "outputId": "85e09ad7-1990-4ff9-e4dc-0d8018048c96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "552"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "len(train_image_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "aUEvmMBNOPFE"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/test_video_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "  test_image_data_utterance1 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm6Z6Fc0QxSW",
        "outputId": "858fca28-7695-4c5d-8d22-f6dfed97f37b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "len(test_image_data_utterance1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIEMeHJ6RoXS",
        "outputId": "ec4155b6-475c-47ad-9740-9ec1ffd17fac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "tp = torch.ones(4,5)\n",
        "tp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8vsmq4FS48j",
        "outputId": "e7775f78-ef72-47cb-974c-af2c7063be6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "torch.zeros(5 - tp.shape[0], 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CNdKCeQRrN8",
        "outputId": "df77e4f4-07c1-43d7-9311-1305e752a57f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "torch.cat([tp, torch.zeros(5 - tp.shape[0], 5)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qO9elV8EQAi1"
      },
      "outputs": [],
      "source": [
        "def pad_seq(tensor, dim, max_len):\n",
        "  if max_len > tensor.shape[0] :\n",
        "    return torch.cat([tensor, torch.zeros(max_len - tensor.shape[0], dim)])\n",
        "  else:\n",
        "    return tensor[:max_len]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmBt6xZ1TElw",
        "outputId": "0a71dcfb-08e8-4214-b7d3-b01a2f99b4c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "ACOUSTIC_DIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cUPe2LxQ04S",
        "outputId": "ae753af1-a139-40bb-ec7a-68a70f230285"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552, 1000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "train_audio_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = ACOUSTIC_DIM,\n",
        "                                                   max_len = ACOUSTIC_MAX_LEN)\n",
        "                                                  for a in train_audio_data_utterance1], 0)\n",
        "train_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6-frFIpTzrW",
        "outputId": "469a88e0-d7f9-4b40-d59d-b74bc8eb632c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138, 1000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "test_audio_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = ACOUSTIC_DIM,\n",
        "                                                   max_len = ACOUSTIC_MAX_LEN)\n",
        "                                                  for a in test_audio_data_utterance1], 0)\n",
        "test_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEDVn1TaUicC",
        "outputId": "c02f33ef-bdec-486e-c6c8-583dcbfb29a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2048"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "VISUAL_DIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ArX7k9VUkWO",
        "outputId": "834a0967-fccf-465f-98f3-5dc764acdf35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "480"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "VISUAL_MAX_LEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PO2OLh5yUTF2",
        "outputId": "5756544c-e690-49b1-b949-89d6d8857246"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552, 480, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "train_image_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = VISUAL_DIM,\n",
        "                                                   max_len = VISUAL_MAX_LEN)\n",
        "                                                  for a in train_image_data_utterance1], 0)\n",
        "train_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BAAQoCcUv34",
        "outputId": "e2911e39-3896-4162-cdf2-d5b04e520825"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138, 480, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "test_image_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = VISUAL_DIM,\n",
        "                                                   max_len = VISUAL_MAX_LEN)\n",
        "                                                  for a in test_image_data_utterance1], 0)\n",
        "test_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "TTMNyPhSUdBs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "j3V8UO6HH0xE"
      },
      "outputs": [],
      "source": [
        "# path = \"/content/drive/MyDrive/Colab Notebooks/32/datasetTrue_original/sarcasmDataset_speaker_dependent_True.npz\"\n",
        "# data2 = np.load(path, mmap_mode=True)   \n",
        "\n",
        "# train_audio_data_utterance1 = data2['feautesUA_train'][foldNum]\n",
        "# train_image_data_utterance1 = data2['feautesUV_train'][foldNum]\n",
        "\n",
        "# test_audio_data_utterance1 = data2['feautesUA_test'][foldNum]\n",
        "# test_image_data_utterance1 = data2['feautesUV_test'][foldNum]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "pdf5hGlVH5Zw"
      },
      "outputs": [],
      "source": [
        "# model =  MultiModalRobertaModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
        "# print(model)\n",
        "\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# print(tokenizer)\n",
        "\n",
        "# num_param = sum(p.numel() for p in model.parameters())\n",
        "# print(\"Total parameters : \", num_param/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "nYjq5Tz4ceZa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "540707edcb21450fa4c581e7f4d2a54c",
            "bfd192ef777c44829b12b59eeb82024f",
            "3a71db66c74f4daf8420e931e1abd8da",
            "6e28c23c5f014dc3b011977218b4df0b",
            "de54b013b09144558d69c76b93f4e7c5",
            "b5615c47b00043bfbb43af2447cbab67",
            "d8c1ca111f974c1c9bad380515024fa2",
            "6ce39296ebb44d41be54e2f1aa2a0693",
            "0dbba3c22fae4f3e9f35fd57a822cc6e",
            "1916ffc68e854a50b4734dcf16bd0926",
            "469ca1475f6047adbfadf77fc98d7c33",
            "018d4394ad4e47038571687c464a3374",
            "8fdf9fdf454e4d72805c5d5a8d601007",
            "2b3e13d33c014f68ba1eefcfb6a8e0f8",
            "951af7c7798c4690a35c39513813fc70",
            "353e8db282454c91b5cd5a8cd4dfaa54",
            "45fa65065f11426aae169bb37cfe2af5",
            "5d933e630c744bd197e7436b134bcece",
            "a74d0ab8132e47eda68cd3ca03ce43f8",
            "0e3fc36ca92b4ca3bd221449d727152f",
            "cef6da4a8b3c4d268f4863f8d57ee75b",
            "dde44b6dafb24afabd05ab1a92e147d3",
            "c882505f77be4351b7187f6aeac604ec",
            "650c0c58cd2248bfb4b2585e8fce6f34",
            "a6ca7642924a4bdb83319dbfe87ede90",
            "e821d9ab810f4c24b72576709d21a50d",
            "aa02cf1548cb4780ad7334b1a37e2b7a",
            "ce90b56e8af7448ca7102c60c018ac0c",
            "b3c055d09d5647e8b025fbaead0d69ed",
            "6222788ac421407d88972e3bf2716ef4",
            "aa19786307a6433ea3f78dd9b7f62af3",
            "71ff6135352f4bf3a9bade07465ed35c",
            "69e28c7607474d69aa0f620ace86f329",
            "869dfd3e853a4156bad48ea82efb79fa",
            "ef340412d28847ea822676dc910dbb12",
            "8954cb47c03e4c9a8267f048b1d5be97",
            "bdb28610d5ae4af7bd626e57842c61bd",
            "86b1b52682d84608bed5562a171c6370",
            "cae7d0c441e446dfb485b8bbface2b07",
            "71462387878b49cdb31c6a6726c47776",
            "b4065b5541d64140b60e7cc331b7b038",
            "270561b7fd124853ae570ae37e780fae",
            "e72975bd48594f1293be3e4cd765bb69",
            "483f58f12d074569a68ce5c825793ad0"
          ]
        },
        "outputId": "14bf6cf3-84da-4959-cc1f-f8521ef84506"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "540707edcb21450fa4c581e7f4d2a54c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "018d4394ad4e47038571687c464a3374"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MultiModalBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing MultiModalBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MultiModalBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of MultiModalBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.visual_dim.weight', 'bert.encoder.concat_linear.weight', 'bert.output.dense.bias', 'bert.encoder.visual_context_transform.weight', 'bert.encoder.acoustic_context_transform.weight', 'bert.output.dense.weight', 'bert.encoder.acoustic_dim.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiModalBertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): MultimodalBertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (acoustic_context_transform): Linear(in_features=1000, out_features=500, bias=False)\n",
            "    (visual_context_transform): Linear(in_features=480, out_features=500, bias=False)\n",
            "    (acoustic_dim): Linear(in_features=768, out_features=768, bias=False)\n",
            "    (visual_dim): Linear(in_features=2048, out_features=768, bias=False)\n",
            "    (concat_linear): Linear(in_features=2304, out_features=768, bias=False)\n",
            "  )\n",
            "  (output): MultiModalBertClassification(\n",
            "    (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c882505f77be4351b7187f6aeac604ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "869dfd3e853a4156bad48ea82efb79fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
            "Total trainanable parameters :  113.565346\n"
          ]
        }
      ],
      "source": [
        "model = MultiModalBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "\n",
        "print(model)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(tokenizer)\n",
        "\n",
        "num_param = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total trainanable parameters : \", num_param/1e6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "7t2C-xFIQXla"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "n_N4HcMQ9hXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e6d63d-c3b2-4110-a025-5155fa303744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count :  0  name :  embeddings.word_embeddings.weight\n",
            "Count :  1  name :  embeddings.position_embeddings.weight\n",
            "Count :  2  name :  embeddings.token_type_embeddings.weight\n",
            "Count :  3  name :  embeddings.LayerNorm.weight\n",
            "Count :  4  name :  embeddings.LayerNorm.bias\n",
            "Count :  5  name :  encoder.layer.0.attention.self.query.weight\n",
            "Count :  6  name :  encoder.layer.0.attention.self.query.bias\n",
            "Count :  7  name :  encoder.layer.0.attention.self.key.weight\n",
            "Count :  8  name :  encoder.layer.0.attention.self.key.bias\n",
            "Count :  9  name :  encoder.layer.0.attention.self.value.weight\n",
            "Count :  10  name :  encoder.layer.0.attention.self.value.bias\n",
            "Count :  11  name :  encoder.layer.0.attention.output.dense.weight\n",
            "Count :  12  name :  encoder.layer.0.attention.output.dense.bias\n",
            "Count :  13  name :  encoder.layer.0.attention.output.LayerNorm.weight\n",
            "Count :  14  name :  encoder.layer.0.attention.output.LayerNorm.bias\n",
            "Count :  15  name :  encoder.layer.0.intermediate.dense.weight\n",
            "Count :  16  name :  encoder.layer.0.intermediate.dense.bias\n",
            "Count :  17  name :  encoder.layer.0.output.dense.weight\n",
            "Count :  18  name :  encoder.layer.0.output.dense.bias\n",
            "Count :  19  name :  encoder.layer.0.output.LayerNorm.weight\n",
            "Count :  20  name :  encoder.layer.0.output.LayerNorm.bias\n",
            "Count :  21  name :  encoder.layer.1.attention.self.query.weight\n",
            "Count :  22  name :  encoder.layer.1.attention.self.query.bias\n",
            "Count :  23  name :  encoder.layer.1.attention.self.key.weight\n",
            "Count :  24  name :  encoder.layer.1.attention.self.key.bias\n",
            "Count :  25  name :  encoder.layer.1.attention.self.value.weight\n",
            "Count :  26  name :  encoder.layer.1.attention.self.value.bias\n",
            "Count :  27  name :  encoder.layer.1.attention.output.dense.weight\n",
            "Count :  28  name :  encoder.layer.1.attention.output.dense.bias\n",
            "Count :  29  name :  encoder.layer.1.attention.output.LayerNorm.weight\n",
            "Count :  30  name :  encoder.layer.1.attention.output.LayerNorm.bias\n",
            "Count :  31  name :  encoder.layer.1.intermediate.dense.weight\n",
            "Count :  32  name :  encoder.layer.1.intermediate.dense.bias\n",
            "Count :  33  name :  encoder.layer.1.output.dense.weight\n",
            "Count :  34  name :  encoder.layer.1.output.dense.bias\n",
            "Count :  35  name :  encoder.layer.1.output.LayerNorm.weight\n",
            "Count :  36  name :  encoder.layer.1.output.LayerNorm.bias\n",
            "Count :  37  name :  encoder.layer.2.attention.self.query.weight\n",
            "Count :  38  name :  encoder.layer.2.attention.self.query.bias\n",
            "Count :  39  name :  encoder.layer.2.attention.self.key.weight\n",
            "Count :  40  name :  encoder.layer.2.attention.self.key.bias\n",
            "Count :  41  name :  encoder.layer.2.attention.self.value.weight\n",
            "Count :  42  name :  encoder.layer.2.attention.self.value.bias\n",
            "Count :  43  name :  encoder.layer.2.attention.output.dense.weight\n",
            "Count :  44  name :  encoder.layer.2.attention.output.dense.bias\n",
            "Count :  45  name :  encoder.layer.2.attention.output.LayerNorm.weight\n",
            "Count :  46  name :  encoder.layer.2.attention.output.LayerNorm.bias\n",
            "Count :  47  name :  encoder.layer.2.intermediate.dense.weight\n",
            "Count :  48  name :  encoder.layer.2.intermediate.dense.bias\n",
            "Count :  49  name :  encoder.layer.2.output.dense.weight\n",
            "Count :  50  name :  encoder.layer.2.output.dense.bias\n",
            "Count :  51  name :  encoder.layer.2.output.LayerNorm.weight\n",
            "Count :  52  name :  encoder.layer.2.output.LayerNorm.bias\n",
            "Count :  53  name :  encoder.layer.3.attention.self.query.weight\n",
            "Count :  54  name :  encoder.layer.3.attention.self.query.bias\n",
            "Count :  55  name :  encoder.layer.3.attention.self.key.weight\n",
            "Count :  56  name :  encoder.layer.3.attention.self.key.bias\n",
            "Count :  57  name :  encoder.layer.3.attention.self.value.weight\n",
            "Count :  58  name :  encoder.layer.3.attention.self.value.bias\n",
            "Count :  59  name :  encoder.layer.3.attention.output.dense.weight\n",
            "Count :  60  name :  encoder.layer.3.attention.output.dense.bias\n",
            "Count :  61  name :  encoder.layer.3.attention.output.LayerNorm.weight\n",
            "Count :  62  name :  encoder.layer.3.attention.output.LayerNorm.bias\n",
            "Count :  63  name :  encoder.layer.3.intermediate.dense.weight\n",
            "Count :  64  name :  encoder.layer.3.intermediate.dense.bias\n",
            "Count :  65  name :  encoder.layer.3.output.dense.weight\n",
            "Count :  66  name :  encoder.layer.3.output.dense.bias\n",
            "Count :  67  name :  encoder.layer.3.output.LayerNorm.weight\n",
            "Count :  68  name :  encoder.layer.3.output.LayerNorm.bias\n",
            "Count :  69  name :  encoder.layer.4.attention.self.query.weight\n",
            "Count :  70  name :  encoder.layer.4.attention.self.query.bias\n",
            "Count :  71  name :  encoder.layer.4.attention.self.key.weight\n",
            "Count :  72  name :  encoder.layer.4.attention.self.key.bias\n",
            "Count :  73  name :  encoder.layer.4.attention.self.value.weight\n",
            "Count :  74  name :  encoder.layer.4.attention.self.value.bias\n",
            "Count :  75  name :  encoder.layer.4.attention.output.dense.weight\n",
            "Count :  76  name :  encoder.layer.4.attention.output.dense.bias\n",
            "Count :  77  name :  encoder.layer.4.attention.output.LayerNorm.weight\n",
            "Count :  78  name :  encoder.layer.4.attention.output.LayerNorm.bias\n",
            "Count :  79  name :  encoder.layer.4.intermediate.dense.weight\n",
            "Count :  80  name :  encoder.layer.4.intermediate.dense.bias\n",
            "Count :  81  name :  encoder.layer.4.output.dense.weight\n",
            "Count :  82  name :  encoder.layer.4.output.dense.bias\n",
            "Count :  83  name :  encoder.layer.4.output.LayerNorm.weight\n",
            "Count :  84  name :  encoder.layer.4.output.LayerNorm.bias\n",
            "Count :  85  name :  encoder.layer.5.attention.self.query.weight\n",
            "Count :  86  name :  encoder.layer.5.attention.self.query.bias\n",
            "Count :  87  name :  encoder.layer.5.attention.self.key.weight\n",
            "Count :  88  name :  encoder.layer.5.attention.self.key.bias\n",
            "Count :  89  name :  encoder.layer.5.attention.self.value.weight\n",
            "Count :  90  name :  encoder.layer.5.attention.self.value.bias\n",
            "Count :  91  name :  encoder.layer.5.attention.output.dense.weight\n",
            "Count :  92  name :  encoder.layer.5.attention.output.dense.bias\n",
            "Count :  93  name :  encoder.layer.5.attention.output.LayerNorm.weight\n",
            "Count :  94  name :  encoder.layer.5.attention.output.LayerNorm.bias\n",
            "Count :  95  name :  encoder.layer.5.intermediate.dense.weight\n",
            "Count :  96  name :  encoder.layer.5.intermediate.dense.bias\n",
            "Count :  97  name :  encoder.layer.5.output.dense.weight\n",
            "Count :  98  name :  encoder.layer.5.output.dense.bias\n",
            "Count :  99  name :  encoder.layer.5.output.LayerNorm.weight\n",
            "Count :  100  name :  encoder.layer.5.output.LayerNorm.bias\n",
            "Count :  101  name :  encoder.layer.6.attention.self.query.weight\n",
            "Count :  102  name :  encoder.layer.6.attention.self.query.bias\n",
            "Count :  103  name :  encoder.layer.6.attention.self.key.weight\n",
            "Count :  104  name :  encoder.layer.6.attention.self.key.bias\n",
            "Count :  105  name :  encoder.layer.6.attention.self.value.weight\n",
            "Count :  106  name :  encoder.layer.6.attention.self.value.bias\n",
            "Count :  107  name :  encoder.layer.6.attention.output.dense.weight\n",
            "Count :  108  name :  encoder.layer.6.attention.output.dense.bias\n",
            "Count :  109  name :  encoder.layer.6.attention.output.LayerNorm.weight\n",
            "Count :  110  name :  encoder.layer.6.attention.output.LayerNorm.bias\n",
            "Count :  111  name :  encoder.layer.6.intermediate.dense.weight\n",
            "Count :  112  name :  encoder.layer.6.intermediate.dense.bias\n",
            "Count :  113  name :  encoder.layer.6.output.dense.weight\n",
            "Count :  114  name :  encoder.layer.6.output.dense.bias\n",
            "Count :  115  name :  encoder.layer.6.output.LayerNorm.weight\n",
            "Count :  116  name :  encoder.layer.6.output.LayerNorm.bias\n",
            "Count :  117  name :  encoder.layer.7.attention.self.query.weight\n",
            "Count :  118  name :  encoder.layer.7.attention.self.query.bias\n",
            "Count :  119  name :  encoder.layer.7.attention.self.key.weight\n",
            "Count :  120  name :  encoder.layer.7.attention.self.key.bias\n",
            "Count :  121  name :  encoder.layer.7.attention.self.value.weight\n",
            "Count :  122  name :  encoder.layer.7.attention.self.value.bias\n",
            "Count :  123  name :  encoder.layer.7.attention.output.dense.weight\n",
            "Count :  124  name :  encoder.layer.7.attention.output.dense.bias\n",
            "Count :  125  name :  encoder.layer.7.attention.output.LayerNorm.weight\n",
            "Count :  126  name :  encoder.layer.7.attention.output.LayerNorm.bias\n",
            "Count :  127  name :  encoder.layer.7.intermediate.dense.weight\n",
            "Count :  128  name :  encoder.layer.7.intermediate.dense.bias\n",
            "Count :  129  name :  encoder.layer.7.output.dense.weight\n",
            "Count :  130  name :  encoder.layer.7.output.dense.bias\n",
            "Count :  131  name :  encoder.layer.7.output.LayerNorm.weight\n",
            "Count :  132  name :  encoder.layer.7.output.LayerNorm.bias\n",
            "Count :  133  name :  encoder.layer.8.attention.self.query.weight\n",
            "Count :  134  name :  encoder.layer.8.attention.self.query.bias\n",
            "Count :  135  name :  encoder.layer.8.attention.self.key.weight\n",
            "Count :  136  name :  encoder.layer.8.attention.self.key.bias\n",
            "Count :  137  name :  encoder.layer.8.attention.self.value.weight\n",
            "Count :  138  name :  encoder.layer.8.attention.self.value.bias\n",
            "Count :  139  name :  encoder.layer.8.attention.output.dense.weight\n",
            "Count :  140  name :  encoder.layer.8.attention.output.dense.bias\n",
            "Count :  141  name :  encoder.layer.8.attention.output.LayerNorm.weight\n",
            "Count :  142  name :  encoder.layer.8.attention.output.LayerNorm.bias\n",
            "Count :  143  name :  encoder.layer.8.intermediate.dense.weight\n",
            "Count :  144  name :  encoder.layer.8.intermediate.dense.bias\n",
            "Count :  145  name :  encoder.layer.8.output.dense.weight\n",
            "Count :  146  name :  encoder.layer.8.output.dense.bias\n",
            "Count :  147  name :  encoder.layer.8.output.LayerNorm.weight\n",
            "Count :  148  name :  encoder.layer.8.output.LayerNorm.bias\n",
            "Count :  149  name :  encoder.layer.9.attention.self.query.weight\n",
            "Count :  150  name :  encoder.layer.9.attention.self.query.bias\n",
            "Count :  151  name :  encoder.layer.9.attention.self.key.weight\n",
            "Count :  152  name :  encoder.layer.9.attention.self.key.bias\n",
            "Count :  153  name :  encoder.layer.9.attention.self.value.weight\n",
            "Count :  154  name :  encoder.layer.9.attention.self.value.bias\n",
            "Count :  155  name :  encoder.layer.9.attention.output.dense.weight\n",
            "Count :  156  name :  encoder.layer.9.attention.output.dense.bias\n",
            "Count :  157  name :  encoder.layer.9.attention.output.LayerNorm.weight\n",
            "Count :  158  name :  encoder.layer.9.attention.output.LayerNorm.bias\n",
            "Count :  159  name :  encoder.layer.9.intermediate.dense.weight\n",
            "Count :  160  name :  encoder.layer.9.intermediate.dense.bias\n",
            "Count :  161  name :  encoder.layer.9.output.dense.weight\n",
            "Count :  162  name :  encoder.layer.9.output.dense.bias\n",
            "Count :  163  name :  encoder.layer.9.output.LayerNorm.weight\n",
            "Count :  164  name :  encoder.layer.9.output.LayerNorm.bias\n",
            "Count :  165  name :  encoder.layer.10.attention.self.query.weight\n",
            "Count :  166  name :  encoder.layer.10.attention.self.query.bias\n",
            "Count :  167  name :  encoder.layer.10.attention.self.key.weight\n",
            "Count :  168  name :  encoder.layer.10.attention.self.key.bias\n",
            "Count :  169  name :  encoder.layer.10.attention.self.value.weight\n",
            "Count :  170  name :  encoder.layer.10.attention.self.value.bias\n",
            "Count :  171  name :  encoder.layer.10.attention.output.dense.weight\n",
            "Count :  172  name :  encoder.layer.10.attention.output.dense.bias\n",
            "Count :  173  name :  encoder.layer.10.attention.output.LayerNorm.weight\n",
            "Count :  174  name :  encoder.layer.10.attention.output.LayerNorm.bias\n",
            "Count :  175  name :  encoder.layer.10.intermediate.dense.weight\n",
            "Count :  176  name :  encoder.layer.10.intermediate.dense.bias\n",
            "Count :  177  name :  encoder.layer.10.output.dense.weight\n",
            "Count :  178  name :  encoder.layer.10.output.dense.bias\n",
            "Count :  179  name :  encoder.layer.10.output.LayerNorm.weight\n",
            "Count :  180  name :  encoder.layer.10.output.LayerNorm.bias\n",
            "Count :  181  name :  encoder.layer.11.attention.self.query.weight\n",
            "Count :  182  name :  encoder.layer.11.attention.self.query.bias\n",
            "Count :  183  name :  encoder.layer.11.attention.self.key.weight\n",
            "Count :  184  name :  encoder.layer.11.attention.self.key.bias\n",
            "Count :  185  name :  encoder.layer.11.attention.self.value.weight\n",
            "Count :  186  name :  encoder.layer.11.attention.self.value.bias\n",
            "Count :  187  name :  encoder.layer.11.attention.output.dense.weight\n",
            "Count :  188  name :  encoder.layer.11.attention.output.dense.bias\n",
            "Count :  189  name :  encoder.layer.11.attention.output.LayerNorm.weight\n",
            "Count :  190  name :  encoder.layer.11.attention.output.LayerNorm.bias\n",
            "Count :  191  name :  encoder.layer.11.intermediate.dense.weight\n",
            "Count :  192  name :  encoder.layer.11.intermediate.dense.bias\n",
            "Count :  193  name :  encoder.layer.11.output.dense.weight\n",
            "Count :  194  name :  encoder.layer.11.output.dense.bias\n",
            "Count :  195  name :  encoder.layer.11.output.LayerNorm.weight\n",
            "Count :  196  name :  encoder.layer.11.output.LayerNorm.bias\n",
            "Count :  197  name :  encoder.acoustic_context_transform.weight\n",
            "Count :  198  name :  encoder.visual_context_transform.weight\n",
            "Count :  199  name :  encoder.acoustic_dim.weight\n",
            "Count :  200  name :  encoder.visual_dim.weight\n",
            "Count :  201  name :  encoder.concat_linear.weight\n",
            "Count :  202  name :  output.dense.weight\n",
            "Count :  203  name :  output.dense.bias\n"
          ]
        }
      ],
      "source": [
        "cnt = 0\n",
        "for name, param in model.named_parameters():\n",
        "    print(\"Count : \", cnt, \" name : \", name)\n",
        "    cnt+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "feA-7jSwIBQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7cd94b-6613-43a0-ddc8-a947fd60de62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count :  197  name :  encoder.acoustic_context_transform.weight\n",
            "Count :  198  name :  encoder.visual_context_transform.weight\n",
            "Count :  199  name :  encoder.acoustic_dim.weight\n",
            "Count :  200  name :  encoder.visual_dim.weight\n",
            "Count :  201  name :  encoder.concat_linear.weight\n",
            "Count :  202  name :  output.dense.weight\n",
            "Count :  203  name :  output.dense.bias\n",
            "Total trainanable parameters :  4.673698\n"
          ]
        }
      ],
      "source": [
        "cnt = 0\n",
        "for name, param in model.named_parameters():\n",
        "    \n",
        "    if(cnt>=197):\n",
        "    # if(cnt>=389):  \n",
        "    \n",
        "        param.requires_grad = True\n",
        "        print(\"Count : \", cnt, \" name : \", name)\n",
        "\n",
        "    else:\n",
        "        param.requires_grad = False    \n",
        "    cnt+=1\n",
        "        \n",
        "\n",
        "num_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total trainanable parameters : \", num_param/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "amBD8a4_rhUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481731a7-ec49-4407-c992-63328b41af37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "foldNum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "I1NGerbQcymT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "46OM_4HZjKNp"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/32/json_file_fold.p\", \"rb\") as f:\n",
        "    text_file = pickle.load(f)\n",
        "\n",
        "train_text = text_file[\"json_file_list_\" + str(foldNum) +\"_train\"]\n",
        "test_text =  text_file[\"json_file_list_\" + str(foldNum) +\"_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "HaH0ocF_zD0O"
      },
      "outputs": [],
      "source": [
        "# Trainlen = 483"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "sL8tkEu3kCrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16457419-16be-4282-cd0d-a2af67967cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([552, 1000, 768])\n"
          ]
        }
      ],
      "source": [
        "print(type(train_audio_data_utterance1))\n",
        "print(train_audio_data_utterance1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "DRRQRZeBNYum"
      },
      "outputs": [],
      "source": [
        "# combined_train_data = []\n",
        "# for j in range(len(train_text)):\n",
        "#   temp_text = train_text[j]\n",
        "#   temp_audio = train_audio_data_utterance1[j]\n",
        "#   temp_image = train_image_data_utterance1[j]\n",
        "\n",
        "#   temp_list = []\n",
        "#   temp_list.append(temp_text)\n",
        "#   temp_list.append(temp_audio)\n",
        "#   temp_list.append(temp_image)\n",
        "\n",
        "#   combined_train_data.append(temp_list)\n",
        "\n",
        "# random.shuffle(combined_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ZGx2mZfPOWjo"
      },
      "outputs": [],
      "source": [
        "# train_text2 = []\n",
        "# train_audio_utterance2 = []\n",
        "# train_image_utterance2 = []\n",
        "\n",
        "# for j in combined_train_data:\n",
        "#   train_text2.append(j[0])\n",
        "#   train_audio_utterance2.append(j[1])\n",
        "#   train_image_utterance2.append(j[2])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Qe5xfQYURWrs"
      },
      "outputs": [],
      "source": [
        "# print(len(train_text2))\n",
        "# print(len(train_audio_utterance2))\n",
        "# print(len(train_image_utterance2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "nPa_FWppRfpn"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance2 = torch.tensor(train_audio_utterance2)\n",
        "# train_audio_data_utterance2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "uuHaeA7JiRQz"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance2 = torch.tensor(train_image_utterance2)\n",
        "# train_image_data_utterance2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "mQCW6oZTifhd"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance = torch.tensor(train_audio_data_utterance2)[:Trainlen]\n",
        "# # train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "984ZSUXVWQfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019cc783-913f-4148-d7a3-00ea9be464a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552, 1000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "train_audio_data_utterance = torch.tensor(train_audio_data_utterance1)\n",
        "# train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "eYe20QnlkJwg"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance = torch.tensor(train_audio_data_utterance1)[:Trainlen]\n",
        "# # train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "VLqtw_6-irl3"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance = torch.tensor(train_image_data_utterance2)[:Trainlen]\n",
        "# # train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "AleXnjQNWTKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a616992d-a8aa-4635-aee6-23fca5a0def3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552, 480, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "train_image_data_utterance = torch.tensor(train_image_data_utterance1)\n",
        "# train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "G1NXg6uikX5P"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance = torch.tensor(train_image_data_utterance1)[:Trainlen]\n",
        "# # train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "YkC6buI-iu39"
      },
      "outputs": [],
      "source": [
        "# valid_audio_data_utterance = torch.tensor(train_audio_data_utterance2)[Trainlen:]\n",
        "# valid_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "NU5wt_vv998E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18bbdea3-2e6b-41dd-e79a-7d05913b9a2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "len(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "F4yP2ap7Pl27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2646960d-cc99-4f8a-d4d7-12129aefee83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138, 480, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "test_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "g5rke66XPvU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d18d1259-bab9-4801-d802-7cda6ed7d392"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138, 1000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "test_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "fyi5s9iS9jej"
      },
      "outputs": [],
      "source": [
        "# combined_test_data = []\n",
        "\n",
        "# for j in range(len(test_text)):\n",
        "#   temp_text = test_text[j]\n",
        "#   temp_audio = test_audio_data_utterance1[j]\n",
        "#   temp_image = test_image_data_utterance1[j]\n",
        "\n",
        "#   temp_list = []\n",
        "#   temp_list.append(temp_text)\n",
        "#   temp_list.append(temp_audio)\n",
        "#   temp_list.append(temp_image)\n",
        "\n",
        "#   combined_test_data.append(temp_list)\n",
        "\n",
        "# random.shuffle(combined_test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "YvgGLs8S-cW9"
      },
      "outputs": [],
      "source": [
        "# test_text2 = []\n",
        "# test_audio_utterance2 = []\n",
        "# test_image_utterance2 = []\n",
        "\n",
        "# for j in combined_test_data:\n",
        "#   test_text2.append(j[0])\n",
        "#   test_audio_utterance2.append(j[1])\n",
        "#   test_image_utterance2.append(j[2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "UgCracq6--vg"
      },
      "outputs": [],
      "source": [
        "# VALIDLEN = 69"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "UarPmcdo_uaU"
      },
      "outputs": [],
      "source": [
        "# len(test_audio_utterance2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "CgOQVaRV_-iV"
      },
      "outputs": [],
      "source": [
        "# test_audio_utterance2[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "AsLJeYkvAdoK"
      },
      "outputs": [],
      "source": [
        "# torch.stack(test_audio_utterance2).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "bDznHLJbzAbl"
      },
      "outputs": [],
      "source": [
        "# valid_audio_data_utterance = test_audio_data_utterance1[:VALIDLEN]\n",
        "# valid_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "6XNjZNpBAYd_"
      },
      "outputs": [],
      "source": [
        "# test_audio_data_utterance = test_audio_data_utterance1[VALIDLEN:]\n",
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Y-kqcb9tiyB8"
      },
      "outputs": [],
      "source": [
        "# valid_image_data_utterance = torch.tensor(train_image_data_utterance2)[Trainlen:]\n",
        "# valid_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "z8KNyWBm2Ie4"
      },
      "outputs": [],
      "source": [
        "# valid_image_data_utterance = test_image_data_utterance1[:VALIDLEN]\n",
        "# valid_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "B5rPLXq6At1F"
      },
      "outputs": [],
      "source": [
        "# test_image_data_utterance = test_image_data_utterance1[VALIDLEN:]\n",
        "# test_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "YCxFywiVko_v"
      },
      "outputs": [],
      "source": [
        "# test_audio_data_utterance = torch.tensor(test_audio_data_utterance1)\n",
        "# # test_audio_data_utterance = test_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "hxqP1M_2k2Y_"
      },
      "outputs": [],
      "source": [
        "# test_image_data_utterance = torch.tensor(test_image_data_utterance1)\n",
        "# # test_image_data_utterance = test_image_data_utterance.unsqueeze(dim = 1)\n",
        "# test_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "PKt4UobTIJQY"
      },
      "outputs": [],
      "source": [
        "   \n",
        "\n",
        "# # print(len(text_file[train_text]))\n",
        "# # print(train_text)\n",
        "# # print(len(text_file[test_text]))\n",
        "\n",
        "# train_audio_broadcast_utterance = audio_video_broadcast(train_audio_data_utterance)\n",
        "\n",
        "# print(\"train_audio_broadcast_utterance complete : \", train_audio_broadcast_utterance.shape)\n",
        "# train_image_broadcast_utterance = audio_video_broadcast(train_image_data_utterance)\n",
        "# print(\"train_image_broadcast_utterance complete : \",train_image_broadcast_utterance.shape)\n",
        "\n",
        "# valid_audio_broadcast_utterance = audio_video_broadcast(valid_audio_data_utterance)\n",
        "# print(\"valid_audio_broadcast_utterance complete : \", valid_audio_broadcast_utterance.shape)\n",
        "\n",
        "# valid_image_broadcast_utterance = audio_video_broadcast(valid_image_data_utterance)\n",
        "# print('valid_image_broadcast_utterance complete : ', valid_image_broadcast_utterance.shape)\n",
        "\n",
        "# test_audio_broadcast_utterance = audio_video_broadcast(test_audio_data_utterance)\n",
        "# print(\"test_audio_broadcast_utterance complete : \",test_audio_broadcast_utterance.shape)\n",
        "# test_image_broadcast_utterance = audio_video_broadcast(test_image_data_utterance)\n",
        "# print(\"test_image_broadcast_utterance complete : \",test_image_broadcast_utterance.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "9Ik_vBo7x6Rm"
      },
      "outputs": [],
      "source": [
        "# tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "X0y5DYklxuUT"
      },
      "outputs": [],
      "source": [
        "# p = {\n",
        "#         'additional_special_tokens' : ['[CONTEXT]', '[UTTERANCE]']\n",
        "#     }\n",
        "\n",
        "# tokenizer.add_special_tokens(p)\n",
        "\n",
        "# tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "P1Ctmwokx-T2"
      },
      "outputs": [],
      "source": [
        "# model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "uExwN4zRe0Wq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a75c42-501a-4d11-cdd3-4d365aaab0b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[CONTEXT]', '[UTTERANCE]']})"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "p = {\n",
        "        'additional_special_tokens' : ['[CONTEXT]', '[UTTERANCE]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(p)\n",
        "\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "XRsujb2Nmwks"
      },
      "outputs": [],
      "source": [
        "# tokenizer2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "L_h3zgFvfrGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51abbb9-9dc8-4a6b-ef10-0ca042741616"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30524, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "Mg1kq4a5e3Cp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda9e047-f6cc-4aad-e367-ba65fe16f5e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[CONTEXT]', '[UTTERANCE]']})"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "o7mKmBRp2XqP"
      },
      "outputs": [],
      "source": [
        "# def prepare_dataset_context(text_data):\n",
        "                    \n",
        "\n",
        "#             context = []\n",
        "#             # labels = []\n",
        "#             for i in range(len(text_data)):\n",
        "#                 data_point = text_data[i]\n",
        "\n",
        "#                 # example_speaker = data_point['speaker']\n",
        "#                 # example_utterance = data_point['utterance']\n",
        "#                 # temp_label = int(data_point['sarcasm'])\n",
        "\n",
        "#                 # example_context = '[CONTEXT] '\n",
        "#                 example_context = ''\n",
        "\n",
        "#                 temp_len = len(data_point['context_speakers'])\n",
        "#                 cnt = 0\n",
        "#                 print(\"Temp len : \", temp_len)\n",
        "#                 for speaker, utterance in list(zip(data_point['context_speakers'], data_point['context'])):\n",
        "#                     print(\"count : \", cnt)\n",
        "#                     if(cnt == temp_len - 1):\n",
        "#                       example_context = example_context + speaker.upper() + \" : \" + utterance\n",
        "#                     else:\n",
        "#                       example_context = example_context + speaker.upper() + \" : \" + utterance + \" , \"\n",
        "#                     cnt+=1\n",
        "\n",
        "                    \n",
        "\n",
        "                \n",
        "#                 # print(example_dialog)\n",
        "#                 example_context = re.sub(' +', ' ', example_context)\n",
        "\n",
        "#                 context.append(example_context)\n",
        "#                 # labels.append(temp_label)\n",
        "\n",
        "#             # df = pd.DataFrame(dialog, columns=['dialog'])\n",
        "\n",
        "#             # labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "            \n",
        "\n",
        "#             enc = bert_tokenizer(context, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "\n",
        "#             # df['audio_features'] = acoustic_data\n",
        "#             # df['visual_features'] = visual_data\n",
        "\n",
        "#             return torch.tensor(enc['input_ids'], dtype=torch.long), torch.tensor(enc['attention_mask'], dtype=torch.bool)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "HiN-BMSgISy7"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(text_data):\n",
        "                    \n",
        "\n",
        "            dialog = []\n",
        "            labels = []\n",
        "            for i in range(len(text_data)):\n",
        "                data_point = text_data[i]\n",
        "\n",
        "                example_speaker = data_point['speaker']\n",
        "                example_utterance = data_point['utterance']\n",
        "                temp_label = int(data_point['sarcasm'])\n",
        "\n",
        "                # example_dialog = '[CONTEXT] '\n",
        "                # example_dialog = '[TARGET] '\n",
        "                example_dialog = '[CONTEXT] '\n",
        "\n",
        "\n",
        "                for speaker, utterance in list(zip(data_point['context_speakers'], data_point['context'])):\n",
        "                    example_dialog = example_dialog + speaker.upper() + \" : \" + utterance + \" | \"\n",
        "\n",
        "                example_dialog = example_dialog + ' [UTTERANCE] ' + example_speaker + \" : \" + example_utterance + \" | \"\n",
        "                # example_dialog = example_dialog + example_speaker + \" : \" + example_utterance\n",
        "                # example_dialog = example_dialog + example_speaker + \" : \" + example_utterance \n",
        "                # print(example_dialog)\n",
        "                example_dialog = re.sub(' +', ' ', example_dialog)\n",
        "\n",
        "                dialog.append(example_dialog)\n",
        "                labels.append(temp_label)\n",
        "\n",
        "            # df = pd.DataFrame(dialog, columns=['dialog'])\n",
        "\n",
        "            labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "            \n",
        "\n",
        "            # enc = tokenizer(dialog, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "            enc = tokenizer(dialog, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "\n",
        "            # df['audio_features'] = acoustic_data\n",
        "            # df['visual_features'] = visual_data\n",
        "\n",
        "            return torch.tensor(enc['input_ids'], dtype=torch.long), torch.tensor(enc['attention_mask'], dtype=torch.bool), labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "C2dU7MfiZaDq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "H7VlrC_WI9ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fa05ff5-08c9-448f-c36e-cb8ed1a8e106"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[CONTEXT]', '[UTTERANCE]']})"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "MgoPnVD4qfO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6fd9a7-7a7a-437e-9b1c-da3de15581a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "552"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "len(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "j7p229zVGBUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca043ff-1f0d-492a-ecf2-94ea1ac2fbfd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "train_text_input_ids1, train_text_attention_mask1, train_ground_truth1 = prepare_dataset(train_text)\n",
        "train_ground_truth1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "WdGsoVBDJxLw"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids1, train_text_attention_mask1, train_ground_truth1 = prepare_dataset_utterance(train_text)\n",
        "# train_ground_truth1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "wQdk9Tk8WYzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea1c1dc-41ca-4a09-ed25-791656a0c8ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "train_text_input_ids = train_text_input_ids1\n",
        "train_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "_rqaEYpWZMVx"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids = train_text_input_ids1[:Trainlen]\n",
        "# train_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "BIYgUkhfWcL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f38120-dc38-4893-8d08-8a1893ce9e56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "train_text_attention_mask = train_text_attention_mask1\n",
        "train_text_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "MRRAuyVaaOun"
      },
      "outputs": [],
      "source": [
        "# train_text_attention_mask = train_text_attention_mask1[:Trainlen]\n",
        "# train_text_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "q819IVudYtDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9093b15b-6ba3-409c-a679-649f9a657098"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([552])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "train_ground_truth = train_ground_truth1\n",
        "train_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "U8iuUm6vWeN-"
      },
      "outputs": [],
      "source": [
        "# train_ground_truth = train_ground_truth1[:Trainlen]\n",
        "# train_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "pKXPrrh_jR_O"
      },
      "outputs": [],
      "source": [
        "# train_ground_truth = train_ground_truth1[:Trainlen]\n",
        "# train_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "JAcvsTpeaeyB"
      },
      "outputs": [],
      "source": [
        "# train_ground_truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "bctWHziiWqR0"
      },
      "outputs": [],
      "source": [
        "# context_input_ids, context_attention_mask = prepare_dataset_context(train_text)\n",
        "# print(context_input_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "ScKZf52FX6AD"
      },
      "outputs": [],
      "source": [
        "# context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "P0C9PnF9JypC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c12fb8e-09b0-4d0c-d84e-9481eab14dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TYPE : train_text_input_ids :  <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "print(\"TYPE : train_text_input_ids : \", type(train_text_input_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "yzsrAIieWgvT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# train_context_input_ids = context_input_ids\n",
        "# train_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "815UHYi_juEa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# train_context_input_ids = context_input_ids[:Trainlen]\n",
        "# train_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "KPScSL-aWiyC"
      },
      "outputs": [],
      "source": [
        "# train_context_attention_mask = context_attention_mask\n",
        "# train_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "tEYf1vyblLxm"
      },
      "outputs": [],
      "source": [
        "# train_context_attention_mask = context_attention_mask[:Trainlen]\n",
        "# train_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "cZECJ_D6YwSF"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids = train_text_input_ids1[Trainlen:]\n",
        "# valid_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "6Iqag88ipliS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# valid_text_attention_mask = train_text_attention_mask1[Trainlen:]\n",
        "# valid_text_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "bbjcYjztp523"
      },
      "outputs": [],
      "source": [
        "# valid_ground_truth = train_ground_truth1[Trainlen:]\n",
        "# valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "NDIcnT9t-G1l"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids, valid_text_attention_mask, valid_ground_truth = prepare_dataset_utterance(valid_text)\n",
        "# valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "NkfN5cn6YYoz"
      },
      "outputs": [],
      "source": [
        "# valid_context_input_ids = context_input_ids[Trainlen:]\n",
        "# valid_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "1ogVRlSWrgJf"
      },
      "outputs": [],
      "source": [
        "# valid_context_attention_mask = context_attention_mask[Trainlen:]\n",
        "# valid_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "AuNLAd5CIWLd"
      },
      "outputs": [],
      "source": [
        "# test_text_input_ids, test_text_attention_mask, test_ground_truth = prepare_dataset_utterance(test_text)\n",
        "# test_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "7vB39rShGL4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7de5970-aef8-4b8e-aa14-799d3b7d0d20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "test_text_input_ids, test_text_attention_mask, test_ground_truth = prepare_dataset(test_text)\n",
        "test_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "6dIyrYyhGDFH"
      },
      "outputs": [],
      "source": [
        "# valid_id = test_text_input_ids[:VALID_LEN]\n",
        "# valid_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "LmfvmZTEGbiQ"
      },
      "outputs": [],
      "source": [
        "# test_id = test_text_input_ids[VALIDLEN:]\n",
        "# test_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "0Wi2Kyr5Gm52"
      },
      "outputs": [],
      "source": [
        "# valid_mask = test_text_attention_mask[:VALIDLEN]\n",
        "# valid_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "Z7V0wmGiHAB_"
      },
      "outputs": [],
      "source": [
        "# test_mask = test_text_attention_mask[VALIDLEN:]\n",
        "# test_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "JFZQBCCTHFzt"
      },
      "outputs": [],
      "source": [
        "# valid_truth = test_ground_truth[:VALIDLEN]\n",
        "# valid_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "5wiu7X_lHWFi"
      },
      "outputs": [],
      "source": [
        "# test_truth = test_ground_truth[VALIDLEN:]\n",
        "# test_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "E-ovTep1tWmK"
      },
      "outputs": [],
      "source": [
        "# test_context_input_ids, test_context_attention_mask = prepare_dataset_context(test_text)\n",
        "# test_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "iMO8hPfyHkja"
      },
      "outputs": [],
      "source": [
        "# valid_context_id = test_context_input_ids[:VALIDLEN]\n",
        "# valid_context_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "IG3A9ASzHwW3"
      },
      "outputs": [],
      "source": [
        "# test_context_id = test_context_input_ids[VALIDLEN:]\n",
        "# test_context_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "I1iZshp8tld_"
      },
      "outputs": [],
      "source": [
        "# test_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "doTkSyhKIMOt"
      },
      "outputs": [],
      "source": [
        "# valid_context_mask = test_context_attention_mask[:VALIDLEN]\n",
        "# valid_context_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "UGa0EsBVIpzX"
      },
      "outputs": [],
      "source": [
        "# test_context_mask = test_context_attention_mask[VALIDLEN:]\n",
        "# test_context_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "_0kqUoFDKPkS"
      },
      "outputs": [],
      "source": [
        "# tokenizer.add_tokens(['[CONTEXT]', '[TARGET]'], special_tokens = True)\n",
        "# model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "6EEHQlvrRM3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd9c149-4d50-42c8-e317-14042d12ceff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138, 1000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ],
      "source": [
        "test_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "5PdHAFkTRsyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26cfc308-486f-49f5-ce94-ea390b07c2a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([138, 480, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "test_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "2FilVGy_RxCw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ac1914-f43f-4ab8-af9c-7ede8dd9e261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([138, 500])\n",
            "torch.Size([138, 500])\n",
            "torch.Size([138])\n"
          ]
        }
      ],
      "source": [
        "print(test_text_input_ids.shape)\n",
        "print(test_text_attention_mask.shape)\n",
        "print(test_ground_truth.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "Icba89ykR_5g"
      },
      "outputs": [],
      "source": [
        "# print(test_context_input_ids.shape)\n",
        "# print(test_context_attention_mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "kXqJ0fcjSJ4I"
      },
      "outputs": [],
      "source": [
        "# test_input_data = []\n",
        "\n",
        "\n",
        "# for j in range(test_ground_truth.shape[0]):\n",
        "#   temp_list = []\n",
        "#   temp_list.append(test_text_input_ids[j])\n",
        "#   temp_list.append(test_text_attention_mask[j])\n",
        "#   temp_list.append(test_context_input_ids[j])\n",
        "#   temp_list.append(test_context_attention_mask[j])\n",
        "#   temp_list.append(test_audio_data_utterance1[j])\n",
        "#   temp_list.append(test_image_data_utterance1[j])\n",
        "\n",
        "#   test_input_data.append(temp_list)\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "Z9swXKn_TQsW"
      },
      "outputs": [],
      "source": [
        "# print(type(test_input_data))\n",
        "# print(len(test_input_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "cjn1S99ZS_SX"
      },
      "outputs": [],
      "source": [
        "# test_output_data = test_ground_truth.tolist()\n",
        "# print(type(test_output_data))\n",
        "# print(len(test_output_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "ecXLmWNsT6h-"
      },
      "outputs": [],
      "source": [
        "# X_valid, X_test, Y_valid, Y_test = train_test_split(\n",
        "#     test_input_data, test_output_data, test_size = 0.5, stratify = test_output_data\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "GsXIaU_jUUwH"
      },
      "outputs": [],
      "source": [
        "# len(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "J6lG18S9UcWx"
      },
      "outputs": [],
      "source": [
        "# len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "IfleSSo6UfWk"
      },
      "outputs": [],
      "source": [
        "# len(Y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "vi0Vpd2lUhR7"
      },
      "outputs": [],
      "source": [
        "# len(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "MnzHh4kBUk6f"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids = []\n",
        "# valid_text_attention_mask = []\n",
        "# valid_context_input_ids = []\n",
        "# valid_context_attention_mask = []\n",
        "\n",
        "# valid_audio_data = []\n",
        "# valid_image_data = []\n",
        "\n",
        "# for j in range(len(X_valid)):\n",
        "#   valid_text_input_ids.append(X_valid[j][0])\n",
        "#   valid_text_attention_mask.append(X_valid[j][1])\n",
        "  \n",
        "#   valid_context_input_ids.append(X_valid[j][2])\n",
        "#   valid_context_attention_mask.append(X_valid[j][3])\n",
        "  \n",
        "#   valid_audio_data.append(X_valid[j][4])\n",
        "\n",
        "#   valid_image_data.append(X_valid[j][5])\n",
        "\n",
        "# print(len(valid_text_input_ids))\n",
        "# print(len(valid_text_attention_mask))\n",
        "# print(len(valid_context_input_ids))\n",
        "# print(len(valid_context_attention_mask))\n",
        "# print(len(valid_audio_data))\n",
        "# print(len(valid_image_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "xyqnoPspY06u"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "XoyGWRN9V6C9"
      },
      "outputs": [],
      "source": [
        "# valid_text_input_ids = torch.stack(valid_text_input_ids)\n",
        "# print(valid_text_input_ids.shape)\n",
        "# valid_text_attention_mask = torch.stack(valid_text_attention_mask)\n",
        "# print(valid_text_attention_mask.shape)\n",
        "# valid_context_input_ids = torch.stack(valid_context_input_ids)\n",
        "# print(valid_context_input_ids.shape)\n",
        "# valid_context_attention_mask = torch.stack(valid_context_attention_mask)\n",
        "# print(valid_context_attention_mask.shape)\n",
        "# valid_audio_data = torch.stack(valid_audio_data)\n",
        "# print(valid_audio_data.shape)\n",
        "# valid_image_data = torch.stack(valid_image_data)\n",
        "# print(valid_image_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "ipNEkx0wV2Wz"
      },
      "outputs": [],
      "source": [
        "# valid_ground_truth = torch.tensor(Y_valid)\n",
        "# valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "4FM6TlZIatoC"
      },
      "outputs": [],
      "source": [
        "# test_text_id = []\n",
        "# test_text_mask = []\n",
        "\n",
        "# test_context_id = []\n",
        "# test_context_mask = []\n",
        "\n",
        "# test_audio_data = []\n",
        "# test_image_data = []\n",
        "\n",
        "# for j in range(len(X_test)):\n",
        "#   test_text_id.append(X_test[j][0])\n",
        "#   test_text_mask.append(X_test[j][1])\n",
        "\n",
        "#   test_context_id.append(X_test[j][2])\n",
        "#   test_context_mask.append(X_test[j][3])\n",
        "\n",
        "#   test_audio_data.append(X_test[j][4])\n",
        "\n",
        "#   test_image_data.append(X_test[j][5])\n",
        "\n",
        "# test_text_id = torch.stack(test_text_id)\n",
        "# print(test_text_id.shape)\n",
        "# test_text_mask = torch.stack(test_text_mask)\n",
        "# print(test_text_mask.shape)\n",
        "# test_context_id = torch.stack(test_context_id)\n",
        "# print(test_context_id.shape)\n",
        "# test_context_mask = torch.stack(test_context_mask)\n",
        "# print(test_context_mask.shape)\n",
        "# test_audio_data = torch.stack(test_audio_data)\n",
        "# print(test_audio_data.shape)\n",
        "# test_image_data = torch.stack(test_image_data)\n",
        "# print(test_image_data.shape)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "RiJSqVpFckAT"
      },
      "outputs": [],
      "source": [
        "# test_ground_truth = torch.tensor(Y_test)\n",
        "# print(test_ground_truth.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "TMN_GJV1Jjps"
      },
      "outputs": [],
      "source": [
        "\n",
        "# tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "SY3KgbQUKoFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877bbc8f-76d3-448a-8d01-82fbc2ddc43e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[CONTEXT]', '[UTTERANCE]']"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "tokenizer.all_special_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "M7uPY9VkLRcr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "oGASW03jLFtd"
      },
      "outputs": [],
      "source": [
        "# tokenizer.all_special_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "JVqVQYAULBg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fe1822-e4c2-4a7e-e654-027f7da0f269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100, 102, 0, 101, 103, 30522, 30523]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.all_special_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "hNcRXEYZLK7w"
      },
      "outputs": [],
      "source": [
        "# train_audio_broadcast_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "eP1Y_my0-btZ"
      },
      "outputs": [],
      "source": [
        "# valid_audio_data_utterance = test_audio_data_utterance[:VALID_LEN, :, :]\n",
        "# valid_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "K4c_xiCD-tn0"
      },
      "outputs": [],
      "source": [
        "# valid_image_data_utterance = test_image_data_utterance[:VALID_LEN, :, :]\n",
        "# valid_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "QwLEXwtB-6KN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# test_audio_data_utterance = test_audio_data_utterance[VALID_LEN:, :, :]\n",
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "fF_ErwW5_EsF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# test_image_data_utterance = test_image_data_utterance[VALID_LEN:, :, :]\n",
        "# test_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "q-7eDQZmEhNZ"
      },
      "outputs": [],
      "source": [
        "test_input_data = []\n",
        "\n",
        "\n",
        "for j in range(test_ground_truth.shape[0]):\n",
        "  temp_list = []\n",
        "  temp_list.append(test_text_input_ids[j])\n",
        "  temp_list.append(test_text_attention_mask[j])\n",
        "\n",
        "  temp_list.append(test_audio_data_utterance1[j])\n",
        "  temp_list.append(test_image_data_utterance1[j])\n",
        "\n",
        "  test_input_data.append(temp_list)\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "D5UZkOdvEl6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "340215af-c3f7-45bd-deda-698e50d2b84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "138\n"
          ]
        }
      ],
      "source": [
        "print(type(test_input_data))\n",
        "print(len(test_input_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "rnW2xGe6Enfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad3dd93-0136-4607-9309-e1651a087674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "138\n"
          ]
        }
      ],
      "source": [
        "test_output_data = test_ground_truth.tolist()\n",
        "print(type(test_output_data))\n",
        "print(len(test_output_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "jO18wKiHEoto"
      },
      "outputs": [],
      "source": [
        "X_valid, X_test, Y_valid, Y_test = train_test_split(\n",
        "    test_input_data, test_output_data, test_size = 0.5, stratify = test_output_data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "ZFOS59YEKznY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "fegJT6LBEsM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06aa71d1-9b35-449d-b13f-c673705a3b69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ],
      "source": [
        "len(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "iRaVQfSEEtmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7a5e52-4a19-4393-8bf2-c153a194c680"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ],
      "source": [
        "len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "I_Q2A4vkEu2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfafc3c7-4dde-4a6a-ddce-974e5b227640"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ],
      "source": [
        "len(Y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "MCjoibAAEwTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1cd89c-777e-4861-84fd-0f20015569ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ],
      "source": [
        "len(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "bccK__bzExmv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd067f3e-f2a3-4e50-8a16-fce8c685f744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69\n",
            "69\n",
            "69\n",
            "69\n"
          ]
        }
      ],
      "source": [
        "valid_text_input_ids = []\n",
        "valid_text_attention_mask = []\n",
        "valid_context_input_ids = []\n",
        "valid_context_attention_mask = []\n",
        "\n",
        "valid_audio_data = []\n",
        "valid_image_data = []\n",
        "\n",
        "for j in range(len(X_valid)):\n",
        "  valid_text_input_ids.append(X_valid[j][0])\n",
        "  valid_text_attention_mask.append(X_valid[j][1])\n",
        "  \n",
        "  \n",
        "  valid_audio_data.append(X_valid[j][2])\n",
        "\n",
        "  valid_image_data.append(X_valid[j][3])\n",
        "\n",
        "print(len(valid_text_input_ids))\n",
        "print(len(valid_text_attention_mask))\n",
        "\n",
        "print(len(valid_audio_data))\n",
        "print(len(valid_image_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "pK8TO60JEy_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498eb133-d432-4489-ca7f-e24146dd6b4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([500])"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ],
      "source": [
        "valid_text_input_ids[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "XECiB9_qE0KB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "683c4e5e-01d7-4a40-c424-07c8647b4b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([69, 500])\n",
            "torch.Size([69, 500])\n",
            "torch.Size([69, 1000, 768])\n",
            "torch.Size([69, 480, 2048])\n"
          ]
        }
      ],
      "source": [
        "valid_text_input_ids = torch.stack(valid_text_input_ids)\n",
        "print(valid_text_input_ids.shape)\n",
        "valid_text_attention_mask = torch.stack(valid_text_attention_mask)\n",
        "print(valid_text_attention_mask.shape)\n",
        "\n",
        "valid_audio_data = torch.stack(valid_audio_data)\n",
        "print(valid_audio_data.shape)\n",
        "valid_image_data = torch.stack(valid_image_data)\n",
        "print(valid_image_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "RqE-5ALLE1en",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f71c560-bf17-47f9-e3c3-5eb03bcfc4a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69])"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ],
      "source": [
        "valid_ground_truth = torch.tensor(Y_valid)\n",
        "valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "bTV-5u5IE2zS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73e498c-b734-4ffe-cbe0-524844d44909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([69, 500])\n",
            "torch.Size([69, 500])\n",
            "torch.Size([69, 1000, 768])\n",
            "torch.Size([69, 480, 2048])\n"
          ]
        }
      ],
      "source": [
        "test_text_id = []\n",
        "test_text_mask = []\n",
        "\n",
        "test_context_id = []\n",
        "test_context_mask = []\n",
        "\n",
        "test_audio_data = []\n",
        "test_image_data = []\n",
        "\n",
        "for j in range(len(X_test)):\n",
        "  test_text_id.append(X_test[j][0])\n",
        "  test_text_mask.append(X_test[j][1])\n",
        "\n",
        "\n",
        "  test_audio_data.append(X_test[j][2])\n",
        "\n",
        "  test_image_data.append(X_test[j][3])\n",
        "\n",
        "test_text_id = torch.stack(test_text_id)\n",
        "print(test_text_id.shape)\n",
        "test_text_mask = torch.stack(test_text_mask)\n",
        "print(test_text_mask.shape)\n",
        "\n",
        "test_audio_data = torch.stack(test_audio_data)\n",
        "print(test_audio_data.shape)\n",
        "test_image_data = torch.stack(test_image_data)\n",
        "print(test_image_data.shape)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "s73tO--wE4aD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b1d443-a0d8-4e91-b3a6-bec4d0c7c228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([69])\n"
          ]
        }
      ],
      "source": [
        "test_ground_truth = torch.tensor(Y_test)\n",
        "print(test_ground_truth.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "qrligkveIb4f"
      },
      "outputs": [],
      "source": [
        "class MultimodalSarcasmDataset(Dataset):\n",
        "    # def __init__(self, utterance_input_ids, utterance_attention_mask, context_input_ids, context_attention_mask, acoustic_data, visual_data, labels):\n",
        "    def __init__(self, utterance_input_ids, utterance_attention_mask, acoustic_data, visual_data, labels):\n",
        "\n",
        "        self.utterance_input_ids = utterance_input_ids\n",
        "        self.utterance_attention_mask = utterance_attention_mask\n",
        "        # self.context_input_ids = context_input_ids\n",
        "        # self.context_attention_mask = context_attention_mask\n",
        "        # self.context_attention_mask\n",
        "        self.acoustic_data = acoustic_data\n",
        "        self.visual_data = visual_data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.utterance_input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # return self.utterance_input_ids[idx], self.utterance_attention_mask[idx], self.context_input_ids[idx], self.context_attention_mask[idx], self.acoustic_data[idx], self.visual_data[idx], self.labels[idx]\n",
        "        return self.utterance_input_ids[idx], self.utterance_attention_mask[idx],  self.acoustic_data[idx], self.visual_data[idx], self.labels[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "8IhvAU6M_0mh"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "8UJo5LQ3RjCx"
      },
      "outputs": [],
      "source": [
        "# train_context_attention_mask.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "1oQY9qKjXbPU"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "V1wRtyIFYS2m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "ILQZRYToYYDd"
      },
      "outputs": [],
      "source": [
        "# train_text_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "U3SQyApZYbH7"
      },
      "outputs": [],
      "source": [
        "# train_text_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "_JFdrEk1Yd-J"
      },
      "outputs": [],
      "source": [
        "# train_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "sYiUBLjTYgdj"
      },
      "outputs": [],
      "source": [
        "# train_context_attention_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "lRpLc5f7Yiyz"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "6c8Wsp6wYlcV"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "mBLinpQcYo_4"
      },
      "outputs": [],
      "source": [
        "# train_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "gya_VngQJFlx"
      },
      "outputs": [],
      "source": [
        "# valid_context_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "cvLOlLI1JJmE"
      },
      "outputs": [],
      "source": [
        "# valid_context_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "YOwejwPbJPwB"
      },
      "outputs": [],
      "source": [
        "# valid_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "-KcNwn5qJcGa"
      },
      "outputs": [],
      "source": [
        "# test_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "R_5HoO4YdY0U"
      },
      "outputs": [],
      "source": [
        "# test_context_input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "tYWmjXQZ5y2W"
      },
      "outputs": [],
      "source": [
        "# test_audio_data_utterance1.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "SOHMQ9HIFRnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a8cb82-5fff-4b65-fbad-3de9927ac4bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69, 480, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ],
      "source": [
        "test_image_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "0C6e9caDFSy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d6cb351-3445-4543-bb50-ee2defbbb0b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69, 1000, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ],
      "source": [
        "test_audio_data.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "NEIpohRVFUO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32d1c06-fc1b-4c13-9cc0-608919e34d58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ],
      "source": [
        "test_text_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "3jEKs32xFVsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aedeb374-3519-44b5-8c05-063ed9a3de53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ],
      "source": [
        "test_text_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "qBTmMfh6FXAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13c19dd-9310-43a2-c958-9a9e5cb4d2e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69])"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ],
      "source": [
        "valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "Ux-4pOaOFYYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96608cc6-b53f-43c1-f6c3-1f67333c1cc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([69])"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ],
      "source": [
        "test_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "5j_r6ZqtIeD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9c9d0b-5093-49ab-ae93-04c374769a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7fc7a44d1160>\n"
          ]
        }
      ],
      "source": [
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask, train_context_input_ids, train_context_attention_mask,train_audio_broadcast_utterance, train_image_broadcast_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "# valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask, valid_audio_broadcast_utterance, valid_image_broadcast_utterance, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask, test_context_input_ids, test_context_attention_mask, test_audio_broadcast_utterance, test_image_broadcast_utterance, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask, train_context_input_ids, train_context_attention_mask, train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "# # valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask,   valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask, test_context_input_ids, test_context_attention_mask,  test_audio_data_utterance1, test_image_data_utterance1, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask,  train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "# # valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask,   valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask,   test_audio_data_utterance1, test_image_data_utterance1, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask,  train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=4, shuffle = True)\n",
        "# # valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask,   valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask,   test_audio_data_utterance1, test_image_data_utterance1, test_ground_truth), batch_size=4, shuffle = False)\n",
        "\n",
        "train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask,  train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask,    valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "test_loader = DataLoader(MultimodalSarcasmDataset(test_text_id, test_text_mask,   test_audio_data, test_image_data, test_ground_truth), batch_size=32, shuffle = False)\n",
        "\n",
        "\n",
        "print(test_loader)\n",
        "\n",
        "# print(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "XJYdQp5vwRim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7cca01-d998-485a-d775-b357372a80db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "552"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ],
      "source": [
        "len(train_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "ZYX5DVe_wYtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b19042-f534-44e3-85a1-11184d598bc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ],
      "source": [
        "len(valid_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "NPTKQs5uwdsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516aa9cf-4d20-4165-a942-067a58be5d2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ],
      "source": [
        "len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "44TjmFmSIh9D"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "O1EC_JPPUpq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3fdfc4f-d4c6-4d87-bce7-8754a0c71490"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"_name_or_path\": \"bert-base-uncased\",\n",
              "  \"architectures\": [\n",
              "    \"BertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.26.1\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30524\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "AbrSjHApAMiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad61c97-7b36-4dfb-93ba-a4828ffeffea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ],
      "source": [
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "TF3U8SkdZJ9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3b360d-496e-4b49-90f7-71fa9454ce69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiModalBertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30524, 768)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): MultimodalBertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (acoustic_context_transform): Linear(in_features=1000, out_features=500, bias=False)\n",
              "    (visual_context_transform): Linear(in_features=480, out_features=500, bias=False)\n",
              "    (acoustic_dim): Linear(in_features=768, out_features=768, bias=False)\n",
              "    (visual_dim): Linear(in_features=2048, out_features=768, bias=False)\n",
              "    (concat_linear): Linear(in_features=2304, out_features=768, bias=False)\n",
              "  )\n",
              "  (output): MultiModalBertClassification(\n",
              "    (dense): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ],
      "source": [
        "model = model.to(DEVICE)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "i5MxPWWyIotE"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader):\n",
        "      model.train()\n",
        "      epoch_train_loss = 0.0\n",
        "    \n",
        "      \n",
        "      for step, batch in enumerate(tqdm(data_loader, desc = 'Training Iteration')):\n",
        "        # for i, t in enumerate(batch):\n",
        "        #     print(\"Inside hello\")\n",
        "        #     print(i, \" : \", type(t))\n",
        "        batch = tuple(t.to(DEVICE) for t in batch)\n",
        "        # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "        input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "        # print('\\ninput ids shape : ', input_ids.shape)\n",
        "        # print(\"attention mask shape : \", attention_mask.shape)\n",
        "        # print('acoustic_input shape : ', acoustic_input.shape)\n",
        "        # print('visual_input shape : ', visual_input.shape)\n",
        "        optimizer.zero_grad()\n",
        "        # print(\"Input ids shape : \", input_ids.shape)\n",
        "        # print(\"Input ids shape : \", input_ids.shape)\n",
        "        outputs = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask,\n",
        "                        \n",
        "                        # context_input_ids = context_input_ids,\n",
        "                        # context_attention_mask = context_attention_mask,\n",
        "                        acoustic_input = acoustic_input,\n",
        "                        visual_input = visual_input,\n",
        "                        labels = labels)\n",
        "\n",
        "        # outputs = model2(input_ids = input_ids,\n",
        "        #                 attention_mask = attention_mask,\n",
        "        #                 )\n",
        "        # last_hidden_state = outputs['last_hidde,n_state']\n",
        "        # print('last hidden_state shape : ', last_hidden_state.shape)\n",
        "        loss = outputs['loss']\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "        # print(\"Batch wise loss : \", epoch_train_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    \n",
        "\n",
        "      print(\"Epoch train loss : \", epoch_train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "D0H6Hk2w62xu"
      },
      "outputs": [],
      "source": [
        "def valid_epoch(model, data_loader):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  gold = []\n",
        "\n",
        "  valid_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(tqdm(data_loader)):\n",
        "      batch = tuple(t.to(DEVICE) for t in batch)\n",
        "      # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "      input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "      \n",
        "\n",
        "      outputs = model(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            # context_input_ids = context_input_ids,\n",
        "                            # context_attention_mask = context_attention_mask,\n",
        "                            acoustic_input = acoustic_input,\n",
        "                            visual_input = visual_input,\n",
        "                            labels = labels)\n",
        "      \n",
        "      logits = outputs['logits']\n",
        "      loss = outputs['loss']\n",
        "\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "      pred = logits.argmax(dim = -1)\n",
        "\n",
        "      predictions.extend(pred.tolist())\n",
        "      gold.extend(labels.tolist())\n",
        "\n",
        "  return valid_loss, predictions, gold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "93nGID9-Imr-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def test_epoch(model, data_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    gold = []\n",
        "\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(tqdm(data_loader)):\n",
        "            batch = tuple(t.to(DEVICE) for t in batch)\n",
        "            # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "            input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "\n",
        "            # print(\"attention mask shape : \", attention_mask.shape)\n",
        "\n",
        "            outputs = model(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            # context_input_ids = context_input_ids,\n",
        "                            # context_attention_mask = context_attention_mask,\n",
        "                      \n",
        "                            acoustic_input = acoustic_input,\n",
        "                            visual_input = visual_input,\n",
        "                            labels = labels)\n",
        "\n",
        "            logits = outputs['logits']\n",
        "\n",
        "            pred = logits.argmax(dim = -1)\n",
        "\n",
        "            predictions.extend(pred.tolist())\n",
        "\n",
        "            gold.extend(labels.tolist())\n",
        "\n",
        "            correct += int((pred == labels).sum())\n",
        "\n",
        "    return correct/len(data_loader.dataset), predictions, gold "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "s5QauBgKBtYS"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "  def __init__(self, patience, min_delta):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.min_validation = np.inf\n",
        "\n",
        "  def early_stop(self, valid_loss):\n",
        "    if valid_loss < self.min_validation:\n",
        "      self.min_validation = valid_loss\n",
        "      self.counter = 0\n",
        "    elif valid_loss > (self.min_validation + self.min_delta):\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        return True\n",
        "    return False          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "8dHmyfM2Cc9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba0d040d-61cf-4950-de09-5feb2cbb86de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.EarlyStopping at 0x7fc7a44e5370>"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ],
      "source": [
        "early_stopper = EarlyStopping(patience = 15, min_delta = 0.2)\n",
        "early_stopper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "B0HB4PIc6ixU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_and_validation(model, train_loader, valid_loader):\n",
        "  # lowest_loss = 1e6\n",
        "  best_f1 = 0.0\n",
        "  for epoch in range(30):\n",
        "    print(\"\\n=============Epoch : \", epoch)\n",
        "    train_epoch(model, train_loader)\n",
        "    valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "    if early_stopper.early_stop(valid_loss):\n",
        "      break\n",
        "\n",
        "    print(\"Length of predictions : \", len(valid_pred))\n",
        "    print(\"Length of gold : \", len(valid_gold))\n",
        "    print(\"Valid loss : \", valid_loss)\n",
        "    print(\"\\n Valid Accuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "    print(\"\\n Valid Precision : \", precision_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "    print(\"\\n Valid Recall : \", recall_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "    print(\"\\nValid F1 score : \", f1_score(valid_gold, valid_pred, average = 'weighted')) \n",
        "\n",
        "    \n",
        "    curr_f1 = f1_score(valid_gold, valid_pred, average = 'weighted')\n",
        "\n",
        "    curr_loss = valid_loss\n",
        "    # if((curr_f1 > best_f1) and (epoch>=4)):\n",
        "    if(curr_f1 > best_f1):  \n",
        "    # if(curr_loss < lowest_loss):    \n",
        "      best_f1 = curr_f1\n",
        "      # print(\"Valid pred : \", valid_pred)\n",
        "      # print('valid_gold : ', valid_gold)\n",
        "      # torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/roberta/new_seed/best_model_epoch_'+str(epoch)+'_best_f1_'+str(int(best_f1*100))+'_foldNum_'+str(foldNum)+'.pt')\n",
        "      torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/32/saved_model/bert_only/best_model_epoch_'+str(epoch)+'_best_f1_'+str(int(best_f1*100))+'_foldNum_'+str(foldNum)+'.pt')\n",
        "\n",
        "      print(\"model saved\\n\")\n",
        "      # print(\"best model\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "RMJL4HbkEBh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d7bef2-c825-41a3-88ed-41c3d5c9d720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=============Epoch :  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:14<00:00,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  11.793063044548035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.6938942074775696\n",
            "\n",
            " Valid Accuracy :  0.6956521739130435\n",
            "\n",
            " Valid Precision :  0.7108350586611456\n",
            "\n",
            " Valid Recall :  0.6956521739130435\n",
            "\n",
            "Valid F1 score :  0.6837620035709115\n",
            "model saved\n",
            "\n",
            "\n",
            "=============Epoch :  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  10.823167860507965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.4241958260536194\n",
            "\n",
            " Valid Accuracy :  0.7536231884057971\n",
            "\n",
            " Valid Precision :  0.839100857734398\n",
            "\n",
            " Valid Recall :  0.7536231884057971\n",
            "\n",
            "Valid F1 score :  0.7427371434490683\n",
            "model saved\n",
            "\n",
            "\n",
            "=============Epoch :  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  10.202643066644669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.3984546065330505\n",
            "\n",
            " Valid Accuracy :  0.7391304347826086\n",
            "\n",
            " Valid Precision :  0.8330434782608697\n",
            "\n",
            " Valid Recall :  0.7391304347826086\n",
            "\n",
            "Valid F1 score :  0.7258369943947887\n",
            "\n",
            "=============Epoch :  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  8.913562536239624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.2955211102962494\n",
            "\n",
            " Valid Accuracy :  0.7536231884057971\n",
            "\n",
            " Valid Precision :  0.839100857734398\n",
            "\n",
            " Valid Recall :  0.7536231884057971\n",
            "\n",
            "Valid F1 score :  0.7427371434490683\n",
            "\n",
            "=============Epoch :  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  8.664743959903717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.2311185598373413\n",
            "\n",
            " Valid Accuracy :  0.7391304347826086\n",
            "\n",
            " Valid Precision :  0.7484702093397746\n",
            "\n",
            " Valid Recall :  0.7391304347826086\n",
            "\n",
            "Valid F1 score :  0.7332222390546888\n",
            "\n",
            "=============Epoch :  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  7.139508381485939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.1218978613615036\n",
            "\n",
            " Valid Accuracy :  0.8115942028985508\n",
            "\n",
            " Valid Precision :  0.83727213048528\n",
            "\n",
            " Valid Recall :  0.8115942028985508\n",
            "\n",
            "Valid F1 score :  0.8101647806233869\n",
            "model saved\n",
            "\n",
            "\n",
            "=============Epoch :  6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  5.569907918572426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.5003653764724731\n",
            "\n",
            " Valid Accuracy :  0.7391304347826086\n",
            "\n",
            " Valid Precision :  0.8102139406487232\n",
            "\n",
            " Valid Recall :  0.7391304347826086\n",
            "\n",
            "Valid F1 score :  0.7292353823088455\n",
            "\n",
            "=============Epoch :  7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  4.1973133608698845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.7887021899223328\n",
            "\n",
            " Valid Accuracy :  0.6811594202898551\n",
            "\n",
            " Valid Precision :  0.81105743424584\n",
            "\n",
            " Valid Recall :  0.6811594202898551\n",
            "\n",
            "Valid F1 score :  0.654494309196028\n",
            "\n",
            "=============Epoch :  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  2.9898218885064125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.0836580842733383\n",
            "\n",
            " Valid Accuracy :  0.7971014492753623\n",
            "\n",
            " Valid Precision :  0.8028107158541941\n",
            "\n",
            " Valid Recall :  0.7971014492753623\n",
            "\n",
            "Valid F1 score :  0.7972719522591646\n",
            "\n",
            "=============Epoch :  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  1.5215866975486279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.309374839067459\n",
            "\n",
            " Valid Accuracy :  0.782608695652174\n",
            "\n",
            " Valid Precision :  0.7976960237829803\n",
            "\n",
            " Valid Recall :  0.782608695652174\n",
            "\n",
            "Valid F1 score :  0.7819689062342909\n",
            "\n",
            "=============Epoch :  10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.9116394445300102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.3820438981056213\n",
            "\n",
            " Valid Accuracy :  0.7536231884057971\n",
            "\n",
            " Valid Precision :  0.7679301374953549\n",
            "\n",
            " Valid Recall :  0.7536231884057971\n",
            "\n",
            "Valid F1 score :  0.7528980937321964\n",
            "\n",
            "=============Epoch :  11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.6514099109917879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  2.0522098541259766\n",
            "\n",
            " Valid Accuracy :  0.7101449275362319\n",
            "\n",
            " Valid Precision :  0.7955453852021358\n",
            "\n",
            " Valid Recall :  0.7101449275362319\n",
            "\n",
            "Valid F1 score :  0.6953744382164319\n",
            "\n",
            "=============Epoch :  12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.5268744304776192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  2.191405177116394\n",
            "\n",
            " Valid Accuracy :  0.7536231884057971\n",
            "\n",
            " Valid Precision :  0.839100857734398\n",
            "\n",
            " Valid Recall :  0.7536231884057971\n",
            "\n",
            "Valid F1 score :  0.7427371434490683\n",
            "\n",
            "=============Epoch :  13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.4331874577328563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.7568296492099762\n",
            "\n",
            " Valid Accuracy :  0.7681159420289855\n",
            "\n",
            " Valid Precision :  0.7869065467266366\n",
            "\n",
            " Valid Recall :  0.7681159420289855\n",
            "\n",
            "Valid F1 score :  0.7669448104230713\n",
            "\n",
            "=============Epoch :  14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.6031566392630339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.6245561242103577\n",
            "\n",
            " Valid Accuracy :  0.7536231884057971\n",
            "\n",
            " Valid Precision :  0.7570941420046279\n",
            "\n",
            " Valid Recall :  0.7536231884057971\n",
            "\n",
            "Valid F1 score :  0.7539339432659117\n",
            "\n",
            "=============Epoch :  15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.2873191200196743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.898152768611908\n",
            "\n",
            " Valid Accuracy :  0.782608695652174\n",
            "\n",
            " Valid Precision :  0.8068095743069231\n",
            "\n",
            " Valid Recall :  0.782608695652174\n",
            "\n",
            "Valid F1 score :  0.7809593622577542\n",
            "\n",
            "=============Epoch :  16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.23329328931868076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.8012375831604004\n",
            "\n",
            " Valid Accuracy :  0.7681159420289855\n",
            "\n",
            " Valid Precision :  0.7792623213011491\n",
            "\n",
            " Valid Recall :  0.7681159420289855\n",
            "\n",
            "Valid F1 score :  0.7678236511996103\n",
            "\n",
            "=============Epoch :  17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.16901805298402905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.9881378412246704\n",
            "\n",
            " Valid Accuracy :  0.782608695652174\n",
            "\n",
            " Valid Precision :  0.8068095743069231\n",
            "\n",
            " Valid Recall :  0.782608695652174\n",
            "\n",
            "Valid F1 score :  0.7809593622577542\n",
            "\n",
            "=============Epoch :  18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.1560749216005206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.9261693358421326\n",
            "\n",
            " Valid Accuracy :  0.782608695652174\n",
            "\n",
            " Valid Precision :  0.8068095743069231\n",
            "\n",
            " Valid Recall :  0.782608695652174\n",
            "\n",
            "Valid F1 score :  0.7809593622577542\n",
            "\n",
            "=============Epoch :  19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.12879068218171597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.8799075484275818\n",
            "\n",
            " Valid Accuracy :  0.8115942028985508\n",
            "\n",
            " Valid Precision :  0.820162553858206\n",
            "\n",
            " Valid Recall :  0.8115942028985508\n",
            "\n",
            "Valid F1 score :  0.8115942028985508\n",
            "model saved\n",
            "\n",
            "\n",
            "=============Epoch :  20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.10800815001130104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  2.006830096244812\n",
            "\n",
            " Valid Accuracy :  0.782608695652174\n",
            "\n",
            " Valid Precision :  0.8068095743069231\n",
            "\n",
            " Valid Recall :  0.782608695652174\n",
            "\n",
            "Valid F1 score :  0.7809593622577542\n",
            "\n",
            "=============Epoch :  21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.09947434160858393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  1.957995593547821\n",
            "\n",
            " Valid Accuracy :  0.782608695652174\n",
            "\n",
            " Valid Precision :  0.7908710340775559\n",
            "\n",
            " Valid Recall :  0.782608695652174\n",
            "\n",
            "Valid F1 score :  0.7826086956521738\n",
            "\n",
            "=============Epoch :  22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.08892924804240465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of predictions :  69\n",
            "Length of gold :  69\n",
            "Valid loss :  2.0215758681297302\n",
            "\n",
            " Valid Accuracy :  0.782608695652174\n",
            "\n",
            " Valid Precision :  0.8068095743069231\n",
            "\n",
            " Valid Recall :  0.782608695652174\n",
            "\n",
            "Valid F1 score :  0.7809593622577542\n",
            "\n",
            "=============Epoch :  23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Iteration: 100%|██████████| 18/18 [00:10<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss :  0.08259772253222764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.47it/s]\n"
          ]
        }
      ],
      "source": [
        "# train_and_validation(model, train_loader, test_loader)\n",
        "train_and_validation(model, train_loader, valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "3o5p6PC96ZJQ"
      },
      "outputs": [],
      "source": [
        "# best_model_epoch_12_best_f1_72_foldNum_0.pt\n",
        "# best_model_epoch_9_best_f1_69_foldNum_1.pt\n",
        "# best_model_epoch_9_best_f1_79_foldNum_2.pt\n",
        "# best_model_epoch_9_best_f1_73_foldNum_3.pt\n",
        "# best_model_epoch_7_best_f1_75_foldNum_4.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "DO0Qsq9ufvhi"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/32/saved_model/bert_only/best_model_epoch_19_best_f1_81_foldNum_0.pt'\n",
        "# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "NAZqaFeUAiMW"
      },
      "outputs": [],
      "source": [
        "# PATH_0 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_12_f1_76.pt'\n",
        "# PATH_1 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_13_f1_78_foldNum_1.pt'\n",
        "# PATH_2 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_9_f1_64_foldNum_2.pt'\n",
        "# PATH_3 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_5_f1_73_foldNum_3.pt'\n",
        "# PATH_4 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_74_foldNum_4.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "NU9qavVgEwJ0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# PATH_0 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_6_f1_50_foldNum_0.pt'\n",
        "# PATH_1 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_70_foldNum_1.pt'\n",
        "# PATH_2 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_11_f1_63_foldNum_2.pt'\n",
        "# PATH_3 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_44_foldNum_3.pt'\n",
        "# PATH_4 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_62_foldNum_4.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "5PMNVLL3BM-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad56378-775f-4ad7-98f0-8b5f3224e725"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ],
      "source": [
        "foldNum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "_J_qDFj5_ie3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b525ddd-ed56-4398-ae36-d5914dc00ad2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "11LQROttIrMQ"
      },
      "outputs": [],
      "source": [
        "# 2*3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "UgEpzAt-_mSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2da8e18-8fc5-453d-b5c2-9c8e82ce3294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00,  4.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6231884057971014\n",
            "\n",
            "Accuracy :  0.6231884057971014\n",
            "\n",
            "Precision :  0.6364817591204397\n",
            "\n",
            "Recall :  0.6231884057971014\n",
            "\n",
            "F1 score :  0.6212853169374908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "print(acc)\n",
        "\n",
        "print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "print(\"\\nPrecision : \", precision_score(test_gold, test_pred, average = 'weighted'))\n",
        "print(\"\\nRecall : \", recall_score(test_gold, test_pred, average = 'weighted'))\n",
        "print(\"\\nF1 score : \", f1_score(test_gold, test_pred, average = 'weighted'))\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                                                 \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "50ctOavcBUOt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "LTd5vAY4HOp6"
      },
      "outputs": [],
      "source": [
        "# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred, average = 'weighted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "0eD8Mc-spvSz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "s3K5movSJPIf"
      },
      "outputs": [],
      "source": [
        "# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred))\n",
        "# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred))\n",
        "# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "wWNb08EkJPu0"
      },
      "outputs": [],
      "source": [
        "# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred, average = 'micro'))\n",
        "# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred, average = 'micro'))\n",
        "# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred, average = 'micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "Ixy_k2L9GgEV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "H3XQZOkxJ4eP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(test_gold, test_pred))\n",
        "# print(\"\\nRecall : \", recall_score(test_gold, test_pred))\n",
        "# print(\"\\nF1 score : \", f1_score(test_gold, test_pred))\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                                                 \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "jryD1kV-J5MG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "# # print(acc)\n",
        "\n",
        "# print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "# print(\"\\nPrecision : \", precision_score(test_gold, test_pred, average = 'micro'))\n",
        "# print(\"\\nRecall : \", recall_score(test_gold, test_pred, average = 'micro'))\n",
        "# print(\"\\nF1 score : \", f1_score(test_gold, test_pred, average = 'micro'))\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                                                 \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "993-qFHhZxPp"
      },
      "outputs": [],
      "source": [
        "# test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "ZPaJ3qk0isrD"
      },
      "outputs": [],
      "source": [
        "# test_gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "94i6uFJtivrG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "540707edcb21450fa4c581e7f4d2a54c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfd192ef777c44829b12b59eeb82024f",
              "IPY_MODEL_3a71db66c74f4daf8420e931e1abd8da",
              "IPY_MODEL_6e28c23c5f014dc3b011977218b4df0b"
            ],
            "layout": "IPY_MODEL_de54b013b09144558d69c76b93f4e7c5"
          }
        },
        "bfd192ef777c44829b12b59eeb82024f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5615c47b00043bfbb43af2447cbab67",
            "placeholder": "​",
            "style": "IPY_MODEL_d8c1ca111f974c1c9bad380515024fa2",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "3a71db66c74f4daf8420e931e1abd8da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ce39296ebb44d41be54e2f1aa2a0693",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0dbba3c22fae4f3e9f35fd57a822cc6e",
            "value": 570
          }
        },
        "6e28c23c5f014dc3b011977218b4df0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1916ffc68e854a50b4734dcf16bd0926",
            "placeholder": "​",
            "style": "IPY_MODEL_469ca1475f6047adbfadf77fc98d7c33",
            "value": " 570/570 [00:00&lt;00:00, 26.7kB/s]"
          }
        },
        "de54b013b09144558d69c76b93f4e7c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5615c47b00043bfbb43af2447cbab67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8c1ca111f974c1c9bad380515024fa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ce39296ebb44d41be54e2f1aa2a0693": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dbba3c22fae4f3e9f35fd57a822cc6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1916ffc68e854a50b4734dcf16bd0926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "469ca1475f6047adbfadf77fc98d7c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "018d4394ad4e47038571687c464a3374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fdf9fdf454e4d72805c5d5a8d601007",
              "IPY_MODEL_2b3e13d33c014f68ba1eefcfb6a8e0f8",
              "IPY_MODEL_951af7c7798c4690a35c39513813fc70"
            ],
            "layout": "IPY_MODEL_353e8db282454c91b5cd5a8cd4dfaa54"
          }
        },
        "8fdf9fdf454e4d72805c5d5a8d601007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45fa65065f11426aae169bb37cfe2af5",
            "placeholder": "​",
            "style": "IPY_MODEL_5d933e630c744bd197e7436b134bcece",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "2b3e13d33c014f68ba1eefcfb6a8e0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a74d0ab8132e47eda68cd3ca03ce43f8",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e3fc36ca92b4ca3bd221449d727152f",
            "value": 440473133
          }
        },
        "951af7c7798c4690a35c39513813fc70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cef6da4a8b3c4d268f4863f8d57ee75b",
            "placeholder": "​",
            "style": "IPY_MODEL_dde44b6dafb24afabd05ab1a92e147d3",
            "value": " 440M/440M [00:02&lt;00:00, 208MB/s]"
          }
        },
        "353e8db282454c91b5cd5a8cd4dfaa54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45fa65065f11426aae169bb37cfe2af5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d933e630c744bd197e7436b134bcece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a74d0ab8132e47eda68cd3ca03ce43f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e3fc36ca92b4ca3bd221449d727152f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cef6da4a8b3c4d268f4863f8d57ee75b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dde44b6dafb24afabd05ab1a92e147d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c882505f77be4351b7187f6aeac604ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_650c0c58cd2248bfb4b2585e8fce6f34",
              "IPY_MODEL_a6ca7642924a4bdb83319dbfe87ede90",
              "IPY_MODEL_e821d9ab810f4c24b72576709d21a50d"
            ],
            "layout": "IPY_MODEL_aa02cf1548cb4780ad7334b1a37e2b7a"
          }
        },
        "650c0c58cd2248bfb4b2585e8fce6f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce90b56e8af7448ca7102c60c018ac0c",
            "placeholder": "​",
            "style": "IPY_MODEL_b3c055d09d5647e8b025fbaead0d69ed",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "a6ca7642924a4bdb83319dbfe87ede90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6222788ac421407d88972e3bf2716ef4",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa19786307a6433ea3f78dd9b7f62af3",
            "value": 231508
          }
        },
        "e821d9ab810f4c24b72576709d21a50d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71ff6135352f4bf3a9bade07465ed35c",
            "placeholder": "​",
            "style": "IPY_MODEL_69e28c7607474d69aa0f620ace86f329",
            "value": " 232k/232k [00:00&lt;00:00, 1.93MB/s]"
          }
        },
        "aa02cf1548cb4780ad7334b1a37e2b7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce90b56e8af7448ca7102c60c018ac0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3c055d09d5647e8b025fbaead0d69ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6222788ac421407d88972e3bf2716ef4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa19786307a6433ea3f78dd9b7f62af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71ff6135352f4bf3a9bade07465ed35c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69e28c7607474d69aa0f620ace86f329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "869dfd3e853a4156bad48ea82efb79fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef340412d28847ea822676dc910dbb12",
              "IPY_MODEL_8954cb47c03e4c9a8267f048b1d5be97",
              "IPY_MODEL_bdb28610d5ae4af7bd626e57842c61bd"
            ],
            "layout": "IPY_MODEL_86b1b52682d84608bed5562a171c6370"
          }
        },
        "ef340412d28847ea822676dc910dbb12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cae7d0c441e446dfb485b8bbface2b07",
            "placeholder": "​",
            "style": "IPY_MODEL_71462387878b49cdb31c6a6726c47776",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "8954cb47c03e4c9a8267f048b1d5be97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4065b5541d64140b60e7cc331b7b038",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_270561b7fd124853ae570ae37e780fae",
            "value": 28
          }
        },
        "bdb28610d5ae4af7bd626e57842c61bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e72975bd48594f1293be3e4cd765bb69",
            "placeholder": "​",
            "style": "IPY_MODEL_483f58f12d074569a68ce5c825793ad0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.45kB/s]"
          }
        },
        "86b1b52682d84608bed5562a171c6370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cae7d0c441e446dfb485b8bbface2b07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71462387878b49cdb31c6a6726c47776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4065b5541d64140b60e7cc331b7b038": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "270561b7fd124853ae570ae37e780fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e72975bd48594f1293be3e4cd765bb69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "483f58f12d074569a68ce5c825793ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}