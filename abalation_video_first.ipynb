{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8261,"status":"ok","timestamp":1682417974668,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"WHwHkm4rGCLK","outputId":"6f90616f-7500-4883-e21c-e08e1cc83668"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1682417974669,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"VFg6mNX7Gdxg","outputId":"1daaf542-3f3e-4e8d-d231-8499bf8c1933"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Apr 25 10:19:33 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    48W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","source":["!pip install transformers==4.26.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3fAxtYIcEwy","executionInfo":{"status":"ok","timestamp":1682417977811,"user_tz":-330,"elapsed":3145,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}},"outputId":"aa45690f-a00f-4b17-f9ec-554c58a0a741"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers==4.26.1 in /usr/local/lib/python3.9/dist-packages (4.26.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (2.27.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (0.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (3.11.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (23.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (4.65.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (6.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1) (3.4)\n"]}]},{"cell_type":"code","source":["!pip uninstall torch -y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5eZP-MmncGe7","executionInfo":{"status":"ok","timestamp":1682417981651,"user_tz":-330,"elapsed":3849,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}},"outputId":"fd05b454-0984-4c6f-e9b4-0fb5941fe5d4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 1.13.1\n","Uninstalling torch-1.13.1:\n","  Successfully uninstalled torch-1.13.1\n"]}]},{"cell_type":"code","source":["!pip install torch==1.13.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v2zP-uJocH_B","executionInfo":{"status":"ok","timestamp":1682418009396,"user_tz":-330,"elapsed":27781,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}},"outputId":"dd0c6809-ba03-4631-8320-3f2040d5ff4d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.13.1\n","  Using cached torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (8.5.0.96)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (11.7.99)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (11.10.3.66)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (4.5.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (11.7.99)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.40.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.1)\n","Installing collected packages: torch\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n","torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n","torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n","torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.13.1\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2015,"status":"ok","timestamp":1682418011391,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"ZQU-d0knGyhl"},"outputs":[],"source":["import os \n","import numpy as np\n","import pandas as pd\n","import json\n","import warnings\n","import logging\n","import gc\n","import random\n","import math\n","import re\n","import ast\n","from tqdm import tqdm\n","from typing import Optional\n","from datetime import datetime\n","import pickle\n","\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import jaccard_score, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","import copy"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682418011393,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"suRa-hBHG_21"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"THe-J3aMatJU","executionInfo":{"status":"ok","timestamp":1682418011394,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qrnNZdPxXQ3F","executionInfo":{"status":"ok","timestamp":1682418011394,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2884,"status":"ok","timestamp":1682418014271,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"r_Ovlo42G4Kh","outputId":"a92847d1-a06a-459d-d70d-43f1e243b070"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU\n"]}],"source":["import nltk\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.meteor_score import meteor_score\n","# from rouge_score.rouge_scorer import RougeScorer\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","from transformers import (\n","    BartTokenizerFast,\n","    AdamW\n",")\n","\n","if torch.cuda.is_available():\n","    DEVICE = torch.device(\"cuda\")\n","    print(\"Using GPU\")\n","\n","else:\n","    DEVICE = torch.device(\"cpu\")\n","    print(\"Using CPU\")\n","\n","foldNum = 0\n","\n","from transformers import BertTokenizer, BertModel\n","\n","SOURCE_MAX_LEN = 500\n","# TARGET_MAX_LEN = 50\n","# MAX_UTTERANCES = 25\n","\n","ACOUSTIC_DIM = 768\n","ACOUSTIC_MAX_LEN = 1000\n","\n","\n","\n","\n","\n","VISUAL_DIM = 2048\n","VISUAL_MAX_LEN = 480\n","\n","\n","\n","import random\n","\n","\n","\n","\n","LEARNING_RATE = 1e-4\n","\n","\n","VALID_LEN = 69\n","\n","# BASE_LEARNING_RATE = 5e-6\n","# NEW_LEARNING_RATE = 5e-5\n","# WEIGHT_DECAY = 1e-4\n","\n","# NUM_BEAMS = 5\n","# EARLY_STOPPING = True\n","# NO_REPEAT_NGRAM_SIZE = 3\n","\n","# EARLY_STOPPING_THRESHOLD = 5"]},{"cell_type":"code","source":[],"metadata":{"id":"Fd4fgjzSaL2b","executionInfo":{"status":"ok","timestamp":1682418014271,"user_tz":-330,"elapsed":24,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1682418014272,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"cMdRaTfAHF7Q","outputId":"06e25b0a-a2dd-417e-e644-6a9945a03133"},"outputs":[{"output_type":"stream","name":"stdout","text":["Seed : 994\n"]}],"source":["def set_random_seed(seed: int):\n","    print(\"Seed : {}\".format(seed))\n","\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.enabled = False\n","    torch.backends.cudnn.deterministic = True\n","\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","# set_random_seed(42)\n","# a = np.random.randint(0, 1000)\n","\n","# set_random_seed(123)\n","set_random_seed(994)\n","# set_random_seed(12345)\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.checkpoint\n","from torch.nn import CrossEntropyLoss, MSELoss\n","\n","from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n","\n","from transformers.modeling_utils import PreTrainedModel, unwrap_model\n","\n","from transformers import (\n","    BartTokenizer,\n","    AdamW\n",")\n","\n","from transformers.models.bart.configuration_bart import BartConfig\n","\n","from transformers.models.bart.modeling_bart import (\n","    BartPretrainedModel,\n","    BartDecoder,\n","    BartModel,\n","    BartLearnedPositionalEmbedding,\n","    BartEncoderLayer,\n","    shift_tokens_right,\n","    _make_causal_mask,\n","    _expand_mask\n",")\n","\n","from transformers.modeling_outputs import (\n","    BaseModelOutput,\n","    Seq2SeqLMOutput,\n","    Seq2SeqModelOutput,\n","    Seq2SeqSequenceClassifierOutput\n",")"]},{"cell_type":"code","source":["import transformers"],"metadata":{"id":"c9haaIGeWspc","executionInfo":{"status":"ok","timestamp":1682418014272,"user_tz":-330,"elapsed":19,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["print(transformers.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LTJglj3WWuqY","executionInfo":{"status":"ok","timestamp":1682418014272,"user_tz":-330,"elapsed":19,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}},"outputId":"b1e889d5-563d-4b01-e61e-acb3097b6fc0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["4.26.1\n"]}]},{"cell_type":"code","source":["print(torch.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PUEQ-CMVWMll","executionInfo":{"status":"ok","timestamp":1682418014272,"user_tz":-330,"elapsed":17,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}},"outputId":"5ce7c1a3-d3e9-4770-da15-eaf0216e0363"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["1.13.1+cu117\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1682418014272,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"MuNh3gFEHQhY"},"outputs":[],"source":["# from transformer_encoder import TransformerEncoder"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1682418014273,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"-ML9b3x4axdj","outputId":"b5e6a383-6d5d-4796-d022-caca941a7f56"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":13}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418014273,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"OtwuoBrcwzUN"},"outputs":[],"source":["# bert_tokenizer  = BertTokenizer.from_pretrained(\"bert-base-cased\")\n","# bert_model = BertModel.from_pretrained(\"bert-base-cased\")\n","# # bert_model"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418014273,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"e1XR3BlgHOY8"},"outputs":[],"source":["# BART_INPUTS_DOCSTRING = r\"\"\"\n","#     Args:\n","#         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n","#             Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n","#             it.\n","#             Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n","#             [`PreTrainedTokenizer.__call__`] for details.\n","#             [What are input IDs?](../glossary#input-ids)\n","#         attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n","#             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n","#             - 1 for tokens that are **not masked**,\n","#             - 0 for tokens that are **masked**.\n","#             [What are attention masks?](../glossary#attention-mask)\n","#         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n","#             Indices of decoder input sequence tokens in the vocabulary.\n","#             Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n","#             [`PreTrainedTokenizer.__call__`] for details.\n","#             [What are decoder input IDs?](../glossary#decoder-input-ids)\n","#             Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n","#             is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n","#             For translation and summarization training, `decoder_input_ids` should be provided. If no\n","#             `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n","#             for denoising pre-training following the paper.\n","#         decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n","#             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n","#             be used by default.\n","#             If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n","#             and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n","#             information on the default strategy.\n","#         head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n","#             Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n","#             - 1 indicates the head is **not masked**,\n","#             - 0 indicates the head is **masked**.\n","#         decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n","#             Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n","#             - 1 indicates the head is **not masked**,\n","#             - 0 indicates the head is **masked**.\n","#         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n","#             Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n","#             1]`:\n","#             - 1 indicates the head is **not masked**,\n","#             - 0 indicates the head is **masked**.\n","#         encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n","#             Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n","#             `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n","#             hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n","#         past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n","#             Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n","#             `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n","#             `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n","#             Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n","#             blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n","#             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n","#             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n","#             `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of shape\n","#             `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing `input_ids` you\n","#             can choose to directly pass an embedded representation. This is useful if you want more control over how to\n","#             convert `input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n","#         decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n","#             Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n","#             representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n","#             input (see `past_key_values`). This is useful if you want more control over how to convert\n","#             `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n","#             If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n","#             of `inputs_embeds`.\n","#         use_cache (`bool`, *optional*):\n","#             If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n","#             `past_key_values`).\n","#         output_attentions (`bool`, *optional*):\n","#             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n","#             tensors for more detail.\n","#         output_hidden_states (`bool`, *optional*):\n","#             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n","#             more detail.\n","#         return_dict (`bool`, *optional*):\n","#             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","# \"\"\"\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418014273,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"aEDR_7N6HWsu"},"outputs":[],"source":["# class ContextAwareAttention(nn.Module):\n","\n","#     def __init__(self,\n","#                  dim_model : int,\n","#                  dim_context : int,\n","#                  dropout_rate : Optional[float] = 0.0 ):\n","\n","#         super(ContextAwareAttention, self).__init__()\n","\n","#         self.dim_model = dim_model\n","#         self.dim_context = dim_context\n","#         self.dropout_rate = dropout_rate\n","#         self.attention_layer = nn.MultiheadAttention(embed_dim=self.dim_model,\n","#                                                      num_heads = 1,\n","#                                                      dropout = self.dropout_rate,\n","#                                                      bias = True,\n","#                                                     add_zero_attn=False,\n","#                                                     batch_first=True,\n","#                                                     device=DEVICE   \n","#         )\n","\n","#         self.u_k = nn.Linear(self.dim_context, self.dim_model, bias = False)\n","#         self.w1_k = nn.Linear(self.dim_model, 1, bias=False)\n","#         self.w2_k = nn.Linear(self.dim_model, 1, bias=False)\n","\n","#         self.u_v = nn.Linear(self.dim_context, self.dim_model, bias=False)\n","#         self.w1_v = nn.Linear(self.dim_model, 1, bias = False)\n","#         self.w2_v = nn.Linear(self.dim_model, 1, bias = False)\n","\n","#     def forward(self, q, k, v, context):\n","\n","#         # print(\"Context shape : \", context.shape)\n","#         # print(\"Dim context : \", self.dim_context, \" : Dim model : \", self.dim_model)\n","#         key_context = self.u_k(context)\n","#         # print(\"Context shape below key context : \", key_context.shape)\n","#         value_context = self.u_v(context)\n","\n","#         lambda_k = F.sigmoid(self.w1_k(k) + self.w2_k(key_context))\n","#         lambda_v = F.sigmoid(self.w1_v(v) + self.w2_v(value_context))\n","\n","#         k_cap = (1-lambda_k) * k + (lambda_k) * key_context\n","#         v_cap = (1-lambda_v) * v + (lambda_v) * value_context\n","\n","#         attention_output, _ = self.attention_layer(query = q,\n","#                                                    key = k_cap,\n","#                                                    value = v_cap)\n","  \n","#         return attention_output                                     \n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1682418014274,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"2ZPn4U3Weq9I"},"outputs":[],"source":["# class MAF_acoustic(nn.Module):\n","#     def __init__(self, \n","#                 dim_model,\n","#                 dropout_rate):\n","#         super(MAF_acoustic, self).__init__()\n","#         self.dropout_rate = dropout_rate\n","\n","#         self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n","#         # self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n","\n","#         self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n","#                                                                 dim_context=ACOUSTIC_DIM,\n","#                                                                 dropout_rate=dropout_rate)\n","\n","#         # self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n","#         #                                                     dim_context=VISUAL_DIM,\n","#         #                                                     dropout_rate=dropout_rate)\n","\n","#         self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n","#         # self.visual_gate = nn.Linear(2*dim_model, dim_model)\n","#         self.dropout_layer = nn.Dropout(dropout_rate)\n","#         self.final_layer_norm = nn.LayerNorm(dim_model)\n","\n","#     def forward(self,\n","#                 text_input,\n","#                 acoustic_context):\n","\n","#         # print(\"Acoustic context shape (A) : \", acoustic_context.shape)        \n","\n","#         acoustic_context = acoustic_context.permute(0,2,1)\n","#         acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n","#         acoustic_context = acoustic_context.permute(0,2,1)\n","\n","#         audio_out = self.acoustic_context_attention(q=text_input,\n","#                                                     k=text_input,\n","#                                                     v=text_input,\n","#                                                     context=acoustic_context)\n","#         # print(\"Audio out (A) : \", audio_out.shape) \n","\n","#         # print(\"Visual context shape : \", visual_context.shape)\n","#         # visual_context = visual_context.permute(0,2,1)\n","#         # visual_context = self.visual_context_transform(visual_context.float())\n","#         # visual_context = visual_context.permute(0,2,1)\n","        \n","#         # video_out = self.visual_context_attention(q=text_input,\n","#         #                                             k=text_input,\n","#         #                                             v=text_input,\n","#         #                                             context=visual_context)\n","\n","#         # print(\"Video out shape : \", video_out.shape)\n","#         # print(\"Text input shape : \", text_input.shape)\n","#         weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n","#         # weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n","\n","#         # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n","\n","#         output = self.final_layer_norm(text_input + weight_a * audio_out)\n","\n","#         return output"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1682418014274,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"ZeZdIlcker1V"},"outputs":[],"source":["# class MAF_visual(nn.Module):\n","#     def __init__(self, \n","#                 dim_model,\n","#                 dropout_rate):\n","#         super(MAF_visual, self).__init__()\n","#         self.dropout_rate = dropout_rate\n","\n","#         # self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n","#         self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n","\n","#         # self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n","#         #                                                         dim_context=ACOUSTIC_DIM,\n","#         #                                                         dropout_rate=dropout_rate)\n","\n","#         self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n","#                                                             dim_context=VISUAL_DIM,\n","#                                                             dropout_rate=dropout_rate)\n","\n","#         # self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n","#         self.visual_gate = nn.Linear(2*dim_model, dim_model)\n","#         self.dropout_layer = nn.Dropout(dropout_rate)\n","#         self.final_layer_norm = nn.LayerNorm(dim_model)\n","\n","#     def forward(self,\n","#                 text_input,\n","#                 visual_context):\n","\n","#         # print(\"Acoustic context shape (A) : \", acoustic_context.shape)        \n","\n","#         # acoustic_context = acoustic_context.permute(0,2,1)\n","#         # acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n","#         # acoustic_context = acoustic_context.permute(0,2,1)\n","\n","#         # audio_out = self.acoustic_context_attention(q=text_input,\n","#         #                                             k=text_input,\n","#         #                                             v=text_input,\n","#         #                                             context=acoustic_context)\n","#         # print(\"Audio out (A) : \", audio_out.shape) \n","\n","#         # print(\"Visual context shape : \", visual_context.shape)\n","#         visual_context = visual_context.permute(0,2,1)\n","#         visual_context = self.visual_context_transform(visual_context.float())\n","#         visual_context = visual_context.permute(0,2,1)\n","        \n","#         video_out = self.visual_context_attention(q=text_input,\n","#                                                     k=text_input,\n","#                                                     v=text_input,\n","#                                                     context=visual_context)\n","\n","#         # print(\"Video out shape : \", video_out.shape)\n","#         # print(\"Text input shape : \", text_input.shape)\n","#         # weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n","#         weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n","\n","#         # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n","\n","#         output = self.final_layer_norm(text_input  + weight_v * video_out)\n","\n","#         return output"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418014274,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"IVX99ZEfRebx"},"outputs":[],"source":["# class CustomTransformerEncoder(torch.nn.Module):\n","#   def __init__(self, vocab_size, hidden_dim, expand_factor):\n","#     super(CustomTransformerEncoder, self).__init__()\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418014274,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"lmfR5TqF20m4"},"outputs":[],"source":["# class MultiHeadSelfAttention(torch.nn.Module):\n","#   def __init__(self, config):\n","#     super(MultiHeadSelfAttention, self).__init__()\n","\n","#     self.num_attention_heads = 12\n","#     self.attention_head_size = config.d_model//12\n","#     self.embed_dim = config.d_model\n","#     self.dropout = config.dropout\n","\n","#     self.query = nn.Linear(config.d_model, config.d_model)\n","#     self.key = nn.Linear(config.d_model, config.d_model)\n","#     self.value = nn.Linear(config.d_model, config.d_model)\n","#     self.out_proj = nn.Linear(config.d_model, config.d_model)\n","\n","#   def forward(self, hidden_states, attention_mask):\n","#     query_states = self.query(hidden_states)\n","#     key_states = self.key(hidden_states)\n","#     value_states = self.value(hidden_states)\n","\n","#     batch_size = hidden_states.shape[0]\n","#     seq_len = hidden_states.shape[1]\n","\n","#     query_states =  query_states.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1,2).contiguous()\n","#     key_states =  key_states.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1,2).contiguous()\n","#     value_states = value_states.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1,2).contiguous()\n","\n","#     query_states = query_states.view(batch_size*self.num_attention_heads, -1, self.attention_head_size)\n","#     key_states  = key_states.view(batch_size*self.num_attention_heads, -1, self.attention_head_size)\n","#     value_states = value_states.view(batch_size*self.num_attention_heads, -1, self.attention_head_size)\n","\n","#     attn_weights = torch.bmm(query_states, key_states.transpose(1,2))\n","\n","#     attn_weights = attn_weights.view(batch_size, self.num_attention_heads, attn_weights.shape[1], attn_weights.shape[2])\n","#     attn_weights = attn_weights/torch.sqrt(torch.tensor(self.attention_head_size))\n","\n","#     attn_weights_masked = attn_weights.masked_fill(\n","#         attention_mask[:, None, None, :] == False, -1e16\n","#     )\n","\n","#     attn_weights = torch.nn.functional.softmax(attn_weights_masked, dim = -1)\n","\n","#     attn_weights = torch.nn.functional.dropout(attn_weights, p = self.dropout, training = self.training)\n","\n","#     attn_weights = attn_weights.view(batch_size*self.num_attention_heads, seq_len, seq_len)\n","\n","\n","    \n","#     attn_output = torch.bmm(attn_weights, value_states)\n","\n","#     attn_output = attn_output.view(batch_size, self.num_attention_heads, seq_len, self.attention_head_size)\n","#     attn_output = attn_output.transpose(1,2)\n","\n","#     attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)\n","\n","#     attn_out = self.out_proj(attn_output)\n","\n","#     return attn_out"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418014274,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"y0mlADX3NWj-"},"outputs":[],"source":["# def gelu(input):\n","#   input = torch.tensor(input)\n","#   return input * 0.5 * (1.0 + torch.erf(input/torch.sqrt(torch.tensor(2.0))))"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418014274,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"DCJvPt5hFS3X"},"outputs":[],"source":["# class CustomEncoder(torch.nn.Module):\n","#   def __init__(self, config):\n","#     super(CustomEncoder, self).__init__()\n","#     self.embed_dim = config.d_model\n","#     self.self_attn = MultiHeadSelfAttention(config)\n","#     self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n","#     self.dropout = config.dropout\n","#     # self.activation_fn = torch.nn.ReLU()\n","#     self.activation_dropout = config.activation_dropout\n","#     self.fc1 = nn.Linear(self.embed_dim, 2*self.embed_dim)\n","#     self.fc2 = nn.Linear(2*self.embed_dim, self.embed_dim)\n","#     self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n","\n","#   def forward(self, hidden_states, attention_mask):\n","#    residual = hidden_states\n","#    hidden_states =  self.self_attn(hidden_states = hidden_states, attention_mask = attention_mask)\n","#    hidden_states = nn.functional.dropout(hidden_states, p = self.dropout, training = self.training)\n","#    hidden_states = residual + hidden_states\n","#    hidden_states = self.self_attn_layer_norm(hidden_states)\n","\n","#    residual = hidden_states\n","#    hidden_states = gelu(self.fc1(hidden_states))\n","#    hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training = self.training)\n","#    hidden_states = self.fc2(hidden_states)\n","#    hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training = self.training)\n","#    hidden_states = residual + hidden_states\n","#    hidden_states = self.final_layer_norm(hidden_states)\n","\n","#    if hidden_states.dtype == torch.float16 and (\n","#        torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n","#    ):\n","#       clamp_value = torch.finfo(hidden_states.dtype).max = -1000\n","#       hidden_states = torch.clamp(hidden_states, min = -clamp_value, max = clamp_value)\n","\n","#    return hidden_states   "]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1682418014275,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"kEWqLUTHbj0t"},"outputs":[],"source":["# class ContextEncoder(nn.Module):\n","#   def __init__(self, config, num_layers):\n","#     super(ContextEncoder, self).__init__()\n","\n","#     self.word_embeddings = nn.Embedding(config.vocab_size, config.d_model, padding_idx = config.pad_token_id)\n","#     self.position_embedding = nn.Embedding(500, config.d_model)\n","#     self.layer_norm = nn.LayerNorm(config.d_model)\n","#     self.context_encoder = torch.nn.ModuleList([CustomEncoder(config) for _ in range(num_layers)])\n","#     self.context_gate = nn.Linear(2*config.d_model, config.d_model)\n","    \n","\n","#   def forward(self, hidden_states, context_input_ids, context_attention_mask) :\n","    \n","#     seq_len = context_input_ids.shape[1]\n","#     pos = torch.arange(seq_len, dtype = torch.long)\n","#     pos = pos.unsqueeze(0).expand_as(context_input_ids)\n","#     pos = pos.to(device)\n","#     context_state = self.word_embeddings(context_input_ids) + self.position_embedding(pos)\n","    \n","#     for layer in self.context_encoder:\n","#             context_state = layer(hidden_states = context_state, attention_mask  = context_attention_mask)\n","    \n","\n","#     weight_c = F.sigmoid(self.context_gate(torch.cat([hidden_states, context_state], dim=-1)))\n","#     output = self.layer_norm(hidden_states + weight_c * context_state)\n","#     # print('transformer context shape :', output.shape)\n","#     return output\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1682418014275,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"7Oa75YDExmAz"},"outputs":[],"source":["# class ContextEncoder(nn.Module):\n","#   def __init__(self, config):\n","#     super(ContextEncoder, self).__init__()\n","\n","#     self.bert = bert_model\n","#     self.layer_norm = nn.LayerNorm(config.d_model)\n","#     self.context_gate = nn.Linear(2*config.d_model, config.d_model)\n","    \n","\n","#   def forward(self, hidden_states, context_input_ids, context_attention_mask) :\n","    \n","#     output_encoding = self.bert(context_input_ids, context_attention_mask)\n","\n","#     context_state = output_encoding['last_hidden_state']\n","    \n","\n","#     weight_c = F.sigmoid(self.context_gate(torch.cat([hidden_states, context_state], dim=-1)))\n","#     output = self.layer_norm(hidden_states + weight_c * context_state)\n","#     # print('transformer context shape :', output.shape)\n","#     return output\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418014275,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"1oESmDliml6-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418014275,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"s88m19VoHbj5"},"outputs":[],"source":["# class MultiModalBartEncoder(BartPretrainedModel):\n","\n","#     def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n","#         super().__init__(config)\n","\n","#         self.dropout = config.dropout\n","#         self.layerdrop = config.encoder_layerdrop\n","\n","#         embed_dim = config.d_model\n","#         self.padding_idx = config.pad_token_id\n","#         self.max_source_position = config.max_position_embeddings\n","#         self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n","\n","#         if embed_tokens is not None:\n","#             self.embed_tokens = embed_tokens\n","#         else:\n","#             self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n","\n","#         self.embed_positions = BartLearnedPositionalEmbedding(\n","#             config.max_position_embeddings,\n","#             embed_dim\n","#         )        \n","\n","        \n","        \n","\n","#         self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n","\n","#         self.layernorm_embedding = nn.LayerNorm(embed_dim)\n","\n","#         self.init_weights()\n","#         self.gradient_checkpointing = False\n","\n","#         # self.fusion_at_layer = [4]\n","#         # self.fusion_at_layer = [3, 4]\n","#         # self.fusion_at_layer3 = [3]\n","#         self.fusion_at_layer4 = [4]\n","#         self.fusion_at_layer5 = [5]\n","\n","#         # self.fusion_of_context = [3]\n","#         # self.visual_transformer = TransformerEncoder(d_model = VISUAL_DIM,\n","#         #                                              n_layers = 4,\n","#         #                                              n_heads=8,\n","#         #                                              d_ff=VISUAL_DIM\n","#         #                                              )\n","#         # self.acoustic_transformer = TransformerEncoder(d_model = ACOUSTIC_DIM,\n","#         #                                                n_layers=4,\n","#         #                                                n_heads=2,\n","#         #                                                d_ff=ACOUSTIC_DIM)\n","\n","#         # self.MAF_layer3 = MAF(dim_model=embed_dim,\n","#         #                      dropout_rate=0.2)\n","        \n","#         self.MAF_layer4 = MAF_acoustic(dim_model=embed_dim,\n","#                              dropout_rate=0.2)\n","        \n","#         self.MAF_layer5 = MAF_visual(dim_model=embed_dim,\n","#                              dropout_rate=0.2)\n","        \n","#         # self.context_encoder = ContextEncoder(config)\n","\n","#         # self.classification = nn.Linear(embed_dim, 2)\n","\n","\n","\n","\n","#     def forward(self,\n","#             input_ids = None,\n","#             attention_mask = None,\n","#             # context_input_ids = None,\n","#             # context_attention_mask = None,\n","#             acoustic_input = None,\n","#             visual_input = None,\n","#             head_mask = None,\n","#             inputs_embeds = None,\n","#             output_attentions = None,\n","#             output_hidden_states  = None,\n","#             return_dict = None):\n","\n","#             # print(\"Input ids shape : \", input_ids.shape)\n","#             output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","\n","#             output_hidden_states = (\n","#                 output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","#             )\n","\n","#             return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","#             if input_ids is not None and inputs_embeds is not None:\n","#                 raise ValueError(\"You can't specify both input_ids and inputs_embeds at the same time\")\n","#             elif input_ids is not None:\n","#                 input_shape = input_ids.size()\n","#                 input_ids = input_ids.view(-1, input_shape[-1])\n","\n","#             elif inputs_embeds is not None:\n","#                 input_shape = inputs_embeds.size()[:-1]\n","#             else:\n","#                 raise ValueError(\"You have to specify either input_ids or input_embeds\")\n","\n","\n","#             if inputs_embeds is None:\n","#                 # print(\"Input ids shape : \", input_ids.shape)\n","#                 inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n","\n","#             # print(\"Input shape type : \", type(input_shape))\n","#             # print(\"Input shape : \", input_shape)\n","#             input_shape = torch.tensor(input_shape)\n","#             # print(\"Input shape type : \", type(input_shape))\n","#             # print(\"Input shape : \", input_shape)\n","#             # print(\"Input shape : \", input_shape.shape)\n","#             embed_pos = self.embed_positions(input_ids)\n","#             # embed_pos = self.embed_positions(input_shape)\n","\n","\n","#             hidden_states = inputs_embeds + embed_pos   \n","#             hidden_states = self.layernorm_embedding(hidden_states)\n","#             hidden_states = F.dropout(hidden_states, p = self.dropout, training=self.training)\n","\n","#             # print(\"attention mask shape 3 : \", attention_mask.shape)\n","#             if attention_mask is not None:\n","#                 attention_mask =  _expand_mask(attention_mask, inputs_embeds.dtype)\n","\n","#             # print(\"attention mask shape 4 : \", attention_mask.shape)\n","#             encoder_states = () if output_hidden_states else None\n","#             all_attentions = () if output_attentions else None\n","\n","#             if head_mask is not None:\n","#                 assert head_mask.size()[0] == (\n","#                     len(self.layers)\n","#                 ), f\"The head mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n","\n","#             for idx, encoder_layer in enumerate(self.layers):\n","#                 # print(\"============Idx : \", idx)\n","\n","#                 # if idx in self.fusion_at_layer3:\n","#                 #     # print(\"Acoustic input shape (B) : \", acoustic_input)\n","#                 #     # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]   \n","#                 #     # print(\"Acoustic input shape (C) : \", acoustic_input)\n","\n","#                 #     # visual_input = self.visual_transformer(visual_input)[-1]\n","#                 #     # print(\"====Idx inside fusion at layer :\", idx)\n","#                 #     hidden_states = self.MAF_layer3(text_input = hidden_states,\n","#                 #                                    acoustic_context = acoustic_input,\n","#                 #                                    visual_context = visual_input) \n","\n","#                 # if idx in self.fusion_of_context:\n","                  \n","#                 #   hidden_states = self.context_encoder(hidden_states = hidden_states, context_input_ids = context_input_ids, context_attention_mask = context_attention_mask)\n","                  \n","\n","#                 if idx in self.fusion_at_layer4:\n","#                     # print(\"Acoustic input shape (B) : \", acoustic_input)\n","#                     # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]   \n","#                     # print(\"Acoustic input shape (C) : \", acoustic_input)\n","\n","#                     # visual_input = self.visual_transformer(visual_input)[-1]\n","#                     # print(\"====Idx inside fusion at layer :\", idx)\n","                    \n","                    \n","\n","#                     hidden_states = self.MAF_layer4(text_input = hidden_states,\n","#                                                    acoustic_context = acoustic_input\n","#                                                    )\n","#                 if idx in self.fusion_at_layer5:\n","#                     # print(\"Acoustic input shape (B) : \", acoustic_input)\n","#                     # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]   \n","#                     # print(\"Acoustic input shape (C) : \", acoustic_input)\n","\n","#                     # visual_input = self.visual_transformer(visual_input)[-1]\n","#                     # print(\"====Idx inside fusion at layer :\", idx)\n","\n","                    \n","\n","#                     hidden_states = self.MAF_layer5(text_input = hidden_states,\n","#                                                    visual_context = visual_input)     \n","\n","#                 if output_hidden_states:\n","#                     encoder_states = encoder_states + (hidden_states,)\n","\n","#                 dropout_probability = random.uniform(0,1)   \n","\n","#                 if self.training and (dropout_probability < self.layerdrop):\n","#                     layer_outputs = (None, None)\n","\n","#                 else:\n","#                     if self.gradient_checkpointing and self.training:\n","\n","#                         def create_custom_forward(module):\n","#                             def custom_forward(*inputs):\n","#                                 return module(*inputs, output_attentions)\n","\n","#                             return custom_forward\n","\n","#                         layer_outputs = torch.utils.checkpoint.checkpoint(\n","#                             create_custom_forward(encoder_layer),\n","#                             hidden_states,\n","#                             attention_mask,\n","#                             (head_mask[idx] if head_mask is not None else None),\n","#                         )        \n","\n","#                     else:\n","#                         # print(\"Checking Attention mask shape : \", attention_mask.shape)\n","#                         layer_outputs = encoder_layer(\n","#                             hidden_states,\n","#                             attention_mask,\n","#                             layer_head_mask = (head_mask[idx] if head_mask is not None else None),\n","#                             output_attentions = output_attentions\n","#                         )    \n","\n","#                     hidden_states = layer_outputs[0]\n","\n","#                 if output_attentions:\n","#                     all_attentions  = all_attentions + (layer_outputs[1],)\n","\n","#             if output_hidden_states:\n","#                 encoder_states = encoder_states + (hidden_states,)\n","\n","#             if not return_dict:\n","#                 return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n","\n","#             return BaseModelOutput(\n","#                 last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n","#             )    \n","\n","#             # print(\"Hidden states shape : \", hidden_states)\n","\n","#             # cls = hidden_states.permute(1,0,2)"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418014275,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"6F_nfly0HgKR"},"outputs":[],"source":["# class MultimodalBartModel(BartPretrainedModel):\n","#     def __init__(self, config: BartConfig):\n","#         super().__init__(config)\n","\n","#         padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n","#         self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n","\n","#         self.encoder = MultiModalBartEncoder(config, self.shared)\n","#         self.decoder = BartDecoder(config, self.shared)\n","\n","#         self.init_weights()\n","\n","#     def get_input_embeddings(self):\n","#         return self.shared\n","\n","#     def set_input_embeddings(self, value):\n","#         self.shared = value\n","#         self.encoder.embed_tokens = self.shared\n","#         self.decoder.embed_tokens = self.shared\n","\n","#     def get_encoder(self):\n","#         return self.encoder\n","\n","\n","#     def get_decoder(self):\n","#         return self.decoder\n","\n","#     def forward(\n","#         self, \n","#         input_ids = None,\n","#         attention_mask = None,\n","#         # context_input_ids = None,\n","#         # context_attention_mask = None,\n","#         acoustic_input = None,\n","#         visual_input = None,\n","#         decoder_input_ids = None,\n","#         decoder_attention_mask = None,\n","#         head_mask = None,\n","#         decoder_head_mask = None,\n","#         cross_attn_head_mask = None,\n","#         encoder_outputs = None,\n","#         past_key_values = None,\n","#         inputs_embeds = None,\n","#         decoder_inputs_embeds = None,\n","#         use_cache = None,\n","#         output_attentions = None,\n","#         output_hidden_states = None,\n","#         return_dict = None\n","#     ):\n","\n","#         if decoder_input_ids is None and decoder_inputs_embeds is None:\n","#             decoder_input_ids = shift_tokens_right(\n","#                 input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n","\n","#             )                                \n","\n","#         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","#         output_hidden_states = (\n","#             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","#         )    \n","#         use_cache = use_cache if use_cache is not None else self.config.use_cache\n","#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","#         # print(\"attention mask shape 2 : \", attention_mask.shape)\n","\n","#         if encoder_outputs is None:\n","#             encoder_outputs = self.encoder(\n","#                 input_ids = input_ids,\n","#                 attention_mask = attention_mask,\n","#                 # context_input_ids = context_input_ids,\n","#                 # context_attention_mask = context_attention_mask,\n","#                 acoustic_input = acoustic_input,\n","#                 visual_input = visual_input,\n","#                 head_mask = head_mask,\n","#                 inputs_embeds = inputs_embeds,\n","#                 output_attentions = output_attentions,\n","#                 output_hidden_states = output_hidden_states,\n","#                 return_dict = return_dict\n","#             )\n","\n","#         elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n","#             encoder_outputs = BaseModelOutput(\n","#                 last_hidden_state=encoder_outputs[0],\n","#                 hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n","#                 attentions = encoder_outputs[2] if len(encoder_outputs) > 2 else None\n","#             )    \n","\n","#         decoder_outputs = self.decoder(\n","#             input_ids = decoder_input_ids,\n","#             attention_mask = decoder_attention_mask,\n","#             encoder_hidden_states = encoder_outputs[0],\n","#             encoder_attention_mask = attention_mask,\n","#             head_mask = decoder_head_mask,\n","#             cross_attn_head_mask = cross_attn_head_mask,\n","#             past_key_values = past_key_values,\n","#             inputs_embeds = decoder_inputs_embeds,\n","#             use_cache = use_cache,\n","#             output_attentions = output_attentions,\n","#             output_hidden_states = output_hidden_states,\n","#             return_dict = return_dict\n","#         )    \n","\n","#         if not return_dict:\n","#             return decoder_outputs + encoder_outputs\n","\n","#         return Seq2SeqModelOutput(\n","#             last_hidden_state=decoder_outputs.last_hidden_state,\n","#             past_key_values=decoder_outputs.past_key_values,\n","#             decoder_hidden_states=decoder_outputs.hidden_states,\n","#             decoder_attentions=decoder_outputs.attentions,\n","#             cross_attentions=decoder_outputs.cross_attentions,\n","#             encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n","#             encoder_hidden_states=encoder_outputs.hidden_states,\n","#             encoder_attentions=encoder_outputs.attentions\n","\n","#         )"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682418014275,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"VO1KdKseHlkn"},"outputs":[],"source":["# class MultimodalBartClassification(nn.Module):\n","#     def __init__(\n","#         self, \n","#         input_dim: int,\n","#         inned_dim: int,\n","#         num_classes: int,\n","#         pooler_dropout: float\n","#     ):\n","#         super().__init__()\n","#         self.dense = nn.Linear(input_dim, inned_dim)\n","#         self.dropout = nn.Dropout(p = pooler_dropout)\n","#         self.out_proj = nn.Linear(inned_dim, num_classes)\n","\n","#     def forward(self, hidden_states):\n","#         hidden_states = self.dropout(hidden_states)\n","#         hidden_states = self.dense(hidden_states)\n","#         hidden_states = torch.tanh(hidden_states)\n","#         hidden_states = self.dropout(hidden_states)\n","#         hidden_states = self.out_proj(hidden_states)\n","#         return hidden_states    \n","\n","\n","\n","\n","# class MultimodalBartForSequenceClassification(BartPretrainedModel):\n","#     _keys_to_ignore_on_load_missing = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n","\n","#     def __init__(self, config: BartConfig, **kwargs):\n","#         super().__init__(config, **kwargs)\n","#         self.model = MultimodalBartModel(config)\n","#         self.classification_head = MultimodalBartClassification(\n","#             config.d_model,\n","#             config.d_model,\n","#             2,\n","#             config.classifier_dropout\n","#         )\n","#         self.model._init_weights(self.classification_head.dense)\n","#         self.model._init_weights(self.classification_head.out_proj)\n","\n","#     def forward(\n","#         self,\n","#         input_ids: torch.LongTensor = None,\n","#         attention_mask: Optional[torch.tensor] = None,\n","#         # context_input_ids : torch.LongTensor = None,\n","#         # context_attention_mask : Optional[torch.tensor] = None,\n","#         acoustic_input = None,\n","#         visual_input = None,\n","#         decoder_input_ids: Optional[torch.LongTensor] = None,\n","#         decoder_attention_mask: Optional[torch.LongTensor] = None,\n","#         head_mask: Optional[torch.Tensor] = None,\n","#         decoder_head_mask: Optional[torch.Tensor] = None,\n","#         cross_attn_head_mask: Optional[torch.Tensor] = None,\n","#         encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n","#         inputs_embeds: Optional[torch.FloatTensor] = None,\n","#         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n","#         labels: Optional[torch.LongTensor] = None,\n","#         use_cache: Optional[bool] = None,\n","#         output_attentions: Optional[bool] = None,\n","#         output_hidden_states: Optional[bool] = None,\n","#         return_dict: Optional[bool] = None\n","#     ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n","#         r\"\"\"\n","#         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","#             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n","#             config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","#         \"\"\"\n","\n","#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","#         if labels is not None:\n","#             use_cache = False\n","\n","#         if input_ids is None and inputs_embeds is not None:\n","#             raise NotImplementedError(\n","#                 f\"Passing input embeddings is currently not supported for {self.__class__.__name__}\"\n","#             )\n","\n","#         # print(\"attention mask shape 1 : \", attention_mask.shape )\n","#         outputs = self.model(\n","#             input_ids,\n","#             attention_mask = attention_mask,\n","#             # context_input_ids = context_input_ids,\n","#             # context_attention_mask = context_attention_mask,\n","#             acoustic_input = acoustic_input,\n","#             visual_input = visual_input,\n","#             decoder_input_ids = decoder_input_ids,\n","#             decoder_attention_mask = decoder_attention_mask,\n","#             head_mask = head_mask,\n","#             decoder_head_mask = decoder_head_mask,\n","#             cross_attn_head_mask = cross_attn_head_mask,\n","#             encoder_outputs = encoder_outputs,\n","#             inputs_embeds = inputs_embeds,\n","#             decoder_inputs_embeds = decoder_inputs_embeds,\n","#             use_cache = use_cache,\n","#             output_attentions = output_attentions,\n","#             output_hidden_states = output_hidden_states,\n","#             return_dict = return_dict\n","\n","#         )    \n","\n","#         hidden_states = outputs[0]\n","\n","#         eos_mask = input_ids.eq(self.config.eos_token_id).to(hidden_states.device)\n","\n","#         if len(torch.unique_consecutive(eos_mask.sum(1))) > 1:\n","#             raise ValueError(\"All examples must have same number of <eos> tokens\")\n","\n","#         sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[\n","#             :, -1, :\n","#         ]    \n","\n","#         logits = self.classification_head(sentence_representation)\n","\n","#         loss = None\n","\n","#         loss_fct = CrossEntropyLoss()\n","#         # print(\"logits shape : \", logits.shape)\n","#         loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n","#         # print(\"Loss : \", loss)\n","#         if not return_dict:\n","#             output = (logits,) + outputs[1:]\n","#             return ((loss,) + output) if loss is not None else output\n","\n","\n","#         return Seq2SeqSequenceClassifierOutput(\n","#             loss=loss,\n","#             logits=logits,\n","#             past_key_values=outputs.past_key_values,\n","#             decoder_hidden_states=outputs.decoder_hidden_states,\n","#             decoder_attentions = outputs.decoder_attentions,\n","#             cross_attentions=outputs.cross_attentions,\n","#             encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n","#             encoder_hidden_states=outputs.encoder_hidden_states,\n","#             encoder_attentions=outputs.encoder_attentions\n","#         ) "]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682418014275,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"m4lcNczUHpOH"},"outputs":[],"source":["# def audio_video_broadcast(x):\n","# #     z = torch.empty()\n","#     temp_all = torch.Tensor()\n","#     for j in range(x.shape[0]):\n","#         print(\"j : \", j)\n","#         temp_x = x[j,:]\n","# #         print(\"Temp x shape : \", temp_x.shape)\n","#         temp_x = torch.tensor(temp_x, dtype=torch.float)\n","#         temp_x = torch.broadcast_to(temp_x, (SOURCE_MAX_LEN, temp_x.shape[0]))\n","# #         print(\"Temp x shape : \", temp_x.shape)\n","#         temp_x = temp_x.unsqueeze(0)\n","# #         print(\"Temp x shape : \", temp_x.shape)\n","        \n","#         if(j==0):\n","#             temp_all = temp_x\n","#         else:\n","#             temp_all = torch.cat([temp_all, temp_x], dim = 0)\n","        \n","        \n","#     return temp_all "]},{"cell_type":"code","source":["tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n","bart_model = BartModel.from_pretrained('facebook/bart-base')\n","\n","p = {\n","        'additional_special_tokens' : ['[CONTEXT]', '[UTTERANCE]']\n","    }\n","\n","tokenizer.add_special_tokens(p)\n","\n","bart_model.resize_token_embeddings(len(tokenizer))\n","\n","# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n","# outputs = model(**inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pshLRQxd2_Pc","executionInfo":{"status":"ok","timestamp":1682418025964,"user_tz":-330,"elapsed":11697,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}},"outputId":"8cc1de27-413a-41d4-fc88-008c3ec46e77"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(50267, 768)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["# inputs.keys()"],"metadata":{"id":"OvT88kvIF24N","executionInfo":{"status":"ok","timestamp":1682418025964,"user_tz":-330,"elapsed":26,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# outputs.keys()"],"metadata":{"id":"DSkqbNaGFxAH","executionInfo":{"status":"ok","timestamp":1682418025964,"user_tz":-330,"elapsed":25,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","execution_count":31,"metadata":{"id":"Q7Z30bGs2_V6","executionInfo":{"status":"ok","timestamp":1682418025964,"user_tz":-330,"elapsed":24,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":32,"metadata":{"id":"oVQvabBN2_V6","executionInfo":{"status":"ok","timestamp":1682418025965,"user_tz":-330,"elapsed":25,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":["# class MAF_one(nn.Module):\n","#     def __init__(self, \n","#                 dim_model,\n","#                 dropout_rate):\n","#         super(MAF_one, self).__init__()\n","#         self.dropout_rate = dropout_rate\n","\n","        \n","#         # self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n","\n","#         self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n","#                                                                 # dim_context=ACOUSTIC_DIM,\n","#                                                                 dim_context = dim_model,\n","#                                                                 dropout_rate=dropout_rate)\n","\n","#         # self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n","#         #                                                     dim_context=VISUAL_DIM,\n","#         #                                                     dropout_rate=dropout_rate)\n","\n","#         self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n","#         # self.visual_gate = nn.Linear(2*dim_model, dim_model)\n","#         self.dropout_layer = nn.Dropout(dropout_rate)\n","#         self.final_layer_norm = nn.LayerNorm(dim_model)\n","\n","#     def forward(self,\n","#                 text_input,\n","#                 # acoustic_context\n","#                 acoustic_input):\n","\n","#         # print(\"Acoustic context shape (A) : \", acoustic_context.shape)        \n","\n","\n","\n","\n","\n","#         # audio_out = self.acoustic_context_attention(q=text_input,\n","#         #                                             k=text_input,\n","#         #                                             v=text_input,\n","#         #                                             context=acoustic_context)\n","        \n","#         text_out = self.acoustic_context_attention(q=acoustic_input,\n","#                                                     k=acoustic_input,\n","#                                                     v=acoustic_input,\n","#                                                     context=text_input)\n","#         # print(\"Audio out (A) : \", audio_out.shape) \n","\n","#         # print(\"Visual context shape : \", visual_context.shape)\n","#         # visual_context = visual_context.permute(0,2,1)\n","#         # visual_context = self.visual_context_transform(visual_context.float())\n","#         # visual_context = visual_context.permute(0,2,1)\n","        \n","#         # video_out = self.visual_context_attention(q=text_input,\n","#         #                                             k=text_input,\n","#         #                                             v=text_input,\n","#         #                                             context=visual_context)\n","\n","#         # print(\"Video out shape : \", video_out.shape)\n","#         # print(\"Text input shape : \", text_input.shape)\n","#         # weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n","#         weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_out, acoustic_input], dim=-1)))\n","#         # weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n","\n","#         # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n","\n","#         # output = self.final_layer_norm(text_input + weight_a * audio_out)\n","#         output = self.final_layer_norm(weight_a * text_out +  acoustic_input)\n","\n","#         return output"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"FdSjcJSD2_V7","executionInfo":{"status":"ok","timestamp":1682418025965,"user_tz":-330,"elapsed":25,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":["# class MAF_two(nn.Module):\n","#     def __init__(self, \n","#                 dim_model,\n","#                 dropout_rate):\n","#         super(MAF_two, self).__init__()\n","#         self.dropout_rate = dropout_rate\n","\n","#         # self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n","        \n","#         # self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n","#         #                                                         dim_context=ACOUSTIC_DIM,\n","#         #                                                         dropout_rate=dropout_rate)\n","\n","#         # self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n","#         #                                                     dim_context=VISUAL_DIM,\n","#         #                                                     dropout_rate=dropout_rate)\n","#         self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n","#                                                             dim_context=dim_model,\n","#                                                             dropout_rate=dropout_rate)\n","\n","#         # self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n","#         self.visual_gate = nn.Linear(2*dim_model, dim_model)\n","#         self.dropout_layer = nn.Dropout(dropout_rate)\n","#         self.final_layer_norm = nn.LayerNorm(dim_model)\n","\n","#     def forward(self,\n","#                 text_input,\n","#                 # visual_context\n","#                 visual_input):\n","\n","#         # print(\"Acoustic context shape (A) : \", acoustic_context.shape)        \n","\n","#         # acoustic_context = acoustic_context.permute(0,2,1)\n","#         # acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n","#         # acoustic_context = acoustic_context.permute(0,2,1)\n","\n","#         # audio_out = self.acoustic_context_attention(q=text_input,\n","#         #                                             k=text_input,\n","#         #                                             v=text_input,\n","#         #                                             context=acoustic_context)\n","#         # print(\"Audio out (A) : \", audio_out.shape) \n","\n","#         # print(\"Visual context shape : \", visual_context.shape)\n","\n","        \n","#         text_out = self.visual_context_attention(q=visual_input,\n","#                                                     k=visual_input,\n","#                                                     v=visual_input,\n","#                                                     context=text_input)\n","\n","#         # print(\"Video out shape : \", video_out.shape)\n","#         # print(\"Text input shape : \", text_input.shape)\n","#         # weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n","#         # weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n","#         weight_v = F.sigmoid(self.visual_gate(torch.cat([text_out, visual_input], dim=-1)))\n","\n","#         # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n","\n","#         output = self.final_layer_norm(weight_v * text_out  + visual_input)\n","\n","#         return output"]},{"cell_type":"code","source":["class ContextAwareAttention(nn.Module):\n","\n","    def __init__(self,\n","                 dim_model : int,\n","                 dim_context : int,\n","                 dropout_rate : Optional[float] = 0.0 ):\n","\n","        super(ContextAwareAttention, self).__init__()\n","\n","        self.dim_model = dim_model\n","        self.dim_context = dim_context\n","        self.dropout_rate = dropout_rate\n","        self.attention_layer = nn.MultiheadAttention(embed_dim=self.dim_model,\n","                                                     num_heads = 1,\n","                                                     dropout = self.dropout_rate,\n","                                                     bias = True,\n","                                                    add_zero_attn=False,\n","                                                    batch_first=True,\n","                                                    device=DEVICE   \n","        )\n","\n","        self.u_k = nn.Linear(self.dim_context, self.dim_model, bias = False)\n","        self.w1_k = nn.Linear(self.dim_model, 1, bias=False)\n","        self.w2_k = nn.Linear(self.dim_model, 1, bias=False)\n","\n","        self.u_v = nn.Linear(self.dim_context, self.dim_model, bias=False)\n","        self.w1_v = nn.Linear(self.dim_model, 1, bias = False)\n","        self.w2_v = nn.Linear(self.dim_model, 1, bias = False)\n","\n","    def forward(self, q, k, v, context):\n","\n","        # print(\"Context shape : \", context.shape)\n","        # print(\"Dim context : \", self.dim_context, \" : Dim model : \", self.dim_model)\n","        key_context = self.u_k(context)\n","        # print(\"Context shape below key context : \", key_context.shape)\n","        value_context = self.u_v(context)\n","\n","        lambda_k = F.sigmoid(self.w1_k(k) + self.w2_k(key_context))\n","        lambda_v = F.sigmoid(self.w1_v(v) + self.w2_v(value_context))\n","\n","        k_cap = (1-lambda_k) * k + (lambda_k) * key_context\n","        v_cap = (1-lambda_v) * v + (lambda_v) * value_context\n","\n","        attention_output, _ = self.attention_layer(query = q,\n","                                                   key = k_cap,\n","                                                   value = v_cap)\n","  \n","        return attention_output                                     \n","\n"],"metadata":{"id":"lEwhnknKJF1h","executionInfo":{"status":"ok","timestamp":1682418025965,"user_tz":-330,"elapsed":23,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["class MAF_main(nn.Module):\n","    def __init__(self, \n","                dim_model,\n","                dropout_rate):\n","        super(MAF_main, self).__init__()\n","        self.dropout_rate = dropout_rate\n","\n","\n","        self.context_attention = ContextAwareAttention(dim_model=dim_model,\n","                                                            dim_context=dim_model,\n","                                                            dropout_rate=dropout_rate)\n","\n","       \n","        self.gate = nn.Linear(2*dim_model, dim_model)\n","        self.dropout_layer = nn.Dropout(dropout_rate)\n","        self.final_layer_norm = nn.LayerNorm(dim_model)\n","\n","    def forward(self,\n","                main_input,\n","                context_input):\n","\n","\n","        \n","        mixed_out = self.context_attention(q=main_input,\n","                                                    k=main_input,\n","                                                    v=main_input,\n","                                                    context=context_input)\n","\n","        weight_v = F.sigmoid(self.gate(torch.cat([mixed_out, main_input], dim=-1)))\n","\n","        output = self.final_layer_norm(weight_v * mixed_out  + main_input)\n","\n","        return output"],"metadata":{"id":"Rf7mCvxMHlkr","executionInfo":{"status":"ok","timestamp":1682418025965,"user_tz":-330,"elapsed":23,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["class MultimodalClassification(nn.Module):\n","    def __init__(\n","        self, \n","        input_dim: int,\n","        inned_dim: int,\n","        num_classes: int,\n","        pooler_dropout: float\n","    ):\n","        super().__init__()\n","        self.dense = nn.Linear(input_dim, inned_dim)\n","        self.dropout = nn.Dropout(p = pooler_dropout)\n","        self.out_proj = nn.Linear(inned_dim, num_classes)\n","\n","    def forward(self, hidden_states):\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = torch.tanh(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.out_proj(hidden_states)\n","        return hidden_states "],"metadata":{"id":"9ND7fKneCW61","executionInfo":{"status":"ok","timestamp":1682418025965,"user_tz":-330,"elapsed":23,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["class MultimodalVideo(torch.nn.Module):\n","  def __init__(self):\n","    super(MultimodalVideo, self).__init__()\n","    self.bart_model = bart_model\n","\n","    self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n","    self.acoustic_dimension_transform = nn.Linear(ACOUSTIC_DIM, 768, bias = False)\n","\n","    self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n","    self.visual_dimension_transform = nn.Linear(VISUAL_DIM, 768, bias = False)\n","\n","\n","    self.maf_one = MAF_main(dim_model=768,\n","                             dropout_rate=0.2)\n","        \n","    self.maf_two = MAF_main(dim_model=768,\n","                             dropout_rate=0.2)\n","    \n","    self.classification_head = MultimodalClassification(\n","            768,\n","            768,\n","            2,\n","            0.0\n","        )\n","    \n","  def forward(self, input_ids, attention_mask, acoustic_input, visual_input, labels):\n","\n","    acoustic_input = acoustic_input.permute(0,2,1)\n","    acoustic_input = self.acoustic_context_transform(acoustic_input.float())\n","    acoustic_input = acoustic_input.permute(0,2,1)\n","    acoustic_input = self.acoustic_dimension_transform(acoustic_input)\n","\n","\n","    visual_input = visual_input.permute(0,2,1)\n","    visual_input = self.visual_context_transform(visual_input.float())\n","    visual_input = visual_input.permute(0,2,1)\n","    visual_input = self.visual_dimension_transform(visual_input)\n","\n","\n","\n","    bart_output = self.bart_model(input_ids, attention_mask)['last_hidden_state']\n","\n","    output_one = self.maf_one(main_input =  visual_input, context_input = bart_output)  \n","\n","    output_two = self.maf_two(main_input = output_one, context_input = acoustic_input)\n","\n","    # output_one = self.maf_one(main_input =  visual_input, context_input = acoustic_input)  \n","\n","    # output_two = self.maf_two(main_input = output_one, context_input = bart_output)\n","\n","    final_out = output_two[:, -1, :]\n","    # final_out = output_one[:, -1, :]\n","    # final_out = visual_input[:, -1, :]\n","\n","    final_out = self.classification_head(final_out)\n","\n","    loss_fct = CrossEntropyLoss()\n","\n","    loss = loss_fct(final_out.view(-1, 2), labels.view(-1))\n","\n","    temp_dict = {}\n","\n","    temp_dict['logits'] = final_out\n","    temp_dict['loss'] = loss\n","\n","    return temp_dict    \n","\n","\n","\n","    \n","\n","    \n","    \n","\n","\n"],"metadata":{"id":"ovNsT7dg2vRc","executionInfo":{"status":"ok","timestamp":1682418025965,"user_tz":-330,"elapsed":22,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["model = MultimodalVideo()\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkrh6tAdJWDm","executionInfo":{"status":"ok","timestamp":1682418028363,"user_tz":-330,"elapsed":2420,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}},"outputId":"86fc625c-e6f2-4fe5-891c-45752899a18f"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultimodalVideo(\n","  (bart_model): BartModel(\n","    (shared): Embedding(50267, 768)\n","    (encoder): BartEncoder(\n","      (embed_tokens): Embedding(50267, 768)\n","      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n","      (layers): ModuleList(\n","        (0): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): BartDecoder(\n","      (embed_tokens): Embedding(50267, 768)\n","      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n","      (layers): ModuleList(\n","        (0): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (acoustic_context_transform): Linear(in_features=1000, out_features=500, bias=False)\n","  (acoustic_dimension_transform): Linear(in_features=768, out_features=768, bias=False)\n","  (visual_context_transform): Linear(in_features=480, out_features=500, bias=False)\n","  (visual_dimension_transform): Linear(in_features=2048, out_features=768, bias=False)\n","  (maf_one): MAF_main(\n","    (context_attention): ContextAwareAttention(\n","      (attention_layer): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","      )\n","      (u_k): Linear(in_features=768, out_features=768, bias=False)\n","      (w1_k): Linear(in_features=768, out_features=1, bias=False)\n","      (w2_k): Linear(in_features=768, out_features=1, bias=False)\n","      (u_v): Linear(in_features=768, out_features=768, bias=False)\n","      (w1_v): Linear(in_features=768, out_features=1, bias=False)\n","      (w2_v): Linear(in_features=768, out_features=1, bias=False)\n","    )\n","    (gate): Linear(in_features=1536, out_features=768, bias=True)\n","    (dropout_layer): Dropout(p=0.2, inplace=False)\n","    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (maf_two): MAF_main(\n","    (context_attention): ContextAwareAttention(\n","      (attention_layer): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","      )\n","      (u_k): Linear(in_features=768, out_features=768, bias=False)\n","      (w1_k): Linear(in_features=768, out_features=1, bias=False)\n","      (w2_k): Linear(in_features=768, out_features=1, bias=False)\n","      (u_v): Linear(in_features=768, out_features=768, bias=False)\n","      (w1_v): Linear(in_features=768, out_features=1, bias=False)\n","      (w2_v): Linear(in_features=768, out_features=1, bias=False)\n","    )\n","    (gate): Linear(in_features=1536, out_features=768, bias=True)\n","    (dropout_layer): Dropout(p=0.2, inplace=False)\n","    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (classification_head): MultimodalClassification(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.0, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":[],"metadata":{"id":"Uc9qK9bGJWCw","executionInfo":{"status":"ok","timestamp":1682418028363,"user_tz":-330,"elapsed":34,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1682418028364,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"inWKX8K6GV50","outputId":"820d65b3-bb7d-48b0-fb7e-ec98538319e6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":39}],"source":["foldNum"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1682418028364,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"RunFr5b0Ni1u"},"outputs":[],"source":["with open('/content/drive/MyDrive/Colab Notebooks/32/train_audio_fold_'+str(foldNum)+'.p', 'rb') as f:\n","  train_audio_data_utterance1 = pickle.load(f)\n"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1682418028364,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"FAMm_6j1mHg-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1682418028364,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"pGFk4HCwOo-A","outputId":"8c1a55c9-98d1-4306-e3e6-c91c665fde3c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{},"execution_count":41}],"source":["type(train_audio_data_utterance1)"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1682418028364,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"sK4GbT8ZOq2W","outputId":"861117e1-a9c9-45b4-d147-723abb562ded"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["552"]},"metadata":{},"execution_count":42}],"source":["len(train_audio_data_utterance1)"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418028364,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"GZRAOA88iYB-","outputId":"184210c9-0c19-45c6-8cea-509e434f5edd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([200, 768])"]},"metadata":{},"execution_count":43}],"source":["train_audio_data_utterance1[0].shape"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682418028364,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"X73Hp5HxOsEl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682418028365,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"yM1zXLalOifj"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682418028365,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"tvBQbRyFN5zu"},"outputs":[],"source":["with open('/content/drive/MyDrive/Colab Notebooks/32/test_audio_fold_'+str(foldNum)+'.p', 'rb') as f:\n","  test_audio_data_utterance1 = pickle.load(f)"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682418028365,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"v9m8Fi_VmOVK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682418028365,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"ZctmFGmNQsMN","outputId":"e29c508d-71b5-4a0a-8654-c20911cc559b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["138"]},"metadata":{},"execution_count":45}],"source":["len(test_audio_data_utterance1)"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682418028366,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"Dz2xdgV7PzbN","outputId":"9de0a07a-20e9-4d03-80a7-df8c0d1409aa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["138"]},"metadata":{},"execution_count":46}],"source":["len(test_audio_data_utterance1)"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":2009,"status":"ok","timestamp":1682418030370,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"Yw0sc0aaNzOO"},"outputs":[],"source":["with open('/content/drive/MyDrive/Colab Notebooks/32/train_video_fold_'+str(foldNum)+'.p', 'rb') as f:\n","  train_image_data_utterance1 = pickle.load(f)"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1682418030370,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"0yT8QM77QvJS","outputId":"cdd7b0ca-d584-4b96-defb-1fc70ef1c1e6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["552"]},"metadata":{},"execution_count":48}],"source":["len(train_image_data_utterance1)"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1682418030370,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"aUEvmMBNOPFE"},"outputs":[],"source":["with open('/content/drive/MyDrive/Colab Notebooks/32/test_video_fold_'+str(foldNum)+'.p', 'rb') as f:\n","  test_image_data_utterance1 = pickle.load(f)"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1682418030370,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"gm6Z6Fc0QxSW","outputId":"7db61238-1218-4a77-f144-290b47da1107"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["138"]},"metadata":{},"execution_count":50}],"source":["len(test_image_data_utterance1)"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1682418030371,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"KIEMeHJ6RoXS","outputId":"49b316dd-7f8e-4152-c504-1ba0315ef40c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.]])"]},"metadata":{},"execution_count":51}],"source":["tp = torch.ones(4,5)\n","tp"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1682418030371,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"o8vsmq4FS48j","outputId":"a0fe5427-6938-4cf2-d118-cb99e234f715"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 0., 0., 0.]])"]},"metadata":{},"execution_count":52}],"source":["torch.zeros(5 - tp.shape[0], 5)"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1682418030371,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"6CNdKCeQRrN8","outputId":"de33e95a-8bb4-465a-8e14-1a370ad45c75"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [0., 0., 0., 0., 0.]])"]},"metadata":{},"execution_count":53}],"source":["torch.cat([tp, torch.zeros(5 - tp.shape[0], 5)])"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1682418030371,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"qO9elV8EQAi1"},"outputs":[],"source":["def pad_seq(tensor, dim, max_len):\n","  if max_len > tensor.shape[0] :\n","    return torch.cat([tensor, torch.zeros(max_len - tensor.shape[0], dim)])\n","  else:\n","    return tensor[:max_len]  "]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1682418030371,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"HmBt6xZ1TElw","outputId":"509597d3-83c5-4322-f3f9-ca4ea7862e68"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["768"]},"metadata":{},"execution_count":55}],"source":["ACOUSTIC_DIM"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1682418030371,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"4cUPe2LxQ04S","outputId":"5d30bc5d-5f53-43fc-a920-a56b0a931a57"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([552, 1000, 768])"]},"metadata":{},"execution_count":56}],"source":["train_audio_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n","                                                   dim = ACOUSTIC_DIM,\n","                                                   max_len = ACOUSTIC_MAX_LEN)\n","                                                  for a in train_audio_data_utterance1], 0)\n","train_audio_data_utterance1.shape"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1682418030372,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"x6-frFIpTzrW","outputId":"802646bb-da8a-4e97-cec8-f4e6d4101a40"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([138, 1000, 768])"]},"metadata":{},"execution_count":57}],"source":["test_audio_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n","                                                   dim = ACOUSTIC_DIM,\n","                                                   max_len = ACOUSTIC_MAX_LEN)\n","                                                  for a in test_audio_data_utterance1], 0)\n","test_audio_data_utterance1.shape"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682418030372,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"WEDVn1TaUicC","outputId":"f1e01e87-2894-4b61-d966-45b081fb046e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2048"]},"metadata":{},"execution_count":58}],"source":["VISUAL_DIM"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682418030372,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"7ArX7k9VUkWO","outputId":"0ae359ed-8419-414d-d7be-6fde5801cd17"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["480"]},"metadata":{},"execution_count":59}],"source":["VISUAL_MAX_LEN"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1091,"status":"ok","timestamp":1682418031459,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"PO2OLh5yUTF2","outputId":"1d5bb75c-49c0-442d-e217-1f50745b27d0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([552, 480, 2048])"]},"metadata":{},"execution_count":60}],"source":["train_image_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n","                                                   dim = VISUAL_DIM,\n","                                                   max_len = VISUAL_MAX_LEN)\n","                                                  for a in train_image_data_utterance1], 0)\n","train_image_data_utterance1.shape"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1682418031459,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"9BAAQoCcUv34","outputId":"c84c4cb8-c8b6-4084-fc8e-a59e126c1fce"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([138, 480, 2048])"]},"metadata":{},"execution_count":61}],"source":["test_image_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n","                                                   dim = VISUAL_DIM,\n","                                                   max_len = VISUAL_MAX_LEN)\n","                                                  for a in test_image_data_utterance1], 0)\n","test_image_data_utterance1.shape"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1682418031459,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"TTMNyPhSUdBs"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":62,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1682418031460,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"j3V8UO6HH0xE"},"outputs":[],"source":["# path = \"/content/drive/MyDrive/Colab Notebooks/32/datasetTrue_original/sarcasmDataset_speaker_dependent_True.npz\"\n","# data2 = np.load(path, mmap_mode=True)   \n","\n","# train_audio_data_utterance1 = data2['feautesUA_train'][foldNum]\n","# train_image_data_utterance1 = data2['feautesUV_train'][foldNum]\n","\n","# test_audio_data_utterance1 = data2['feautesUA_test'][foldNum]\n","# test_image_data_utterance1 = data2['feautesUV_test'][foldNum]"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"pdf5hGlVH5Zw","executionInfo":{"status":"ok","timestamp":1682418031460,"user_tz":-330,"elapsed":26,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":["# model = MultimodalBartForSequenceClassification.from_pretrained(\"facebook/bart-base\")\n","# print(model)\n","\n","# tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-base')\n","# print(\"Tokenizer : \", tokenizer)\n","\n","# num_param = sum(p.numel() for p in model.parameters())\n","# print(\"Total parameters : \", num_param/1e6)"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1682418031460,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"n_N4HcMQ9hXX","outputId":"307ce68a-302c-4755-867c-ac8844db5856"},"outputs":[{"output_type":"stream","name":"stdout","text":["Count :  0  name :  bart_model.shared.weight\n","Count :  1  name :  bart_model.encoder.embed_positions.weight\n","Count :  2  name :  bart_model.encoder.layers.0.self_attn.k_proj.weight\n","Count :  3  name :  bart_model.encoder.layers.0.self_attn.k_proj.bias\n","Count :  4  name :  bart_model.encoder.layers.0.self_attn.v_proj.weight\n","Count :  5  name :  bart_model.encoder.layers.0.self_attn.v_proj.bias\n","Count :  6  name :  bart_model.encoder.layers.0.self_attn.q_proj.weight\n","Count :  7  name :  bart_model.encoder.layers.0.self_attn.q_proj.bias\n","Count :  8  name :  bart_model.encoder.layers.0.self_attn.out_proj.weight\n","Count :  9  name :  bart_model.encoder.layers.0.self_attn.out_proj.bias\n","Count :  10  name :  bart_model.encoder.layers.0.self_attn_layer_norm.weight\n","Count :  11  name :  bart_model.encoder.layers.0.self_attn_layer_norm.bias\n","Count :  12  name :  bart_model.encoder.layers.0.fc1.weight\n","Count :  13  name :  bart_model.encoder.layers.0.fc1.bias\n","Count :  14  name :  bart_model.encoder.layers.0.fc2.weight\n","Count :  15  name :  bart_model.encoder.layers.0.fc2.bias\n","Count :  16  name :  bart_model.encoder.layers.0.final_layer_norm.weight\n","Count :  17  name :  bart_model.encoder.layers.0.final_layer_norm.bias\n","Count :  18  name :  bart_model.encoder.layers.1.self_attn.k_proj.weight\n","Count :  19  name :  bart_model.encoder.layers.1.self_attn.k_proj.bias\n","Count :  20  name :  bart_model.encoder.layers.1.self_attn.v_proj.weight\n","Count :  21  name :  bart_model.encoder.layers.1.self_attn.v_proj.bias\n","Count :  22  name :  bart_model.encoder.layers.1.self_attn.q_proj.weight\n","Count :  23  name :  bart_model.encoder.layers.1.self_attn.q_proj.bias\n","Count :  24  name :  bart_model.encoder.layers.1.self_attn.out_proj.weight\n","Count :  25  name :  bart_model.encoder.layers.1.self_attn.out_proj.bias\n","Count :  26  name :  bart_model.encoder.layers.1.self_attn_layer_norm.weight\n","Count :  27  name :  bart_model.encoder.layers.1.self_attn_layer_norm.bias\n","Count :  28  name :  bart_model.encoder.layers.1.fc1.weight\n","Count :  29  name :  bart_model.encoder.layers.1.fc1.bias\n","Count :  30  name :  bart_model.encoder.layers.1.fc2.weight\n","Count :  31  name :  bart_model.encoder.layers.1.fc2.bias\n","Count :  32  name :  bart_model.encoder.layers.1.final_layer_norm.weight\n","Count :  33  name :  bart_model.encoder.layers.1.final_layer_norm.bias\n","Count :  34  name :  bart_model.encoder.layers.2.self_attn.k_proj.weight\n","Count :  35  name :  bart_model.encoder.layers.2.self_attn.k_proj.bias\n","Count :  36  name :  bart_model.encoder.layers.2.self_attn.v_proj.weight\n","Count :  37  name :  bart_model.encoder.layers.2.self_attn.v_proj.bias\n","Count :  38  name :  bart_model.encoder.layers.2.self_attn.q_proj.weight\n","Count :  39  name :  bart_model.encoder.layers.2.self_attn.q_proj.bias\n","Count :  40  name :  bart_model.encoder.layers.2.self_attn.out_proj.weight\n","Count :  41  name :  bart_model.encoder.layers.2.self_attn.out_proj.bias\n","Count :  42  name :  bart_model.encoder.layers.2.self_attn_layer_norm.weight\n","Count :  43  name :  bart_model.encoder.layers.2.self_attn_layer_norm.bias\n","Count :  44  name :  bart_model.encoder.layers.2.fc1.weight\n","Count :  45  name :  bart_model.encoder.layers.2.fc1.bias\n","Count :  46  name :  bart_model.encoder.layers.2.fc2.weight\n","Count :  47  name :  bart_model.encoder.layers.2.fc2.bias\n","Count :  48  name :  bart_model.encoder.layers.2.final_layer_norm.weight\n","Count :  49  name :  bart_model.encoder.layers.2.final_layer_norm.bias\n","Count :  50  name :  bart_model.encoder.layers.3.self_attn.k_proj.weight\n","Count :  51  name :  bart_model.encoder.layers.3.self_attn.k_proj.bias\n","Count :  52  name :  bart_model.encoder.layers.3.self_attn.v_proj.weight\n","Count :  53  name :  bart_model.encoder.layers.3.self_attn.v_proj.bias\n","Count :  54  name :  bart_model.encoder.layers.3.self_attn.q_proj.weight\n","Count :  55  name :  bart_model.encoder.layers.3.self_attn.q_proj.bias\n","Count :  56  name :  bart_model.encoder.layers.3.self_attn.out_proj.weight\n","Count :  57  name :  bart_model.encoder.layers.3.self_attn.out_proj.bias\n","Count :  58  name :  bart_model.encoder.layers.3.self_attn_layer_norm.weight\n","Count :  59  name :  bart_model.encoder.layers.3.self_attn_layer_norm.bias\n","Count :  60  name :  bart_model.encoder.layers.3.fc1.weight\n","Count :  61  name :  bart_model.encoder.layers.3.fc1.bias\n","Count :  62  name :  bart_model.encoder.layers.3.fc2.weight\n","Count :  63  name :  bart_model.encoder.layers.3.fc2.bias\n","Count :  64  name :  bart_model.encoder.layers.3.final_layer_norm.weight\n","Count :  65  name :  bart_model.encoder.layers.3.final_layer_norm.bias\n","Count :  66  name :  bart_model.encoder.layers.4.self_attn.k_proj.weight\n","Count :  67  name :  bart_model.encoder.layers.4.self_attn.k_proj.bias\n","Count :  68  name :  bart_model.encoder.layers.4.self_attn.v_proj.weight\n","Count :  69  name :  bart_model.encoder.layers.4.self_attn.v_proj.bias\n","Count :  70  name :  bart_model.encoder.layers.4.self_attn.q_proj.weight\n","Count :  71  name :  bart_model.encoder.layers.4.self_attn.q_proj.bias\n","Count :  72  name :  bart_model.encoder.layers.4.self_attn.out_proj.weight\n","Count :  73  name :  bart_model.encoder.layers.4.self_attn.out_proj.bias\n","Count :  74  name :  bart_model.encoder.layers.4.self_attn_layer_norm.weight\n","Count :  75  name :  bart_model.encoder.layers.4.self_attn_layer_norm.bias\n","Count :  76  name :  bart_model.encoder.layers.4.fc1.weight\n","Count :  77  name :  bart_model.encoder.layers.4.fc1.bias\n","Count :  78  name :  bart_model.encoder.layers.4.fc2.weight\n","Count :  79  name :  bart_model.encoder.layers.4.fc2.bias\n","Count :  80  name :  bart_model.encoder.layers.4.final_layer_norm.weight\n","Count :  81  name :  bart_model.encoder.layers.4.final_layer_norm.bias\n","Count :  82  name :  bart_model.encoder.layers.5.self_attn.k_proj.weight\n","Count :  83  name :  bart_model.encoder.layers.5.self_attn.k_proj.bias\n","Count :  84  name :  bart_model.encoder.layers.5.self_attn.v_proj.weight\n","Count :  85  name :  bart_model.encoder.layers.5.self_attn.v_proj.bias\n","Count :  86  name :  bart_model.encoder.layers.5.self_attn.q_proj.weight\n","Count :  87  name :  bart_model.encoder.layers.5.self_attn.q_proj.bias\n","Count :  88  name :  bart_model.encoder.layers.5.self_attn.out_proj.weight\n","Count :  89  name :  bart_model.encoder.layers.5.self_attn.out_proj.bias\n","Count :  90  name :  bart_model.encoder.layers.5.self_attn_layer_norm.weight\n","Count :  91  name :  bart_model.encoder.layers.5.self_attn_layer_norm.bias\n","Count :  92  name :  bart_model.encoder.layers.5.fc1.weight\n","Count :  93  name :  bart_model.encoder.layers.5.fc1.bias\n","Count :  94  name :  bart_model.encoder.layers.5.fc2.weight\n","Count :  95  name :  bart_model.encoder.layers.5.fc2.bias\n","Count :  96  name :  bart_model.encoder.layers.5.final_layer_norm.weight\n","Count :  97  name :  bart_model.encoder.layers.5.final_layer_norm.bias\n","Count :  98  name :  bart_model.encoder.layernorm_embedding.weight\n","Count :  99  name :  bart_model.encoder.layernorm_embedding.bias\n","Count :  100  name :  bart_model.decoder.embed_positions.weight\n","Count :  101  name :  bart_model.decoder.layers.0.self_attn.k_proj.weight\n","Count :  102  name :  bart_model.decoder.layers.0.self_attn.k_proj.bias\n","Count :  103  name :  bart_model.decoder.layers.0.self_attn.v_proj.weight\n","Count :  104  name :  bart_model.decoder.layers.0.self_attn.v_proj.bias\n","Count :  105  name :  bart_model.decoder.layers.0.self_attn.q_proj.weight\n","Count :  106  name :  bart_model.decoder.layers.0.self_attn.q_proj.bias\n","Count :  107  name :  bart_model.decoder.layers.0.self_attn.out_proj.weight\n","Count :  108  name :  bart_model.decoder.layers.0.self_attn.out_proj.bias\n","Count :  109  name :  bart_model.decoder.layers.0.self_attn_layer_norm.weight\n","Count :  110  name :  bart_model.decoder.layers.0.self_attn_layer_norm.bias\n","Count :  111  name :  bart_model.decoder.layers.0.encoder_attn.k_proj.weight\n","Count :  112  name :  bart_model.decoder.layers.0.encoder_attn.k_proj.bias\n","Count :  113  name :  bart_model.decoder.layers.0.encoder_attn.v_proj.weight\n","Count :  114  name :  bart_model.decoder.layers.0.encoder_attn.v_proj.bias\n","Count :  115  name :  bart_model.decoder.layers.0.encoder_attn.q_proj.weight\n","Count :  116  name :  bart_model.decoder.layers.0.encoder_attn.q_proj.bias\n","Count :  117  name :  bart_model.decoder.layers.0.encoder_attn.out_proj.weight\n","Count :  118  name :  bart_model.decoder.layers.0.encoder_attn.out_proj.bias\n","Count :  119  name :  bart_model.decoder.layers.0.encoder_attn_layer_norm.weight\n","Count :  120  name :  bart_model.decoder.layers.0.encoder_attn_layer_norm.bias\n","Count :  121  name :  bart_model.decoder.layers.0.fc1.weight\n","Count :  122  name :  bart_model.decoder.layers.0.fc1.bias\n","Count :  123  name :  bart_model.decoder.layers.0.fc2.weight\n","Count :  124  name :  bart_model.decoder.layers.0.fc2.bias\n","Count :  125  name :  bart_model.decoder.layers.0.final_layer_norm.weight\n","Count :  126  name :  bart_model.decoder.layers.0.final_layer_norm.bias\n","Count :  127  name :  bart_model.decoder.layers.1.self_attn.k_proj.weight\n","Count :  128  name :  bart_model.decoder.layers.1.self_attn.k_proj.bias\n","Count :  129  name :  bart_model.decoder.layers.1.self_attn.v_proj.weight\n","Count :  130  name :  bart_model.decoder.layers.1.self_attn.v_proj.bias\n","Count :  131  name :  bart_model.decoder.layers.1.self_attn.q_proj.weight\n","Count :  132  name :  bart_model.decoder.layers.1.self_attn.q_proj.bias\n","Count :  133  name :  bart_model.decoder.layers.1.self_attn.out_proj.weight\n","Count :  134  name :  bart_model.decoder.layers.1.self_attn.out_proj.bias\n","Count :  135  name :  bart_model.decoder.layers.1.self_attn_layer_norm.weight\n","Count :  136  name :  bart_model.decoder.layers.1.self_attn_layer_norm.bias\n","Count :  137  name :  bart_model.decoder.layers.1.encoder_attn.k_proj.weight\n","Count :  138  name :  bart_model.decoder.layers.1.encoder_attn.k_proj.bias\n","Count :  139  name :  bart_model.decoder.layers.1.encoder_attn.v_proj.weight\n","Count :  140  name :  bart_model.decoder.layers.1.encoder_attn.v_proj.bias\n","Count :  141  name :  bart_model.decoder.layers.1.encoder_attn.q_proj.weight\n","Count :  142  name :  bart_model.decoder.layers.1.encoder_attn.q_proj.bias\n","Count :  143  name :  bart_model.decoder.layers.1.encoder_attn.out_proj.weight\n","Count :  144  name :  bart_model.decoder.layers.1.encoder_attn.out_proj.bias\n","Count :  145  name :  bart_model.decoder.layers.1.encoder_attn_layer_norm.weight\n","Count :  146  name :  bart_model.decoder.layers.1.encoder_attn_layer_norm.bias\n","Count :  147  name :  bart_model.decoder.layers.1.fc1.weight\n","Count :  148  name :  bart_model.decoder.layers.1.fc1.bias\n","Count :  149  name :  bart_model.decoder.layers.1.fc2.weight\n","Count :  150  name :  bart_model.decoder.layers.1.fc2.bias\n","Count :  151  name :  bart_model.decoder.layers.1.final_layer_norm.weight\n","Count :  152  name :  bart_model.decoder.layers.1.final_layer_norm.bias\n","Count :  153  name :  bart_model.decoder.layers.2.self_attn.k_proj.weight\n","Count :  154  name :  bart_model.decoder.layers.2.self_attn.k_proj.bias\n","Count :  155  name :  bart_model.decoder.layers.2.self_attn.v_proj.weight\n","Count :  156  name :  bart_model.decoder.layers.2.self_attn.v_proj.bias\n","Count :  157  name :  bart_model.decoder.layers.2.self_attn.q_proj.weight\n","Count :  158  name :  bart_model.decoder.layers.2.self_attn.q_proj.bias\n","Count :  159  name :  bart_model.decoder.layers.2.self_attn.out_proj.weight\n","Count :  160  name :  bart_model.decoder.layers.2.self_attn.out_proj.bias\n","Count :  161  name :  bart_model.decoder.layers.2.self_attn_layer_norm.weight\n","Count :  162  name :  bart_model.decoder.layers.2.self_attn_layer_norm.bias\n","Count :  163  name :  bart_model.decoder.layers.2.encoder_attn.k_proj.weight\n","Count :  164  name :  bart_model.decoder.layers.2.encoder_attn.k_proj.bias\n","Count :  165  name :  bart_model.decoder.layers.2.encoder_attn.v_proj.weight\n","Count :  166  name :  bart_model.decoder.layers.2.encoder_attn.v_proj.bias\n","Count :  167  name :  bart_model.decoder.layers.2.encoder_attn.q_proj.weight\n","Count :  168  name :  bart_model.decoder.layers.2.encoder_attn.q_proj.bias\n","Count :  169  name :  bart_model.decoder.layers.2.encoder_attn.out_proj.weight\n","Count :  170  name :  bart_model.decoder.layers.2.encoder_attn.out_proj.bias\n","Count :  171  name :  bart_model.decoder.layers.2.encoder_attn_layer_norm.weight\n","Count :  172  name :  bart_model.decoder.layers.2.encoder_attn_layer_norm.bias\n","Count :  173  name :  bart_model.decoder.layers.2.fc1.weight\n","Count :  174  name :  bart_model.decoder.layers.2.fc1.bias\n","Count :  175  name :  bart_model.decoder.layers.2.fc2.weight\n","Count :  176  name :  bart_model.decoder.layers.2.fc2.bias\n","Count :  177  name :  bart_model.decoder.layers.2.final_layer_norm.weight\n","Count :  178  name :  bart_model.decoder.layers.2.final_layer_norm.bias\n","Count :  179  name :  bart_model.decoder.layers.3.self_attn.k_proj.weight\n","Count :  180  name :  bart_model.decoder.layers.3.self_attn.k_proj.bias\n","Count :  181  name :  bart_model.decoder.layers.3.self_attn.v_proj.weight\n","Count :  182  name :  bart_model.decoder.layers.3.self_attn.v_proj.bias\n","Count :  183  name :  bart_model.decoder.layers.3.self_attn.q_proj.weight\n","Count :  184  name :  bart_model.decoder.layers.3.self_attn.q_proj.bias\n","Count :  185  name :  bart_model.decoder.layers.3.self_attn.out_proj.weight\n","Count :  186  name :  bart_model.decoder.layers.3.self_attn.out_proj.bias\n","Count :  187  name :  bart_model.decoder.layers.3.self_attn_layer_norm.weight\n","Count :  188  name :  bart_model.decoder.layers.3.self_attn_layer_norm.bias\n","Count :  189  name :  bart_model.decoder.layers.3.encoder_attn.k_proj.weight\n","Count :  190  name :  bart_model.decoder.layers.3.encoder_attn.k_proj.bias\n","Count :  191  name :  bart_model.decoder.layers.3.encoder_attn.v_proj.weight\n","Count :  192  name :  bart_model.decoder.layers.3.encoder_attn.v_proj.bias\n","Count :  193  name :  bart_model.decoder.layers.3.encoder_attn.q_proj.weight\n","Count :  194  name :  bart_model.decoder.layers.3.encoder_attn.q_proj.bias\n","Count :  195  name :  bart_model.decoder.layers.3.encoder_attn.out_proj.weight\n","Count :  196  name :  bart_model.decoder.layers.3.encoder_attn.out_proj.bias\n","Count :  197  name :  bart_model.decoder.layers.3.encoder_attn_layer_norm.weight\n","Count :  198  name :  bart_model.decoder.layers.3.encoder_attn_layer_norm.bias\n","Count :  199  name :  bart_model.decoder.layers.3.fc1.weight\n","Count :  200  name :  bart_model.decoder.layers.3.fc1.bias\n","Count :  201  name :  bart_model.decoder.layers.3.fc2.weight\n","Count :  202  name :  bart_model.decoder.layers.3.fc2.bias\n","Count :  203  name :  bart_model.decoder.layers.3.final_layer_norm.weight\n","Count :  204  name :  bart_model.decoder.layers.3.final_layer_norm.bias\n","Count :  205  name :  bart_model.decoder.layers.4.self_attn.k_proj.weight\n","Count :  206  name :  bart_model.decoder.layers.4.self_attn.k_proj.bias\n","Count :  207  name :  bart_model.decoder.layers.4.self_attn.v_proj.weight\n","Count :  208  name :  bart_model.decoder.layers.4.self_attn.v_proj.bias\n","Count :  209  name :  bart_model.decoder.layers.4.self_attn.q_proj.weight\n","Count :  210  name :  bart_model.decoder.layers.4.self_attn.q_proj.bias\n","Count :  211  name :  bart_model.decoder.layers.4.self_attn.out_proj.weight\n","Count :  212  name :  bart_model.decoder.layers.4.self_attn.out_proj.bias\n","Count :  213  name :  bart_model.decoder.layers.4.self_attn_layer_norm.weight\n","Count :  214  name :  bart_model.decoder.layers.4.self_attn_layer_norm.bias\n","Count :  215  name :  bart_model.decoder.layers.4.encoder_attn.k_proj.weight\n","Count :  216  name :  bart_model.decoder.layers.4.encoder_attn.k_proj.bias\n","Count :  217  name :  bart_model.decoder.layers.4.encoder_attn.v_proj.weight\n","Count :  218  name :  bart_model.decoder.layers.4.encoder_attn.v_proj.bias\n","Count :  219  name :  bart_model.decoder.layers.4.encoder_attn.q_proj.weight\n","Count :  220  name :  bart_model.decoder.layers.4.encoder_attn.q_proj.bias\n","Count :  221  name :  bart_model.decoder.layers.4.encoder_attn.out_proj.weight\n","Count :  222  name :  bart_model.decoder.layers.4.encoder_attn.out_proj.bias\n","Count :  223  name :  bart_model.decoder.layers.4.encoder_attn_layer_norm.weight\n","Count :  224  name :  bart_model.decoder.layers.4.encoder_attn_layer_norm.bias\n","Count :  225  name :  bart_model.decoder.layers.4.fc1.weight\n","Count :  226  name :  bart_model.decoder.layers.4.fc1.bias\n","Count :  227  name :  bart_model.decoder.layers.4.fc2.weight\n","Count :  228  name :  bart_model.decoder.layers.4.fc2.bias\n","Count :  229  name :  bart_model.decoder.layers.4.final_layer_norm.weight\n","Count :  230  name :  bart_model.decoder.layers.4.final_layer_norm.bias\n","Count :  231  name :  bart_model.decoder.layers.5.self_attn.k_proj.weight\n","Count :  232  name :  bart_model.decoder.layers.5.self_attn.k_proj.bias\n","Count :  233  name :  bart_model.decoder.layers.5.self_attn.v_proj.weight\n","Count :  234  name :  bart_model.decoder.layers.5.self_attn.v_proj.bias\n","Count :  235  name :  bart_model.decoder.layers.5.self_attn.q_proj.weight\n","Count :  236  name :  bart_model.decoder.layers.5.self_attn.q_proj.bias\n","Count :  237  name :  bart_model.decoder.layers.5.self_attn.out_proj.weight\n","Count :  238  name :  bart_model.decoder.layers.5.self_attn.out_proj.bias\n","Count :  239  name :  bart_model.decoder.layers.5.self_attn_layer_norm.weight\n","Count :  240  name :  bart_model.decoder.layers.5.self_attn_layer_norm.bias\n","Count :  241  name :  bart_model.decoder.layers.5.encoder_attn.k_proj.weight\n","Count :  242  name :  bart_model.decoder.layers.5.encoder_attn.k_proj.bias\n","Count :  243  name :  bart_model.decoder.layers.5.encoder_attn.v_proj.weight\n","Count :  244  name :  bart_model.decoder.layers.5.encoder_attn.v_proj.bias\n","Count :  245  name :  bart_model.decoder.layers.5.encoder_attn.q_proj.weight\n","Count :  246  name :  bart_model.decoder.layers.5.encoder_attn.q_proj.bias\n","Count :  247  name :  bart_model.decoder.layers.5.encoder_attn.out_proj.weight\n","Count :  248  name :  bart_model.decoder.layers.5.encoder_attn.out_proj.bias\n","Count :  249  name :  bart_model.decoder.layers.5.encoder_attn_layer_norm.weight\n","Count :  250  name :  bart_model.decoder.layers.5.encoder_attn_layer_norm.bias\n","Count :  251  name :  bart_model.decoder.layers.5.fc1.weight\n","Count :  252  name :  bart_model.decoder.layers.5.fc1.bias\n","Count :  253  name :  bart_model.decoder.layers.5.fc2.weight\n","Count :  254  name :  bart_model.decoder.layers.5.fc2.bias\n","Count :  255  name :  bart_model.decoder.layers.5.final_layer_norm.weight\n","Count :  256  name :  bart_model.decoder.layers.5.final_layer_norm.bias\n","Count :  257  name :  bart_model.decoder.layernorm_embedding.weight\n","Count :  258  name :  bart_model.decoder.layernorm_embedding.bias\n","Count :  259  name :  acoustic_context_transform.weight\n","Count :  260  name :  acoustic_dimension_transform.weight\n","Count :  261  name :  visual_context_transform.weight\n","Count :  262  name :  visual_dimension_transform.weight\n","Count :  263  name :  maf_one.context_attention.attention_layer.in_proj_weight\n","Count :  264  name :  maf_one.context_attention.attention_layer.in_proj_bias\n","Count :  265  name :  maf_one.context_attention.attention_layer.out_proj.weight\n","Count :  266  name :  maf_one.context_attention.attention_layer.out_proj.bias\n","Count :  267  name :  maf_one.context_attention.u_k.weight\n","Count :  268  name :  maf_one.context_attention.w1_k.weight\n","Count :  269  name :  maf_one.context_attention.w2_k.weight\n","Count :  270  name :  maf_one.context_attention.u_v.weight\n","Count :  271  name :  maf_one.context_attention.w1_v.weight\n","Count :  272  name :  maf_one.context_attention.w2_v.weight\n","Count :  273  name :  maf_one.gate.weight\n","Count :  274  name :  maf_one.gate.bias\n","Count :  275  name :  maf_one.final_layer_norm.weight\n","Count :  276  name :  maf_one.final_layer_norm.bias\n","Count :  277  name :  maf_two.context_attention.attention_layer.in_proj_weight\n","Count :  278  name :  maf_two.context_attention.attention_layer.in_proj_bias\n","Count :  279  name :  maf_two.context_attention.attention_layer.out_proj.weight\n","Count :  280  name :  maf_two.context_attention.attention_layer.out_proj.bias\n","Count :  281  name :  maf_two.context_attention.u_k.weight\n","Count :  282  name :  maf_two.context_attention.w1_k.weight\n","Count :  283  name :  maf_two.context_attention.w2_k.weight\n","Count :  284  name :  maf_two.context_attention.u_v.weight\n","Count :  285  name :  maf_two.context_attention.w1_v.weight\n","Count :  286  name :  maf_two.context_attention.w2_v.weight\n","Count :  287  name :  maf_two.gate.weight\n","Count :  288  name :  maf_two.gate.bias\n","Count :  289  name :  maf_two.final_layer_norm.weight\n","Count :  290  name :  maf_two.final_layer_norm.bias\n","Count :  291  name :  classification_head.dense.weight\n","Count :  292  name :  classification_head.dense.bias\n","Count :  293  name :  classification_head.out_proj.weight\n","Count :  294  name :  classification_head.out_proj.bias\n"]}],"source":["cnt = 0\n","for name, param in model.named_parameters():\n","    print(\"Count : \", cnt, \" name : \", name)\n","    cnt+=1"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1682418031460,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"feA-7jSwIBQO","outputId":"f6a1a5d3-3b04-4671-fb71-79989cc7fb63"},"outputs":[{"output_type":"stream","name":"stdout","text":["Count :  259  name :  acoustic_context_transform.weight\n","Count :  260  name :  acoustic_dimension_transform.weight\n","Count :  261  name :  visual_context_transform.weight\n","Count :  262  name :  visual_dimension_transform.weight\n","Count :  263  name :  maf_one.context_attention.attention_layer.in_proj_weight\n","Count :  264  name :  maf_one.context_attention.attention_layer.in_proj_bias\n","Count :  265  name :  maf_one.context_attention.attention_layer.out_proj.weight\n","Count :  266  name :  maf_one.context_attention.attention_layer.out_proj.bias\n","Count :  267  name :  maf_one.context_attention.u_k.weight\n","Count :  268  name :  maf_one.context_attention.w1_k.weight\n","Count :  269  name :  maf_one.context_attention.w2_k.weight\n","Count :  270  name :  maf_one.context_attention.u_v.weight\n","Count :  271  name :  maf_one.context_attention.w1_v.weight\n","Count :  272  name :  maf_one.context_attention.w2_v.weight\n","Count :  273  name :  maf_one.gate.weight\n","Count :  274  name :  maf_one.gate.bias\n","Count :  275  name :  maf_one.final_layer_norm.weight\n","Count :  276  name :  maf_one.final_layer_norm.bias\n","Count :  277  name :  maf_two.context_attention.attention_layer.in_proj_weight\n","Count :  278  name :  maf_two.context_attention.attention_layer.in_proj_bias\n","Count :  279  name :  maf_two.context_attention.attention_layer.out_proj.weight\n","Count :  280  name :  maf_two.context_attention.attention_layer.out_proj.bias\n","Count :  281  name :  maf_two.context_attention.u_k.weight\n","Count :  282  name :  maf_two.context_attention.w1_k.weight\n","Count :  283  name :  maf_two.context_attention.w2_k.weight\n","Count :  284  name :  maf_two.context_attention.u_v.weight\n","Count :  285  name :  maf_two.context_attention.w1_v.weight\n","Count :  286  name :  maf_two.context_attention.w2_v.weight\n","Count :  287  name :  maf_two.gate.weight\n","Count :  288  name :  maf_two.gate.bias\n","Count :  289  name :  maf_two.final_layer_norm.weight\n","Count :  290  name :  maf_two.final_layer_norm.bias\n","Count :  291  name :  classification_head.dense.weight\n","Count :  292  name :  classification_head.dense.bias\n","Count :  293  name :  classification_head.out_proj.weight\n","Count :  294  name :  classification_head.out_proj.bias\n","Total trainanable parameters :  12.948898\n"]}],"source":["cnt = 0\n","for name, param in model.named_parameters():\n","\n","    if(cnt>=259):\n"," \n","        param.requires_grad = True\n","        print(\"Count : \", cnt, \" name : \", name)\n","\n","    else:\n","        param.requires_grad = False    \n","    cnt+=1\n","        \n","\n","num_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"Total trainanable parameters : \", num_param/1e6)"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1682418031460,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"amBD8a4_rhUM","outputId":"6e9afaf1-2afa-4ae3-a4af-5dc841701a60"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":66}],"source":["foldNum"]},{"cell_type":"code","execution_count":67,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682418031461,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"46OM_4HZjKNp"},"outputs":[],"source":["with open(\"/content/drive/MyDrive/Colab Notebooks/32/json_file_fold.p\", \"rb\") as f:\n","    text_file = pickle.load(f)\n","\n","train_text = text_file[\"json_file_list_\" + str(foldNum) +\"_train\"]\n","test_text =  text_file[\"json_file_list_\" + str(foldNum) +\"_test\"]"]},{"cell_type":"code","execution_count":68,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682418031461,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"HaH0ocF_zD0O"},"outputs":[],"source":["# Trainlen = 483"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682418031461,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"sL8tkEu3kCrJ","outputId":"354c1de2-2ac9-438b-f5ff-5dd59e182813"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.Tensor'>\n","torch.Size([552, 1000, 768])\n"]}],"source":["print(type(train_audio_data_utterance1))\n","print(train_audio_data_utterance1.shape)"]},{"cell_type":"code","execution_count":70,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682418031462,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"DRRQRZeBNYum"},"outputs":[],"source":["# combined_train_data = []\n","# for j in range(len(train_text)):\n","#   temp_text = train_text[j]\n","#   temp_audio = train_audio_data_utterance1[j]\n","#   temp_image = train_image_data_utterance1[j]\n","\n","#   temp_list = []\n","#   temp_list.append(temp_text)\n","#   temp_list.append(temp_audio)\n","#   temp_list.append(temp_image)\n","\n","#   combined_train_data.append(temp_list)\n","\n","# random.shuffle(combined_train_data)"]},{"cell_type":"code","execution_count":71,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682418031462,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"ZGx2mZfPOWjo"},"outputs":[],"source":["# train_text2 = []\n","# train_audio_utterance2 = []\n","# train_image_utterance2 = []\n","\n","# for j in combined_train_data:\n","#   train_text2.append(j[0])\n","#   train_audio_utterance2.append(j[1])\n","#   train_image_utterance2.append(j[2])\n","\n","\n","\n"]},{"cell_type":"code","execution_count":72,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682418031462,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"Qe5xfQYURWrs"},"outputs":[],"source":["# print(len(train_text2))\n","# print(len(train_audio_utterance2))\n","# print(len(train_image_utterance2))"]},{"cell_type":"code","execution_count":73,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682418031462,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"nPa_FWppRfpn"},"outputs":[],"source":["# train_audio_data_utterance2 = torch.tensor(train_audio_utterance2)\n","# train_audio_data_utterance2.shape"]},{"cell_type":"code","execution_count":74,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682418031462,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"uuHaeA7JiRQz"},"outputs":[],"source":["# train_image_data_utterance2 = torch.tensor(train_image_utterance2)\n","# train_image_data_utterance2.shape"]},{"cell_type":"code","execution_count":75,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682418031462,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"mQCW6oZTifhd"},"outputs":[],"source":["# train_audio_data_utterance = torch.tensor(train_audio_data_utterance2)[:Trainlen]\n","# # train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n","# train_audio_data_utterance.shape"]},{"cell_type":"code","execution_count":76,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1413,"status":"ok","timestamp":1682418032869,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"984ZSUXVWQfj","outputId":"3a4db409-6e7d-4285-fe6d-679b8013b464"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([552, 1000, 768])"]},"metadata":{},"execution_count":76}],"source":["train_audio_data_utterance = torch.tensor(train_audio_data_utterance1)\n","# train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n","train_audio_data_utterance.shape"]},{"cell_type":"code","execution_count":77,"metadata":{"executionInfo":{"elapsed":59,"status":"ok","timestamp":1682418032869,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"eYe20QnlkJwg"},"outputs":[],"source":["# train_audio_data_utterance = torch.tensor(train_audio_data_utterance1)[:Trainlen]\n","# # train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n","# train_audio_data_utterance.shape"]},{"cell_type":"code","execution_count":78,"metadata":{"executionInfo":{"elapsed":59,"status":"ok","timestamp":1682418032869,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"VLqtw_6-irl3"},"outputs":[],"source":["# train_image_data_utterance = torch.tensor(train_image_data_utterance2)[:Trainlen]\n","# # train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n","# train_image_data_utterance.shape"]},{"cell_type":"code","execution_count":79,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59,"status":"ok","timestamp":1682418032869,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"AleXnjQNWTKL","outputId":"129fa44d-fe56-4eb6-c577-0160f56af7da"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([552, 480, 2048])"]},"metadata":{},"execution_count":79}],"source":["train_image_data_utterance = torch.tensor(train_image_data_utterance1)\n","# train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n","train_image_data_utterance.shape"]},{"cell_type":"code","execution_count":80,"metadata":{"executionInfo":{"elapsed":56,"status":"ok","timestamp":1682418032869,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"G1NXg6uikX5P"},"outputs":[],"source":["# train_image_data_utterance = torch.tensor(train_image_data_utterance1)[:Trainlen]\n","# # train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n","# train_image_data_utterance.shape"]},{"cell_type":"code","execution_count":81,"metadata":{"executionInfo":{"elapsed":56,"status":"ok","timestamp":1682418032869,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"YkC6buI-iu39"},"outputs":[],"source":["# valid_audio_data_utterance = torch.tensor(train_audio_data_utterance2)[Trainlen:]\n","# valid_audio_data_utterance.shape"]},{"cell_type":"code","execution_count":82,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1682418032869,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"NU5wt_vv998E","outputId":"7a9846b3-43ba-4927-824e-139de2016aae"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["138"]},"metadata":{},"execution_count":82}],"source":["len(test_text)"]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1682418032869,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"F4yP2ap7Pl27","outputId":"284fe9f5-cb1b-4ca4-e662-3a020f253d95"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([138, 480, 2048])"]},"metadata":{},"execution_count":83}],"source":["test_image_data_utterance1.shape"]},{"cell_type":"code","execution_count":84,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1682418032869,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"g5rke66XPvU3","outputId":"764511f5-ba05-4c17-a931-3ab054b48077"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([138, 1000, 768])"]},"metadata":{},"execution_count":84}],"source":["test_audio_data_utterance1.shape"]},{"cell_type":"code","execution_count":85,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1682418032869,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"fyi5s9iS9jej"},"outputs":[],"source":["# combined_test_data = []\n","\n","# for j in range(len(test_text)):\n","#   temp_text = test_text[j]\n","#   temp_audio = test_audio_data_utterance1[j]\n","#   temp_image = test_image_data_utterance1[j]\n","\n","#   temp_list = []\n","#   temp_list.append(temp_text)\n","#   temp_list.append(temp_audio)\n","#   temp_list.append(temp_image)\n","\n","#   combined_test_data.append(temp_list)\n","\n","# random.shuffle(combined_test_data)\n"]},{"cell_type":"code","execution_count":86,"metadata":{"executionInfo":{"elapsed":49,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"YvgGLs8S-cW9"},"outputs":[],"source":["# test_text2 = []\n","# test_audio_utterance2 = []\n","# test_image_utterance2 = []\n","\n","# for j in combined_test_data:\n","#   test_text2.append(j[0])\n","#   test_audio_utterance2.append(j[1])\n","#   test_image_utterance2.append(j[2])\n"]},{"cell_type":"code","execution_count":87,"metadata":{"executionInfo":{"elapsed":49,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"UgCracq6--vg"},"outputs":[],"source":["# VALIDLEN = 69"]},{"cell_type":"code","execution_count":88,"metadata":{"executionInfo":{"elapsed":49,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"UarPmcdo_uaU"},"outputs":[],"source":["# len(test_audio_utterance2)"]},{"cell_type":"code","execution_count":89,"metadata":{"executionInfo":{"elapsed":49,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"CgOQVaRV_-iV"},"outputs":[],"source":["# test_audio_utterance2[0].shape"]},{"cell_type":"code","execution_count":90,"metadata":{"executionInfo":{"elapsed":49,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"AsLJeYkvAdoK"},"outputs":[],"source":["# torch.stack(test_audio_utterance2).shape"]},{"cell_type":"code","execution_count":91,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"bDznHLJbzAbl"},"outputs":[],"source":["# valid_audio_data_utterance = test_audio_data_utterance1[:VALIDLEN]\n","# valid_audio_data_utterance.shape"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"6XNjZNpBAYd_"},"outputs":[],"source":["# test_audio_data_utterance = test_audio_data_utterance1[VALIDLEN:]\n","# test_audio_data_utterance.shape"]},{"cell_type":"code","execution_count":93,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"Y-kqcb9tiyB8"},"outputs":[],"source":["# valid_image_data_utterance = torch.tensor(train_image_data_utterance2)[Trainlen:]\n","# valid_image_data_utterance.shape"]},{"cell_type":"code","execution_count":94,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"z8KNyWBm2Ie4"},"outputs":[],"source":["# valid_image_data_utterance = test_image_data_utterance1[:VALIDLEN]\n","# valid_image_data_utterance.shape"]},{"cell_type":"code","execution_count":95,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"B5rPLXq6At1F"},"outputs":[],"source":["# test_image_data_utterance = test_image_data_utterance1[VALIDLEN:]\n","# test_image_data_utterance.shape"]},{"cell_type":"code","execution_count":96,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"YCxFywiVko_v"},"outputs":[],"source":["# test_audio_data_utterance = torch.tensor(test_audio_data_utterance1)\n","# # test_audio_data_utterance = test_audio_data_utterance.unsqueeze(dim = 1)\n","# test_audio_data_utterance.shape"]},{"cell_type":"code","execution_count":97,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"hxqP1M_2k2Y_"},"outputs":[],"source":["# test_image_data_utterance = torch.tensor(test_image_data_utterance1)\n","# # test_image_data_utterance = test_image_data_utterance.unsqueeze(dim = 1)\n","# test_image_data_utterance.shape"]},{"cell_type":"code","execution_count":98,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"PKt4UobTIJQY"},"outputs":[],"source":["   \n","\n","# # print(len(text_file[train_text]))\n","# # print(train_text)\n","# # print(len(text_file[test_text]))\n","\n","# train_audio_broadcast_utterance = audio_video_broadcast(train_audio_data_utterance)\n","\n","# print(\"train_audio_broadcast_utterance complete : \", train_audio_broadcast_utterance.shape)\n","# train_image_broadcast_utterance = audio_video_broadcast(train_image_data_utterance)\n","# print(\"train_image_broadcast_utterance complete : \",train_image_broadcast_utterance.shape)\n","\n","# valid_audio_broadcast_utterance = audio_video_broadcast(valid_audio_data_utterance)\n","# print(\"valid_audio_broadcast_utterance complete : \", valid_audio_broadcast_utterance.shape)\n","\n","# valid_image_broadcast_utterance = audio_video_broadcast(valid_image_data_utterance)\n","# print('valid_image_broadcast_utterance complete : ', valid_image_broadcast_utterance.shape)\n","\n","# test_audio_broadcast_utterance = audio_video_broadcast(test_audio_data_utterance)\n","# print(\"test_audio_broadcast_utterance complete : \",test_audio_broadcast_utterance.shape)\n","# test_image_broadcast_utterance = audio_video_broadcast(test_image_data_utterance)\n","# print(\"test_image_broadcast_utterance complete : \",test_image_broadcast_utterance.shape)"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"9Ik_vBo7x6Rm","outputId":"e7510654-9346-433d-ede1-067d74cd6ad4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BartTokenizer(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[CONTEXT]', '[UTTERANCE]']})"]},"metadata":{},"execution_count":99}],"source":["tokenizer"]},{"cell_type":"code","execution_count":99,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1682418032870,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"X0y5DYklxuUT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":99,"metadata":{"executionInfo":{"elapsed":46,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"P1Ctmwokx-T2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":100,"metadata":{"executionInfo":{"elapsed":46,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"o7mKmBRp2XqP"},"outputs":[],"source":["# def prepare_dataset_context(text_data):\n","                    \n","\n","#             context = []\n","#             # labels = []\n","#             for i in range(len(text_data)):\n","#                 data_point = text_data[i]\n","\n","#                 # example_speaker = data_point['speaker']\n","#                 # example_utterance = data_point['utterance']\n","#                 # temp_label = int(data_point['sarcasm'])\n","\n","#                 # example_context = '[CONTEXT] '\n","#                 example_context = ''\n","\n","#                 temp_len = len(data_point['context_speakers'])\n","#                 cnt = 0\n","#                 print(\"Temp len : \", temp_len)\n","#                 for speaker, utterance in list(zip(data_point['context_speakers'], data_point['context'])):\n","#                     print(\"count : \", cnt)\n","#                     if(cnt == temp_len - 1):\n","#                       example_context = example_context + speaker.upper() + \" : \" + utterance\n","#                     else:\n","#                       example_context = example_context + speaker.upper() + \" : \" + utterance + \" , \"\n","#                     cnt+=1\n","\n","                    \n","\n","                \n","#                 # print(example_dialog)\n","#                 example_context = re.sub(' +', ' ', example_context)\n","\n","#                 context.append(example_context)\n","#                 # labels.append(temp_label)\n","\n","#             # df = pd.DataFrame(dialog, columns=['dialog'])\n","\n","#             # labels = torch.tensor(labels, dtype=torch.long)\n","\n","            \n","\n","#             enc = bert_tokenizer(context, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n","\n","#             # df['audio_features'] = acoustic_data\n","#             # df['visual_features'] = visual_data\n","\n","#             return torch.tensor(enc['input_ids'], dtype=torch.long), torch.tensor(enc['attention_mask'], dtype=torch.bool)\n"]},{"cell_type":"code","execution_count":101,"metadata":{"executionInfo":{"elapsed":46,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"HiN-BMSgISy7"},"outputs":[],"source":["def prepare_dataset(text_data):\n","                    \n","\n","            dialog = []\n","            labels = []\n","            for i in range(len(text_data)):\n","                data_point = text_data[i]\n","\n","                example_speaker = data_point['speaker']\n","                example_utterance = data_point['utterance']\n","                temp_label = int(data_point['sarcasm'])\n","\n","                # example_dialog = '[CONTEXT] '\n","                # example_dialog = '[TARGET] '\n","                example_dialog = '[CONTEXT] '\n","\n","\n","                for speaker, utterance in list(zip(data_point['context_speakers'], data_point['context'])):\n","                    example_dialog = example_dialog + speaker.upper() + \" : \" + utterance + \" | \"\n","\n","                example_dialog = example_dialog + ' [UTTERANCE] ' + example_speaker + \" : \" + example_utterance + \" | \"\n","                # example_dialog = example_dialog + example_speaker + \" : \" + example_utterance\n","                # example_dialog = example_dialog + example_speaker + \" : \" + example_utterance \n","                # print(example_dialog)\n","                example_dialog = re.sub(' +', ' ', example_dialog)\n","\n","                dialog.append(example_dialog)\n","                labels.append(temp_label)\n","\n","            # df = pd.DataFrame(dialog, columns=['dialog'])\n","\n","            labels = torch.tensor(labels, dtype=torch.long)\n","\n","            \n","\n","            enc = tokenizer(dialog, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n","\n","            # df['audio_features'] = acoustic_data\n","            # df['visual_features'] = visual_data\n","\n","            return torch.tensor(enc['input_ids'], dtype=torch.long), torch.tensor(enc['attention_mask'], dtype=torch.bool), labels\n"]},{"cell_type":"code","execution_count":101,"metadata":{"executionInfo":{"elapsed":46,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"C2dU7MfiZaDq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":102,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"H7VlrC_WI9ey","outputId":"1a9338cb-afd1-412b-d0d7-f44c293d66af"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BartTokenizer(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True), 'additional_special_tokens': ['[CONTEXT]', '[UTTERANCE]']})"]},"metadata":{},"execution_count":102}],"source":["tokenizer"]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"MgoPnVD4qfO_","outputId":"ad0901bf-b2fc-4ab4-dc36-6b6381040c09"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["552"]},"metadata":{},"execution_count":103}],"source":["len(train_text)"]},{"cell_type":"code","execution_count":104,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"j7p229zVGBUG","outputId":"a1da405b-6f02-4600-e5ee-80896ab66c89"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([552])"]},"metadata":{},"execution_count":104}],"source":["train_text_input_ids1, train_text_attention_mask1, train_ground_truth1 = prepare_dataset(train_text)\n","train_ground_truth1.shape"]},{"cell_type":"code","execution_count":105,"metadata":{"executionInfo":{"elapsed":41,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"WdGsoVBDJxLw"},"outputs":[],"source":["# train_text_input_ids1, train_text_attention_mask1, train_ground_truth1 = prepare_dataset_utterance(train_text)\n","# train_ground_truth1.shape"]},{"cell_type":"code","execution_count":106,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"wQdk9Tk8WYzW","outputId":"53b449f6-f185-4360-a8cc-6336ef0c18d0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([552, 500])"]},"metadata":{},"execution_count":106}],"source":["train_text_input_ids = train_text_input_ids1\n","train_text_input_ids.shape"]},{"cell_type":"code","execution_count":107,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"_rqaEYpWZMVx"},"outputs":[],"source":["# train_text_input_ids = train_text_input_ids1[:Trainlen]\n","# train_text_input_ids.shape"]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"BIYgUkhfWcL8","outputId":"5f1e7f0c-7b29-4f1d-93da-1b20d1292027"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([552, 500])"]},"metadata":{},"execution_count":108}],"source":["train_text_attention_mask = train_text_attention_mask1\n","train_text_attention_mask.shape"]},{"cell_type":"code","execution_count":109,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"MRRAuyVaaOun"},"outputs":[],"source":["# train_text_attention_mask = train_text_attention_mask1[:Trainlen]\n","# train_text_attention_mask.shape"]},{"cell_type":"code","execution_count":110,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1682418032871,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"q819IVudYtDG","outputId":"40af1e27-48a3-4feb-ae42-c9075a7cc1fa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([552])"]},"metadata":{},"execution_count":110}],"source":["train_ground_truth = train_ground_truth1\n","train_ground_truth.shape"]},{"cell_type":"code","execution_count":111,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1682418032872,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"U8iuUm6vWeN-"},"outputs":[],"source":["# train_ground_truth = train_ground_truth1[:Trainlen]\n","# train_ground_truth.shape"]},{"cell_type":"code","execution_count":112,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1682418032872,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"pKXPrrh_jR_O"},"outputs":[],"source":["# train_ground_truth = train_ground_truth1[:Trainlen]\n","# train_ground_truth.shape"]},{"cell_type":"code","execution_count":113,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1682418032872,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"JAcvsTpeaeyB"},"outputs":[],"source":["# train_ground_truth"]},{"cell_type":"code","execution_count":114,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1682418032872,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"bctWHziiWqR0"},"outputs":[],"source":["# context_input_ids, context_attention_mask = prepare_dataset_context(train_text)\n","# print(context_input_ids.shape)"]},{"cell_type":"code","execution_count":115,"metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"ScKZf52FX6AD"},"outputs":[],"source":["# context_attention_mask.shape"]},{"cell_type":"code","execution_count":116,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"P0C9PnF9JypC","outputId":"2737cbe3-946e-4d8a-e46f-7394c5f05493"},"outputs":[{"output_type":"stream","name":"stdout","text":["TYPE : train_text_input_ids :  <class 'torch.Tensor'>\n"]}],"source":["print(\"TYPE : train_text_input_ids : \", type(train_text_input_ids))"]},{"cell_type":"code","execution_count":117,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"yzsrAIieWgvT"},"outputs":[],"source":["\n","# train_context_input_ids = context_input_ids\n","# train_context_input_ids.shape"]},{"cell_type":"code","execution_count":118,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"815UHYi_juEa"},"outputs":[],"source":["\n","# train_context_input_ids = context_input_ids[:Trainlen]\n","# train_context_input_ids.shape"]},{"cell_type":"code","execution_count":119,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"KPScSL-aWiyC"},"outputs":[],"source":["# train_context_attention_mask = context_attention_mask\n","# train_context_attention_mask.shape"]},{"cell_type":"code","execution_count":120,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"tEYf1vyblLxm"},"outputs":[],"source":["# train_context_attention_mask = context_attention_mask[:Trainlen]\n","# train_context_attention_mask.shape"]},{"cell_type":"code","execution_count":121,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"cZECJ_D6YwSF"},"outputs":[],"source":["# valid_text_input_ids = train_text_input_ids1[Trainlen:]\n","# valid_text_input_ids.shape"]},{"cell_type":"code","execution_count":122,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"6Iqag88ipliS"},"outputs":[],"source":["\n","# valid_text_attention_mask = train_text_attention_mask1[Trainlen:]\n","# valid_text_attention_mask.shape"]},{"cell_type":"code","execution_count":123,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"bbjcYjztp523"},"outputs":[],"source":["# valid_ground_truth = train_ground_truth1[Trainlen:]\n","# valid_ground_truth.shape"]},{"cell_type":"code","execution_count":124,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"NDIcnT9t-G1l"},"outputs":[],"source":["# valid_text_input_ids, valid_text_attention_mask, valid_ground_truth = prepare_dataset_utterance(valid_text)\n","# valid_ground_truth.shape"]},{"cell_type":"code","execution_count":125,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"NkfN5cn6YYoz"},"outputs":[],"source":["# valid_context_input_ids = context_input_ids[Trainlen:]\n","# valid_context_input_ids.shape"]},{"cell_type":"code","execution_count":126,"metadata":{"executionInfo":{"elapsed":34,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"1ogVRlSWrgJf"},"outputs":[],"source":["# valid_context_attention_mask = context_attention_mask[Trainlen:]\n","# valid_context_attention_mask.shape"]},{"cell_type":"code","execution_count":127,"metadata":{"executionInfo":{"elapsed":34,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"AuNLAd5CIWLd"},"outputs":[],"source":["# test_text_input_ids, test_text_attention_mask, test_ground_truth = prepare_dataset_utterance(test_text)\n","# test_ground_truth.shape"]},{"cell_type":"code","execution_count":128,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1682418032873,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"7vB39rShGL4K","outputId":"4392399c-e248-4b02-faea-86678410b0b3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([138])"]},"metadata":{},"execution_count":128}],"source":["test_text_input_ids, test_text_attention_mask, test_ground_truth = prepare_dataset(test_text)\n","test_ground_truth.shape"]},{"cell_type":"code","execution_count":129,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"6dIyrYyhGDFH"},"outputs":[],"source":["# valid_id = test_text_input_ids[:VALID_LEN]\n","# valid_id.shape"]},{"cell_type":"code","execution_count":130,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"LmfvmZTEGbiQ"},"outputs":[],"source":["# test_id = test_text_input_ids[VALIDLEN:]\n","# test_id.shape"]},{"cell_type":"code","execution_count":131,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"0Wi2Kyr5Gm52"},"outputs":[],"source":["# valid_mask = test_text_attention_mask[:VALIDLEN]\n","# valid_mask.shape"]},{"cell_type":"code","execution_count":132,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"Z7V0wmGiHAB_"},"outputs":[],"source":["# test_mask = test_text_attention_mask[VALIDLEN:]\n","# test_mask.shape"]},{"cell_type":"code","execution_count":133,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"JFZQBCCTHFzt"},"outputs":[],"source":["# valid_truth = test_ground_truth[:VALIDLEN]\n","# valid_truth.shape"]},{"cell_type":"code","execution_count":134,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"5wiu7X_lHWFi"},"outputs":[],"source":["# test_truth = test_ground_truth[VALIDLEN:]\n","# test_truth.shape"]},{"cell_type":"code","execution_count":135,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"E-ovTep1tWmK"},"outputs":[],"source":["# test_context_input_ids, test_context_attention_mask = prepare_dataset_context(test_text)\n","# test_context_input_ids.shape"]},{"cell_type":"code","execution_count":136,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"iMO8hPfyHkja"},"outputs":[],"source":["# valid_context_id = test_context_input_ids[:VALIDLEN]\n","# valid_context_id.shape"]},{"cell_type":"code","execution_count":137,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"IG3A9ASzHwW3"},"outputs":[],"source":["# test_context_id = test_context_input_ids[VALIDLEN:]\n","# test_context_id.shape"]},{"cell_type":"code","execution_count":138,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"I1iZshp8tld_"},"outputs":[],"source":["# test_context_attention_mask.shape"]},{"cell_type":"code","execution_count":139,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"doTkSyhKIMOt"},"outputs":[],"source":["# valid_context_mask = test_context_attention_mask[:VALIDLEN]\n","# valid_context_mask.shape"]},{"cell_type":"code","execution_count":140,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"UGa0EsBVIpzX"},"outputs":[],"source":["# test_context_mask = test_context_attention_mask[VALIDLEN:]\n","# test_context_mask.shape"]},{"cell_type":"code","execution_count":141,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"_0kqUoFDKPkS"},"outputs":[],"source":["# tokenizer.add_tokens(['[CONTEXT]', '[TARGET]'], special_tokens = True)\n","# model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":142,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"6EEHQlvrRM3j","outputId":"650c176f-8b72-4be4-e55d-263cc55224d6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([138, 1000, 768])"]},"metadata":{},"execution_count":142}],"source":["test_audio_data_utterance1.shape"]},{"cell_type":"code","execution_count":143,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1682418032874,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"5PdHAFkTRsyK","outputId":"a093cf8a-068e-4c50-8575-1879c0b5904f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([138, 480, 2048])"]},"metadata":{},"execution_count":143}],"source":["test_image_data_utterance1.shape"]},{"cell_type":"code","execution_count":144,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"2FilVGy_RxCw","outputId":"3f7e92f8-33c8-4560-b084-887cd42f07da"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([138, 500])\n","torch.Size([138, 500])\n","torch.Size([138])\n"]}],"source":["print(test_text_input_ids.shape)\n","print(test_text_attention_mask.shape)\n","print(test_ground_truth.shape)"]},{"cell_type":"code","execution_count":145,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"Icba89ykR_5g"},"outputs":[],"source":["# print(test_context_input_ids.shape)\n","# print(test_context_attention_mask.shape)\n"]},{"cell_type":"code","execution_count":146,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"kXqJ0fcjSJ4I"},"outputs":[],"source":["# test_input_data = []\n","\n","\n","# for j in range(test_ground_truth.shape[0]):\n","#   temp_list = []\n","#   temp_list.append(test_text_input_ids[j])\n","#   temp_list.append(test_text_attention_mask[j])\n","#   temp_list.append(test_context_input_ids[j])\n","#   temp_list.append(test_context_attention_mask[j])\n","#   temp_list.append(test_audio_data_utterance1[j])\n","#   temp_list.append(test_image_data_utterance1[j])\n","\n","#   test_input_data.append(temp_list)\n","\n","  \n"]},{"cell_type":"code","execution_count":147,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"Z9swXKn_TQsW"},"outputs":[],"source":["# print(type(test_input_data))\n","# print(len(test_input_data))"]},{"cell_type":"code","execution_count":148,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"cjn1S99ZS_SX"},"outputs":[],"source":["# test_output_data = test_ground_truth.tolist()\n","# print(type(test_output_data))\n","# print(len(test_output_data))"]},{"cell_type":"code","execution_count":149,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"ecXLmWNsT6h-"},"outputs":[],"source":["# X_valid, X_test, Y_valid, Y_test = train_test_split(\n","#     test_input_data, test_output_data, test_size = 0.5, stratify = test_output_data\n","# )"]},{"cell_type":"code","execution_count":150,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"GsXIaU_jUUwH"},"outputs":[],"source":["# len(X_valid)"]},{"cell_type":"code","execution_count":151,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"J6lG18S9UcWx"},"outputs":[],"source":["# len(X_test)"]},{"cell_type":"code","execution_count":152,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"IfleSSo6UfWk"},"outputs":[],"source":["# len(Y_valid)"]},{"cell_type":"code","execution_count":153,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"vi0Vpd2lUhR7"},"outputs":[],"source":["# len(Y_test)"]},{"cell_type":"code","execution_count":154,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"MnzHh4kBUk6f"},"outputs":[],"source":["# valid_text_input_ids = []\n","# valid_text_attention_mask = []\n","# valid_context_input_ids = []\n","# valid_context_attention_mask = []\n","\n","# valid_audio_data = []\n","# valid_image_data = []\n","\n","# for j in range(len(X_valid)):\n","#   valid_text_input_ids.append(X_valid[j][0])\n","#   valid_text_attention_mask.append(X_valid[j][1])\n","  \n","#   valid_context_input_ids.append(X_valid[j][2])\n","#   valid_context_attention_mask.append(X_valid[j][3])\n","  \n","#   valid_audio_data.append(X_valid[j][4])\n","\n","#   valid_image_data.append(X_valid[j][5])\n","\n","# print(len(valid_text_input_ids))\n","# print(len(valid_text_attention_mask))\n","# print(len(valid_context_input_ids))\n","# print(len(valid_context_attention_mask))\n","# print(len(valid_audio_data))\n","# print(len(valid_image_data))"]},{"cell_type":"code","execution_count":155,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"xyqnoPspY06u"},"outputs":[],"source":["# valid_text_input_ids[0].shape"]},{"cell_type":"code","execution_count":156,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"XoyGWRN9V6C9"},"outputs":[],"source":["# valid_text_input_ids = torch.stack(valid_text_input_ids)\n","# print(valid_text_input_ids.shape)\n","# valid_text_attention_mask = torch.stack(valid_text_attention_mask)\n","# print(valid_text_attention_mask.shape)\n","# valid_context_input_ids = torch.stack(valid_context_input_ids)\n","# print(valid_context_input_ids.shape)\n","# valid_context_attention_mask = torch.stack(valid_context_attention_mask)\n","# print(valid_context_attention_mask.shape)\n","# valid_audio_data = torch.stack(valid_audio_data)\n","# print(valid_audio_data.shape)\n","# valid_image_data = torch.stack(valid_image_data)\n","# print(valid_image_data.shape)"]},{"cell_type":"code","execution_count":157,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"ipNEkx0wV2Wz"},"outputs":[],"source":["# valid_ground_truth = torch.tensor(Y_valid)\n","# valid_ground_truth.shape"]},{"cell_type":"code","execution_count":158,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1682418032875,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"4FM6TlZIatoC"},"outputs":[],"source":["# test_text_id = []\n","# test_text_mask = []\n","\n","# test_context_id = []\n","# test_context_mask = []\n","\n","# test_audio_data = []\n","# test_image_data = []\n","\n","# for j in range(len(X_test)):\n","#   test_text_id.append(X_test[j][0])\n","#   test_text_mask.append(X_test[j][1])\n","\n","#   test_context_id.append(X_test[j][2])\n","#   test_context_mask.append(X_test[j][3])\n","\n","#   test_audio_data.append(X_test[j][4])\n","\n","#   test_image_data.append(X_test[j][5])\n","\n","# test_text_id = torch.stack(test_text_id)\n","# print(test_text_id.shape)\n","# test_text_mask = torch.stack(test_text_mask)\n","# print(test_text_mask.shape)\n","# test_context_id = torch.stack(test_context_id)\n","# print(test_context_id.shape)\n","# test_context_mask = torch.stack(test_context_mask)\n","# print(test_context_mask.shape)\n","# test_audio_data = torch.stack(test_audio_data)\n","# print(test_audio_data.shape)\n","# test_image_data = torch.stack(test_image_data)\n","# print(test_image_data.shape)  "]},{"cell_type":"code","execution_count":159,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"RiJSqVpFckAT"},"outputs":[],"source":["# test_ground_truth = torch.tensor(Y_test)\n","# print(test_ground_truth.shape)"]},{"cell_type":"code","execution_count":160,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"TMN_GJV1Jjps"},"outputs":[],"source":["\n","# tokenizer"]},{"cell_type":"code","execution_count":161,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"SY3KgbQUKoFd","outputId":"2d682b0d-2ebb-4ee6-f02e-0b3c025d7cac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>', '</s>', '<unk>', '<pad>', '<mask>', '[CONTEXT]', '[UTTERANCE]']"]},"metadata":{},"execution_count":161}],"source":["tokenizer.all_special_tokens"]},{"cell_type":"code","execution_count":161,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"M7uPY9VkLRcr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":162,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"oGASW03jLFtd","outputId":"1e038684-9fce-4176-8bef-0330c8a12dc0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>', '</s>', '<unk>', '<pad>', '<mask>', '[CONTEXT]', '[UTTERANCE]']"]},"metadata":{},"execution_count":162}],"source":["tokenizer.all_special_tokens\n"]},{"cell_type":"code","execution_count":163,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"JVqVQYAULBg5","outputId":"f04f9f7d-f3ca-41ea-bcef-aed103cd9654"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 2, 3, 1, 50264, 50265, 50266]\n"]}],"source":["print(tokenizer.all_special_ids)"]},{"cell_type":"code","execution_count":164,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"hNcRXEYZLK7w"},"outputs":[],"source":["# train_audio_broadcast_utterance.shape"]},{"cell_type":"code","execution_count":165,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"eP1Y_my0-btZ"},"outputs":[],"source":["# valid_audio_data_utterance = test_audio_data_utterance[:VALID_LEN, :, :]\n","# valid_audio_data_utterance.shape"]},{"cell_type":"code","execution_count":166,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"K4c_xiCD-tn0"},"outputs":[],"source":["# valid_image_data_utterance = test_image_data_utterance[:VALID_LEN, :, :]\n","# valid_image_data_utterance.shape"]},{"cell_type":"code","execution_count":167,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"QwLEXwtB-6KN"},"outputs":[],"source":["\n","\n","\n","\n","# test_audio_data_utterance = test_audio_data_utterance[VALID_LEN:, :, :]\n","# test_audio_data_utterance.shape"]},{"cell_type":"code","execution_count":168,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"fF_ErwW5_EsF"},"outputs":[],"source":["\n","\n","# test_image_data_utterance = test_image_data_utterance[VALID_LEN:, :, :]\n","# test_image_data_utterance.shape"]},{"cell_type":"code","execution_count":169,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"GGdaP4uxzX05"},"outputs":[],"source":["test_input_data = []\n","\n","\n","for j in range(test_ground_truth.shape[0]):\n","  temp_list = []\n","  temp_list.append(test_text_input_ids[j])\n","  temp_list.append(test_text_attention_mask[j])\n","\n","  temp_list.append(test_audio_data_utterance1[j])\n","  temp_list.append(test_image_data_utterance1[j])\n","\n","  test_input_data.append(temp_list)\n","\n","  \n"]},{"cell_type":"code","execution_count":170,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"IcHY-Z2szays","outputId":"70db39f3-5fa0-433c-cd37-308c5e8014b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'>\n","138\n"]}],"source":["print(type(test_input_data))\n","print(len(test_input_data))"]},{"cell_type":"code","execution_count":171,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"9Vp5_UG4zm_e","outputId":"5beb70e5-cba6-466a-8ef3-acc94a92e296"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'>\n","138\n"]}],"source":["test_output_data = test_ground_truth.tolist()\n","print(type(test_output_data))\n","print(len(test_output_data))"]},{"cell_type":"code","execution_count":172,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1682418032876,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"0OYmBDf1zpVv"},"outputs":[],"source":["X_valid, X_test, Y_valid, Y_test = train_test_split(\n","    test_input_data, test_output_data, test_size = 0.5, stratify = test_output_data\n",")"]},{"cell_type":"code","execution_count":173,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1682418032877,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"ya-gq91pzs55","outputId":"a4326799-4d13-4afe-f3e9-5354cfb4b1c4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["69"]},"metadata":{},"execution_count":173}],"source":["len(X_valid)"]},{"cell_type":"code","execution_count":174,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1682418032877,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"P6HposQMzvVI","outputId":"c6664a79-c0eb-40f0-d373-e827e21dcd8e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["69"]},"metadata":{},"execution_count":174}],"source":["len(X_test)"]},{"cell_type":"code","execution_count":175,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1682418032877,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"37krGL8vzxgX","outputId":"150be184-c136-4ab8-d1ca-743bda00b676"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["69"]},"metadata":{},"execution_count":175}],"source":["len(Y_valid)"]},{"cell_type":"code","execution_count":176,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1682418032877,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"zVCKId-SzzO5","outputId":"a3ab5346-33d1-48f3-be0e-38a052893d2a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["69"]},"metadata":{},"execution_count":176}],"source":["len(Y_test)"]},{"cell_type":"code","execution_count":177,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1682418032877,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"O_TuTdsjz2gx","outputId":"71f790f3-015b-4ae3-e696-32d1ed43b090"},"outputs":[{"output_type":"stream","name":"stdout","text":["69\n","69\n","69\n","69\n"]}],"source":["valid_text_input_ids = []\n","valid_text_attention_mask = []\n","valid_context_input_ids = []\n","valid_context_attention_mask = []\n","\n","valid_audio_data = []\n","valid_image_data = []\n","\n","for j in range(len(X_valid)):\n","  valid_text_input_ids.append(X_valid[j][0])\n","  valid_text_attention_mask.append(X_valid[j][1])\n","  \n","  \n","  valid_audio_data.append(X_valid[j][2])\n","\n","  valid_image_data.append(X_valid[j][3])\n","\n","print(len(valid_text_input_ids))\n","print(len(valid_text_attention_mask))\n","\n","print(len(valid_audio_data))\n","print(len(valid_image_data))"]},{"cell_type":"code","execution_count":178,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1682418032877,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"mU-7c-w7z9fK","outputId":"049156d2-c515-4d19-dc53-50eaf8a1977f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([500])"]},"metadata":{},"execution_count":178}],"source":["valid_text_input_ids[0].shape"]},{"cell_type":"code","execution_count":179,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58,"status":"ok","timestamp":1682418033590,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"s_RHsBNLz_d8","outputId":"00e27a10-def9-4883-e42a-8c8845a77f6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([69, 500])\n","torch.Size([69, 500])\n","torch.Size([69, 1000, 768])\n","torch.Size([69, 480, 2048])\n"]}],"source":["valid_text_input_ids = torch.stack(valid_text_input_ids)\n","print(valid_text_input_ids.shape)\n","valid_text_attention_mask = torch.stack(valid_text_attention_mask)\n","print(valid_text_attention_mask.shape)\n","\n","valid_audio_data = torch.stack(valid_audio_data)\n","print(valid_audio_data.shape)\n","valid_image_data = torch.stack(valid_image_data)\n","print(valid_image_data.shape)"]},{"cell_type":"code","execution_count":180,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1682418033590,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"ctw1YWnX0DrP","outputId":"f545bad3-6400-4d64-b6a4-9dbb84112870"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([69])"]},"metadata":{},"execution_count":180}],"source":["valid_ground_truth = torch.tensor(Y_valid)\n","valid_ground_truth.shape"]},{"cell_type":"code","execution_count":181,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1682418033590,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"Ogsa3Gqd0FvA","outputId":"095f064d-c44a-4203-cb10-825bd24ad6d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([69, 500])\n","torch.Size([69, 500])\n","torch.Size([69, 1000, 768])\n","torch.Size([69, 480, 2048])\n"]}],"source":["test_text_id = []\n","test_text_mask = []\n","\n","test_context_id = []\n","test_context_mask = []\n","\n","test_audio_data = []\n","test_image_data = []\n","\n","for j in range(len(X_test)):\n","  test_text_id.append(X_test[j][0])\n","  test_text_mask.append(X_test[j][1])\n","\n","\n","  test_audio_data.append(X_test[j][2])\n","\n","  test_image_data.append(X_test[j][3])\n","\n","test_text_id = torch.stack(test_text_id)\n","print(test_text_id.shape)\n","test_text_mask = torch.stack(test_text_mask)\n","print(test_text_mask.shape)\n","\n","test_audio_data = torch.stack(test_audio_data)\n","print(test_audio_data.shape)\n","test_image_data = torch.stack(test_image_data)\n","print(test_image_data.shape)  "]},{"cell_type":"code","execution_count":182,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1682418033590,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"5uzCN2Pz0NdD","outputId":"8385b86f-1e3d-488e-dddc-9f96b377a651"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([69])\n"]}],"source":["test_ground_truth = torch.tensor(Y_test)\n","print(test_ground_truth.shape)"]},{"cell_type":"code","execution_count":182,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1682418033590,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"4yazzz3i0OpP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":183,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1682418033590,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"qrligkveIb4f"},"outputs":[],"source":["class MultimodalSarcasmDataset(Dataset):\n","    # def __init__(self, utterance_input_ids, utterance_attention_mask, context_input_ids, context_attention_mask, acoustic_data, visual_data, labels):\n","    def __init__(self, utterance_input_ids, utterance_attention_mask, acoustic_data, visual_data, labels):\n","\n","        self.utterance_input_ids = utterance_input_ids\n","        self.utterance_attention_mask = utterance_attention_mask\n","        # self.context_input_ids = context_input_ids\n","        # self.context_attention_mask = context_attention_mask\n","        # self.context_attention_mask\n","        self.acoustic_data = acoustic_data\n","        self.visual_data = visual_data\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.utterance_input_ids)\n","\n","    def __getitem__(self, idx):\n","        # return self.utterance_input_ids[idx], self.utterance_attention_mask[idx], self.context_input_ids[idx], self.context_attention_mask[idx], self.acoustic_data[idx], self.visual_data[idx], self.labels[idx]\n","        return self.utterance_input_ids[idx], self.utterance_attention_mask[idx],  self.acoustic_data[idx], self.visual_data[idx], self.labels[idx]\n","\n"]},{"cell_type":"code","execution_count":184,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418033590,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"8IhvAU6M_0mh"},"outputs":[],"source":["# train_text_input_ids.shape"]},{"cell_type":"code","execution_count":185,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418033590,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"8UJo5LQ3RjCx"},"outputs":[],"source":["# train_context_attention_mask.dtype"]},{"cell_type":"code","execution_count":186,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418033590,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"1oQY9qKjXbPU"},"outputs":[],"source":["# train_image_data_utterance.shape"]},{"cell_type":"code","execution_count":186,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1682418033591,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"V1wRtyIFYS2m"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":187,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1682418033591,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"ILQZRYToYYDd"},"outputs":[],"source":["# train_text_input_ids.shape"]},{"cell_type":"code","execution_count":188,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1682418033591,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"U3SQyApZYbH7"},"outputs":[],"source":["# train_text_attention_mask.shape"]},{"cell_type":"code","execution_count":189,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418033591,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"_JFdrEk1Yd-J"},"outputs":[],"source":["# train_context_input_ids.shape"]},{"cell_type":"code","execution_count":190,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418033591,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"sYiUBLjTYgdj"},"outputs":[],"source":["# train_context_attention_mask.shape"]},{"cell_type":"code","execution_count":191,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418033591,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"lRpLc5f7Yiyz"},"outputs":[],"source":["# train_audio_data_utterance.shape"]},{"cell_type":"code","execution_count":192,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418033591,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"6c8Wsp6wYlcV"},"outputs":[],"source":["# train_image_data_utterance.shape"]},{"cell_type":"code","execution_count":193,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418033591,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"mBLinpQcYo_4"},"outputs":[],"source":["# train_ground_truth.shape"]},{"cell_type":"code","execution_count":194,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418033591,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"gya_VngQJFlx"},"outputs":[],"source":["# valid_context_mask.shape"]},{"cell_type":"code","execution_count":195,"metadata":{"executionInfo":{"elapsed":43,"status":"ok","timestamp":1682418033591,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"cvLOlLI1JJmE"},"outputs":[],"source":["# valid_context_id.shape"]},{"cell_type":"code","execution_count":196,"metadata":{"executionInfo":{"elapsed":43,"status":"ok","timestamp":1682418033591,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"YOwejwPbJPwB"},"outputs":[],"source":["# valid_truth.shape"]},{"cell_type":"code","execution_count":197,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418033592,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"-KcNwn5qJcGa"},"outputs":[],"source":["# test_audio_data_utterance.shape"]},{"cell_type":"code","execution_count":198,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418033592,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"R_5HoO4YdY0U"},"outputs":[],"source":["# test_context_input_ids.shape"]},{"cell_type":"code","execution_count":199,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1682418033592,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"UXW_gI7m0pLB","outputId":"ed176c94-e60b-4ee9-ee8b-dd98d6d04595"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([69, 480, 2048])"]},"metadata":{},"execution_count":199}],"source":["test_image_data.shape"]},{"cell_type":"code","execution_count":200,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1682418033592,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"tYWmjXQZ5y2W","outputId":"24ce19d1-6d4c-4da6-8a77-5e23fcd11287"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([69, 1000, 768])"]},"metadata":{},"execution_count":200}],"source":["test_audio_data.shape\n"]},{"cell_type":"code","execution_count":201,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1682418033592,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"0p-j9gl10hAx","outputId":"875cf595-8625-4bd6-b18f-a98a42eb36de"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([69, 500])"]},"metadata":{},"execution_count":201}],"source":["test_text_id.shape"]},{"cell_type":"code","execution_count":202,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1682418033592,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"5na9PPNa0tak","outputId":"7db351f8-6198-49cb-c296-cbb9f4991d94"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([69, 500])"]},"metadata":{},"execution_count":202}],"source":["test_text_mask.shape"]},{"cell_type":"code","execution_count":203,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1682418033592,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"03WRDNsp0zkZ","outputId":"6b8a4b1c-cc9a-4eb0-cfe3-16640e5938ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([69])"]},"metadata":{},"execution_count":203}],"source":["valid_ground_truth.shape"]},{"cell_type":"code","execution_count":204,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1682418033592,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"fj1vwA8H02Pt","outputId":"269380a3-45d4-4e91-9ffe-96029de13e49"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([69])"]},"metadata":{},"execution_count":204}],"source":["test_ground_truth.shape"]},{"cell_type":"code","execution_count":205,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1682418033592,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"5j_r6ZqtIeD9","outputId":"0c0686bf-1fc6-486b-f2ab-117e9b492ee9"},"outputs":[{"output_type":"stream","name":"stdout","text":["<torch.utils.data.dataloader.DataLoader object at 0x7f4c218f1be0>\n"]}],"source":["# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask, train_context_input_ids, train_context_attention_mask,train_audio_broadcast_utterance, train_image_broadcast_utterance, train_ground_truth), batch_size=32, shuffle = True)\n","# valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask, valid_audio_broadcast_utterance, valid_image_broadcast_utterance, valid_ground_truth), batch_size = 32, shuffle = False)\n","# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask, test_context_input_ids, test_context_attention_mask, test_audio_broadcast_utterance, test_image_broadcast_utterance, test_ground_truth), batch_size=32, shuffle = False)\n","\n","# train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask, train_context_input_ids, train_context_attention_mask, train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n","# # valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_context_input_ids, valid_context_attention_mask,   valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n","# test_loader = DataLoader(MultimodalSarcasmDataset(test_text_input_ids, test_text_attention_mask, test_context_input_ids, test_context_attention_mask,  test_audio_data_utterance1, test_image_data_utterance1, test_ground_truth), batch_size=32, shuffle = False)\n","\n","train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask,  train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n","valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask,    valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n","test_loader = DataLoader(MultimodalSarcasmDataset(test_text_id, test_text_mask,   test_audio_data, test_image_data, test_ground_truth), batch_size=32, shuffle = False)\n","\n","\n","print(test_loader)\n","\n","# print(train_loader)"]},{"cell_type":"code","execution_count":206,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1682418033592,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"XJYdQp5vwRim","outputId":"d92049da-45a9-411e-f35d-8c013bd881ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["552"]},"metadata":{},"execution_count":206}],"source":["len(train_loader.dataset)"]},{"cell_type":"code","execution_count":207,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1682418033592,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"ZYX5DVe_wYtG","outputId":"b0cb766a-6d30-4259-b31b-2010b8e4f9de"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["69"]},"metadata":{},"execution_count":207}],"source":["len(valid_loader.dataset)"]},{"cell_type":"code","execution_count":208,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1682418033593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"NPTKQs5uwdsY","outputId":"07491daa-509e-4b83-8e3b-8660e3fbcb03"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["69"]},"metadata":{},"execution_count":208}],"source":["len(test_loader.dataset)"]},{"cell_type":"code","execution_count":209,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1682418033593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"MxfmB8BbZHbj"},"outputs":[],"source":["# base_params_list = []\n","# new_params_list = []\n","# weight_decay = 1e-4\n","# for name, param in model.named_parameters():\n","#         if \"classification_head\" or \"MAF_layer\" in name:\n","#             new_params_list.append(param)\n","#         else:\n","#             base_params_list.append(param)\n","            \n","#         optimizer = torch.optim.AdamW(\n","#         [\n","#             {'params': base_params_list,'lr': 5e-6, 'weight_decay': weight_decay},\n","#             {'params': new_params_list,'lr': LEARNING_RATE, 'weight_decay': weight_decay}            \n","#         ],\n","#         lr=5e-6,\n","#         weight_decay=weight_decay\n","#     )"]},{"cell_type":"code","execution_count":210,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1682418033593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"44TjmFmSIh9D"},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n","criterion = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":211,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1682418033593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"O1EC_JPPUpq_"},"outputs":[],"source":["# model.config"]},{"cell_type":"code","execution_count":212,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1682418033593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"AbrSjHApAMiQ","outputId":"fed855f8-af64-4bb3-8b76-4a7ccf6b6326"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":212}],"source":["DEVICE"]},{"cell_type":"code","execution_count":213,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1682418033593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"TF3U8SkdZJ9P","outputId":"eb39da46-0503-4b46-a1cd-a40486cf73c5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultimodalVideo(\n","  (bart_model): BartModel(\n","    (shared): Embedding(50267, 768)\n","    (encoder): BartEncoder(\n","      (embed_tokens): Embedding(50267, 768)\n","      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n","      (layers): ModuleList(\n","        (0): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): BartDecoder(\n","      (embed_tokens): Embedding(50267, 768)\n","      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n","      (layers): ModuleList(\n","        (0): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (acoustic_context_transform): Linear(in_features=1000, out_features=500, bias=False)\n","  (acoustic_dimension_transform): Linear(in_features=768, out_features=768, bias=False)\n","  (visual_context_transform): Linear(in_features=480, out_features=500, bias=False)\n","  (visual_dimension_transform): Linear(in_features=2048, out_features=768, bias=False)\n","  (maf_one): MAF_main(\n","    (context_attention): ContextAwareAttention(\n","      (attention_layer): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","      )\n","      (u_k): Linear(in_features=768, out_features=768, bias=False)\n","      (w1_k): Linear(in_features=768, out_features=1, bias=False)\n","      (w2_k): Linear(in_features=768, out_features=1, bias=False)\n","      (u_v): Linear(in_features=768, out_features=768, bias=False)\n","      (w1_v): Linear(in_features=768, out_features=1, bias=False)\n","      (w2_v): Linear(in_features=768, out_features=1, bias=False)\n","    )\n","    (gate): Linear(in_features=1536, out_features=768, bias=True)\n","    (dropout_layer): Dropout(p=0.2, inplace=False)\n","    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (maf_two): MAF_main(\n","    (context_attention): ContextAwareAttention(\n","      (attention_layer): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","      )\n","      (u_k): Linear(in_features=768, out_features=768, bias=False)\n","      (w1_k): Linear(in_features=768, out_features=1, bias=False)\n","      (w2_k): Linear(in_features=768, out_features=1, bias=False)\n","      (u_v): Linear(in_features=768, out_features=768, bias=False)\n","      (w1_v): Linear(in_features=768, out_features=1, bias=False)\n","      (w2_v): Linear(in_features=768, out_features=1, bias=False)\n","    )\n","    (gate): Linear(in_features=1536, out_features=768, bias=True)\n","    (dropout_layer): Dropout(p=0.2, inplace=False)\n","    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (classification_head): MultimodalClassification(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.0, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":213}],"source":["model = model.to(DEVICE)\n","model\n"]},{"cell_type":"code","execution_count":214,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1682418033593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"i5MxPWWyIotE"},"outputs":[],"source":["def train_epoch(model, data_loader):\n","      model.train()\n","      epoch_train_loss = 0.0\n","    \n","      \n","      for step, batch in enumerate(tqdm(data_loader, desc = 'Training Iteration')):\n","        # for i, t in enumerate(batch):\n","        #     print(\"Inside hello\")\n","        #     print(i, \" : \", type(t))\n","        batch = tuple(t.to(DEVICE) for t in batch)\n","        # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n","        input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n","        optimizer.zero_grad()\n","        # print(\"Input ids shape : \", input_ids.shape)\n","        # print(\"Input ids shape : \", input_ids.shape)\n","        outputs = model(input_ids = input_ids,\n","                        attention_mask = attention_mask,\n","                        # context_input_ids = context_input_ids,\n","                        # context_attention_mask = context_attention_mask,\n","                        acoustic_input = acoustic_input,\n","                        visual_input = visual_input,\n","                        labels = labels)\n","\n","        loss = outputs['loss']\n","        epoch_train_loss += loss.item()\n","\n","        # print(\"Batch wise loss : \", epoch_train_loss)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    \n","\n","      print(\"Epoch train loss : \", epoch_train_loss)"]},{"cell_type":"code","execution_count":215,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1682418033593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"D0H6Hk2w62xu"},"outputs":[],"source":["def valid_epoch(model, data_loader):\n","  model.eval()\n","  predictions = []\n","  gold = []\n","\n","  valid_loss = 0.0\n","  with torch.no_grad():\n","    for step, batch in enumerate(tqdm(data_loader)):\n","      batch = tuple(t.to(DEVICE) for t in batch)\n","      # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n","      input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n","\n","      outputs = model(input_ids = input_ids,\n","                            attention_mask = attention_mask,\n","                            # context_input_ids = context_input_ids,\n","                            # context_attention_mask = context_attention_mask,\n","                            acoustic_input = acoustic_input,\n","                            visual_input = visual_input,\n","                            labels = labels)\n","      \n","      logits = outputs['logits']\n","      loss = outputs['loss']\n","\n","      valid_loss += loss.item()\n","\n","\n","\n","      pred = logits.argmax(dim = -1)\n","\n","      predictions.extend(pred.tolist())\n","      gold.extend(labels.tolist())\n","\n","  return valid_loss, predictions, gold\n","\n"]},{"cell_type":"code","execution_count":216,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1682418033593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"93nGID9-Imr-"},"outputs":[],"source":["\n","\n","def test_epoch(model, data_loader):\n","    model.eval()\n","    predictions = []\n","    gold = []\n","\n","    correct = 0\n","    with torch.no_grad():\n","        for step, batch in enumerate(tqdm(data_loader)):\n","            batch = tuple(t.to(DEVICE) for t in batch)\n","            # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n","            input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n","\n","            outputs = model(input_ids = input_ids,\n","                            attention_mask = attention_mask,\n","                            # context_input_ids = context_input_ids,\n","                            # context_attention_mask = context_attention_mask,\n","                      \n","                            acoustic_input = acoustic_input,\n","                            visual_input = visual_input,\n","                            labels = labels)\n","\n","            logits = outputs['logits']\n","\n","            pred = logits.argmax(dim = -1)\n","\n","            predictions.extend(pred.tolist())\n","\n","            gold.extend(labels.tolist())\n","\n","            correct += int((pred == labels).sum())\n","\n","    return correct/len(data_loader.dataset), predictions, gold "]},{"cell_type":"code","execution_count":217,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1682418033593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"s5QauBgKBtYS"},"outputs":[],"source":["class EarlyStopping:\n","  def __init__(self, patience, min_delta):\n","    self.patience = patience\n","    self.min_delta = min_delta\n","    self.counter = 0\n","    self.min_validation = np.inf\n","\n","  def early_stop(self, valid_loss):\n","    if valid_loss < self.min_validation:\n","      self.min_validation = valid_loss\n","      self.counter = 0\n","    elif valid_loss > (self.min_validation + self.min_delta):\n","      self.counter += 1\n","      if self.counter >= self.patience:\n","        return True\n","    return False          "]},{"cell_type":"code","execution_count":218,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1682418033593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"8dHmyfM2Cc9b","outputId":"4fe21913-ccbc-4303-e570-16e727be40dc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.EarlyStopping at 0x7f4c218cf430>"]},"metadata":{},"execution_count":218}],"source":["early_stopper = EarlyStopping(patience = 15, min_delta = 0.2)\n","early_stopper"]},{"cell_type":"code","execution_count":218,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1682418033594,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"B0HB4PIc6ixU"},"outputs":[],"source":[]},{"cell_type":"code","source":["\n","def train_and_validation(model, train_loader, valid_loader):\n","  # lowest_loss = 1e6\n","  best_f1 = 0.0\n","  for epoch in range(30):\n","    print(\"\\n=============Epoch : \", epoch)\n","    train_epoch(model, train_loader)\n","    valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n","\n","    if early_stopper.early_stop(valid_loss):\n","      break\n","\n","    print(\"Length of predictions : \", len(valid_pred))\n","    print(\"Length of gold : \", len(valid_gold))\n","    print(\"Valid loss : \", valid_loss)\n","    print(\"\\n Valid Accuracy : \", accuracy_score(valid_gold, valid_pred))\n","    print(\"\\n Valid Precision : \", precision_score(valid_gold, valid_pred, average = 'weighted'))\n","    print(\"\\n Valid Recall : \", recall_score(valid_gold, valid_pred, average = 'weighted'))\n","    print(\"\\nValid F1 score : \", f1_score(valid_gold, valid_pred, average = 'weighted')) \n","\n","    \n","    curr_f1 = f1_score(valid_gold, valid_pred, average = 'weighted')\n","\n","    curr_loss = valid_loss\n","    # if((curr_f1 > best_f1) and (epoch>=4)):\n","    if(curr_f1 > best_f1):  \n","    # if(curr_loss < lowest_loss):    \n","      best_f1 = curr_f1\n","      # print(\"Valid pred : \", valid_pred)\n","      # print('valid_gold : ', valid_gold)\n","      torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/video_first/best_model_epoch_'+str(epoch)+'_best_f1_'+str(int(best_f1*100))+'_foldNum_'+str(foldNum)+'.pt')\n","      \n","      print(\"model saved\\n\")\n","\n","\n"],"metadata":{"id":"JMzMS-JAYMuY","executionInfo":{"status":"ok","timestamp":1682418033594,"user_tz":-330,"elapsed":11,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"execution_count":219,"outputs":[]},{"cell_type":"code","execution_count":220,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":198254,"status":"ok","timestamp":1682418231837,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"RMJL4HbkEBh2","outputId":"754cf0f3-1a3d-4c5f-c990-c85f306504a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=============Epoch :  0\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  14.7883380651474\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  2.227018654346466\n","\n"," Valid Accuracy :  0.5652173913043478\n","\n"," Valid Precision :  0.7598961713173263\n","\n"," Valid Recall :  0.5652173913043478\n","\n","Valid F1 score :  0.43611056462718867\n","model saved\n","\n","\n","=============Epoch :  1\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  11.32961094379425\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  1.78175550699234\n","\n"," Valid Accuracy :  0.6811594202898551\n","\n"," Valid Precision :  0.6980183377698905\n","\n"," Valid Recall :  0.6811594202898551\n","\n","Valid F1 score :  0.6666148142386767\n","model saved\n","\n","\n","=============Epoch :  2\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  10.4155795276165\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  1.8957257270812988\n","\n"," Valid Accuracy :  0.5942028985507246\n","\n"," Valid Precision :  0.783574879227053\n","\n"," Valid Recall :  0.5942028985507246\n","\n","Valid F1 score :  0.5324511657214871\n","\n","=============Epoch :  3\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  9.818294376134872\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  1.2488606870174408\n","\n"," Valid Accuracy :  0.8115942028985508\n","\n"," Valid Precision :  0.8499520365041093\n","\n"," Valid Recall :  0.8115942028985508\n","\n","Valid F1 score :  0.8089625028755464\n","model saved\n","\n","\n","=============Epoch :  4\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  8.828269064426422\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  1.2617135643959045\n","\n"," Valid Accuracy :  0.7391304347826086\n","\n"," Valid Precision :  0.7775494071146246\n","\n"," Valid Recall :  0.7391304347826086\n","\n","Valid F1 score :  0.7344799586624343\n","\n","=============Epoch :  5\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  7.32613205909729\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  1.5073923766613007\n","\n"," Valid Accuracy :  0.7246376811594203\n","\n"," Valid Precision :  0.8028246081041112\n","\n"," Valid Recall :  0.7246376811594203\n","\n","Valid F1 score :  0.7124709250313115\n","\n","=============Epoch :  6\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  6.765204355120659\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  1.2659897059202194\n","\n"," Valid Accuracy :  0.7246376811594203\n","\n"," Valid Precision :  0.7835057326269167\n","\n"," Valid Recall :  0.7246376811594203\n","\n","Valid F1 score :  0.7157759888560333\n","\n","=============Epoch :  7\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  5.016479395329952\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  1.3257133290171623\n","\n"," Valid Accuracy :  0.7681159420289855\n","\n"," Valid Precision :  0.8254568367989918\n","\n"," Valid Recall :  0.7681159420289855\n","\n","Valid F1 score :  0.7618729096989967\n","\n","=============Epoch :  8\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  3.8569566383957863\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  1.033923216164112\n","\n"," Valid Accuracy :  0.8115942028985508\n","\n"," Valid Precision :  0.8122529644268774\n","\n"," Valid Recall :  0.8115942028985508\n","\n","Valid F1 score :  0.8117530275957912\n","model saved\n","\n","\n","=============Epoch :  9\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  3.8671011477708817\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  1.2801758795976639\n","\n"," Valid Accuracy :  0.7971014492753623\n","\n"," Valid Precision :  0.7994748270464072\n","\n"," Valid Recall :  0.7971014492753623\n","\n","Valid F1 score :  0.7955406911928652\n","\n","=============Epoch :  10\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  3.0671491995453835\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  1.425295241177082\n","\n"," Valid Accuracy :  0.7536231884057971\n","\n"," Valid Precision :  0.7564987347596043\n","\n"," Valid Recall :  0.7536231884057971\n","\n","Valid F1 score :  0.7509802275433834\n","\n","=============Epoch :  11\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  1.3719301847741008\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  1.870225335471332\n","\n"," Valid Accuracy :  0.7681159420289855\n","\n"," Valid Precision :  0.7681159420289855\n","\n"," Valid Recall :  0.7681159420289855\n","\n","Valid F1 score :  0.7681159420289855\n","\n","=============Epoch :  12\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  1.4139164928346872\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  2.0059886425733566\n","\n"," Valid Accuracy :  0.6956521739130435\n","\n"," Valid Precision :  0.7154219057718528\n","\n"," Valid Recall :  0.6956521739130435\n","\n","Valid F1 score :  0.6933431071608559\n","\n","=============Epoch :  13\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  2.037162845954299\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  2.2795209931209683\n","\n"," Valid Accuracy :  0.7391304347826086\n","\n"," Valid Precision :  0.7659688674181427\n","\n"," Valid Recall :  0.7391304347826086\n","\n","Valid F1 score :  0.7363763219741479\n","\n","=============Epoch :  14\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  2.131082024425268\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  2.3824128210544586\n","\n"," Valid Accuracy :  0.6666666666666666\n","\n"," Valid Precision :  0.6775793650793651\n","\n"," Valid Recall :  0.6666666666666666\n","\n","Valid F1 score :  0.6536440991490937\n","\n","=============Epoch :  15\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  1.4123142566531897\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  2.2442976385354996\n","\n"," Valid Accuracy :  0.6956521739130435\n","\n"," Valid Precision :  0.6966106893643126\n","\n"," Valid Recall :  0.6956521739130435\n","\n","Valid F1 score :  0.6923873399065325\n","\n","=============Epoch :  16\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  1.5766733782365918\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  2.605666697025299\n","\n"," Valid Accuracy :  0.6521739130434783\n","\n"," Valid Precision :  0.652407248969433\n","\n"," Valid Recall :  0.6521739130434783\n","\n","Valid F1 score :  0.6472263868065968\n","\n","=============Epoch :  17\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  0.7056059041060507\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  1.572681449353695\n","\n"," Valid Accuracy :  0.8115942028985508\n","\n"," Valid Precision :  0.8114588715828842\n","\n"," Valid Recall :  0.8115942028985508\n","\n","Valid F1 score :  0.8113549574419139\n","\n","=============Epoch :  18\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  0.30189890204928815\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  2.2887058034539223\n","\n"," Valid Accuracy :  0.7246376811594203\n","\n"," Valid Precision :  0.7302635046113307\n","\n"," Valid Recall :  0.7246376811594203\n","\n","Valid F1 score :  0.7196278404007874\n","\n","=============Epoch :  19\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  0.3794424089428503\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  2.570231407880783\n","\n"," Valid Accuracy :  0.7391304347826086\n","\n"," Valid Precision :  0.7433304814497939\n","\n"," Valid Recall :  0.7391304347826086\n","\n","Valid F1 score :  0.7354197901049475\n","\n","=============Epoch :  20\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  0.47742230171570554\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  2.785288453102112\n","\n"," Valid Accuracy :  0.7536231884057971\n","\n"," Valid Precision :  0.7871328199942961\n","\n"," Valid Recall :  0.7536231884057971\n","\n","Valid F1 score :  0.7501817345295607\n","\n","=============Epoch :  21\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  0.5848687342368066\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  2.0087692588567734\n","\n"," Valid Accuracy :  0.782608695652174\n","\n"," Valid Precision :  0.782387244408356\n","\n"," Valid Recall :  0.782608695652174\n","\n","Valid F1 score :  0.7823326432022084\n","\n","=============Epoch :  22\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  0.5517897218232974\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Length of predictions :  69\n","Length of gold :  69\n","Valid loss :  5.109422445297241\n","\n"," Valid Accuracy :  0.6231884057971014\n","\n"," Valid Precision :  0.6426032266885425\n","\n"," Valid Recall :  0.6231884057971014\n","\n","Valid F1 score :  0.5938808373590982\n","\n","=============Epoch :  23\n"]},{"output_type":"stream","name":"stderr","text":["Training Iteration: 100%|██████████| 18/18 [00:07<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch train loss :  0.32995545002631843\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.85it/s]\n"]}],"source":["# train_and_validation(model, train_loader, test_loader)\n","train_and_validation(model, train_loader, valid_loader)"]},{"cell_type":"code","execution_count":221,"metadata":{"id":"3o5p6PC96ZJQ","executionInfo":{"status":"ok","timestamp":1682418231837,"user_tz":-330,"elapsed":28,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":["# best_model_epoch_12_best_f1_72_foldNum_0.pt\n","# best_model_epoch_9_best_f1_69_foldNum_1.pt\n","# best_model_epoch_9_best_f1_79_foldNum_2.pt\n","# best_model_epoch_9_best_f1_73_foldNum_3.pt\n","# best_model_epoch_7_best_f1_75_foldNum_4.pt  "]},{"cell_type":"code","execution_count":227,"metadata":{"executionInfo":{"elapsed":480,"status":"ok","timestamp":1682418324937,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"DO0Qsq9ufvhi"},"outputs":[],"source":["# path = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/stratify/best_model_epoch_11_best_f1_78_foldNum_0.pt'\n","\n","path = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/video_first/best_model_epoch_8_best_f1_81_foldNum_0.pt'\n","\n"]},{"cell_type":"code","execution_count":228,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1682418326593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"NAZqaFeUAiMW"},"outputs":[],"source":["# PATH_0 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_12_f1_76.pt'\n","# PATH_1 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_13_f1_78_foldNum_1.pt'\n","# PATH_2 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_9_f1_64_foldNum_2.pt'\n","# PATH_3 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_5_f1_73_foldNum_3.pt'\n","# PATH_4 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_74_foldNum_4.pt'"]},{"cell_type":"code","execution_count":229,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418326593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"NU9qavVgEwJ0"},"outputs":[],"source":["\n","# PATH_0 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_6_f1_50_foldNum_0.pt'\n","# PATH_1 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_70_foldNum_1.pt'\n","# PATH_2 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_11_f1_63_foldNum_2.pt'\n","# PATH_3 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_44_foldNum_3.pt'\n","# PATH_4 = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/best_model_epoch_4_f1_62_foldNum_4.pt'"]},{"cell_type":"code","execution_count":230,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682418326593,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"5PMNVLL3BM-Y","outputId":"dfad4299-ba47-4403-ae36-8d46b9ea4387"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":230}],"source":["foldNum"]},{"cell_type":"code","execution_count":231,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682418326594,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"_J_qDFj5_ie3","outputId":"60c4ed61-d582-4248-fb58-3caf24796429"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":231}],"source":["model.load_state_dict(torch.load(path))"]},{"cell_type":"code","execution_count":232,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2544,"status":"ok","timestamp":1682418331627,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"},"user_tz":-330},"id":"UgEpzAt-_mSQ","outputId":"c7fd4ab8-961b-4071-d39a-d9dbb9ce037b"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:00<00:00,  3.59it/s]"]},{"output_type":"stream","name":"stdout","text":["0.6956521739130435\n","\n","Accuracy :  0.6956521739130435\n","\n","Precision :  0.7029964747356052\n","\n","Recall :  0.6956521739130435\n","\n","F1 score :  0.6956521739130435\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["\n","\n","acc, test_pred, test_gold = test_epoch(model, test_loader)\n","\n","print(acc)\n","\n","print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n","print(\"\\nPrecision : \", precision_score(test_gold, test_pred, average = 'weighted'))\n","print(\"\\nRecall : \", recall_score(test_gold, test_pred, average = 'weighted'))\n","print(\"\\nF1 score : \", f1_score(test_gold, test_pred, average = 'weighted'))\n","        \n","\n","\n","\n","\n","\n","\n","\n","                                                                 \n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"50ctOavcBUOt","executionInfo":{"status":"aborted","timestamp":1682418232915,"user_tz":-330,"elapsed":34,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTd5vAY4HOp6","executionInfo":{"status":"aborted","timestamp":1682418232915,"user_tz":-330,"elapsed":34,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":["# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n","\n","# # print(acc)\n","\n","# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n","# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred, average = 'weighted'))\n","# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred, average = 'weighted'))\n","# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred, average = 'weighted'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eD8Mc-spvSz","executionInfo":{"status":"aborted","timestamp":1682418232915,"user_tz":-330,"elapsed":33,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3K5movSJPIf","executionInfo":{"status":"aborted","timestamp":1682418232916,"user_tz":-330,"elapsed":34,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":["# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n","\n","# # print(acc)\n","\n","# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n","# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred))\n","# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred))\n","# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWNb08EkJPu0","executionInfo":{"status":"aborted","timestamp":1682418232916,"user_tz":-330,"elapsed":34,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":["# valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n","\n","# # print(acc)\n","\n","# print(\"\\nAccuracy : \", accuracy_score(valid_gold, valid_pred))\n","# print(\"\\nPrecision : \", precision_score(valid_gold, valid_pred, average = 'micro'))\n","# print(\"\\nRecall : \", recall_score(valid_gold, valid_pred, average = 'micro'))\n","# print(\"\\nF1 score : \", f1_score(valid_gold, valid_pred, average = 'micro'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ixy_k2L9GgEV","executionInfo":{"status":"aborted","timestamp":1682418232916,"user_tz":-330,"elapsed":33,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3XQZOkxJ4eP","executionInfo":{"status":"aborted","timestamp":1682418232916,"user_tz":-330,"elapsed":33,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":["\n","\n","# acc, test_pred, test_gold = test_epoch(model, test_loader)\n","\n","# # print(acc)\n","\n","# print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n","# print(\"\\nPrecision : \", precision_score(test_gold, test_pred))\n","# print(\"\\nRecall : \", recall_score(test_gold, test_pred))\n","# print(\"\\nF1 score : \", f1_score(test_gold, test_pred))\n","        \n","\n","\n","\n","\n","\n","\n","\n","                                                                 \n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jryD1kV-J5MG","executionInfo":{"status":"aborted","timestamp":1682418232917,"user_tz":-330,"elapsed":34,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":["\n","\n","# acc, test_pred, test_gold = test_epoch(model, test_loader)\n","\n","# # print(acc)\n","\n","# print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n","# print(\"\\nPrecision : \", precision_score(test_gold, test_pred, average = 'micro'))\n","# print(\"\\nRecall : \", recall_score(test_gold, test_pred, average = 'micro'))\n","# print(\"\\nF1 score : \", f1_score(test_gold, test_pred, average = 'micro'))\n","        \n","\n","\n","\n","\n","\n","\n","\n","                                                                 \n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"993-qFHhZxPp","executionInfo":{"status":"aborted","timestamp":1682418232917,"user_tz":-330,"elapsed":33,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":["# test_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPaJ3qk0isrD","executionInfo":{"status":"aborted","timestamp":1682418232917,"user_tz":-330,"elapsed":33,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":["# test_gold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94i6uFJtivrG","executionInfo":{"status":"aborted","timestamp":1682418232917,"user_tz":-330,"elapsed":33,"user":{"displayName":"Mohit Singh Tomar","userId":"08465857202059996456"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPfmI9ojDvCuNbWz9rxgGVX"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}